Title,Body,Upvotes,Date Posted
Monthly Self-Promotion Thread - March 2024,"Hello and howdy, digital miners of /r/webscraping!
The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!

Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?
Maybe you've got a ground-breaking product in need of some intrepid testers?
Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?
Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?

Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!
Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",10,2024-03-01 11:02:14
Scraping Google Search results (on a small scale),"I'm working on a Question Answering system that scrapes the internet for information in real time. It seemed like a pretty simple task, but I'm having trouble with the web-scraping aspect. I'm pretty new to any sort of scraping so I need to get an idea of this - is it a pretty easy task to scrape Google search - like scraping the top 5 links of 10 different search queries? I feel like that's not a huge number, but I'm already having issues and I think they're related to Google blocking bots (basically the scraped links were good at first, and accurate to what was coming up when I searched manually, but then it started returning links only from sites like quora or youtube, which I couldn't get the info I wanted from). Is this something that you need to use an API for or is it pretty easy to work around with some proxy changes and stuff? Is it just Google that's difficult to work around (if it even is, maybe small-scale google search scraping is really basic?) and are other browsers search results easier to scrape?",3,2024-03-12 08:16:31
Slow scraper ?,"import asyncio
import csv
import logging
from playwright.async_api import async_playwright

async def process_tender(page, tender_link):
    retries = 3  # Number of retries
    timeout = 60  # Timeout in seconds

    for attempt in range(retries):
        try:
            await page.goto(tender_link, timeout=timeout * 1000)  # Convert timeout to milliseconds

            # Extract data from the tender page



            return extracted_data

        except asyncio.TimeoutError:
            logging.warning(f'Timed out after {timeout} seconds. Retrying...')
            continue  # Retry the request

        except Exception as e:
            logging.error(f'An error occurred while processing tender {tender_link}: {str(e)}')
            return None
        finally:
            await page.close()

async def main():
    tender_data_list = []  # List to store processed tender data

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            page = await context.new_page()


            # Navigate to the webpage
            logging.info('Navigating to the webpage...')
            await page.goto('')
            await page.wait_for_timeout(5000)

            # Find all the links with class 'link2'
            org_links = await page.locator(""a.link2"").all()
            org_links_abs = []

            # Turn the organisation links into absolute URLs
            for link in org_links:
                href = await link.get_attribute('href')
                org_links_abs.append(f""{href}"")

            semaphore = asyncio.Semaphore(8)  # Define semaphore outside the loop

            logging.info('Processing tenders...')
            # Loop over each organisation link to get its inner tenders
            for org_abs in org_links_abs:
                logging.info(f'Processing organisation: {org_abs}')
                await page.goto(org_abs)
                await page.wait_for_timeout(5000)
                tender_links = await page.locator(""//a[@title='View Tender Information']"").all()

                # List to store tasks for concurrent processing
                tasks = []

                # Process tender links concurrently
                for tender_link in tender_links:
                    href = await tender_link.get_attribute('href')
                    tender_link_abs = f""{href}""
                    await asyncio.sleep(2)
                    async with semaphore:  # Acquire semaphore for each task
                        new_page = await context.new_page()
                        task = asyncio.create_task(process_tender(new_page, tender_link_abs))
                        tasks.append(task)

                # Wait for all tasks to complete
                processed_tender_data = await asyncio.gather(*tasks)
                tender_data_list.extend(processed_tender_data)

            # Close the browser
            logging.info('Closing browser...')
            await browser.close()

        # Write the extracted data to CSV
        logging.info('Writing data to CSV...')
        with open('tender_data.csv', 'w', newline='', encoding='utf-8') as csvfile:


    except Exception as e:
        logging.error(f'An error occurred: {str(e)}')

if __name__ == ""__main__"":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
this is scraping a  publicly available  tender website  . i am using asyncio.Semaphore(8)   to limit the concurrent process  but  instead of opening 8 tabs its opening one tab at a time and process the data   so its taking me 1and half hours to process, 2500 links . is there any logic error in semaphore  ?

â€‹",1,2024-03-12 07:17:41
Email address scraping from social media!,"I was looking for email addresses of creators active on Instagram, YouTube, LinkedIn and couple other websites who hold more than 50k/100k subscribers/followers depending on their location. 
What can be the possible options for the same?
Any leads/suggestions?",1,2024-03-12 06:56:08
"Scraping a table, why does it say ""cannot set a row with mismatched columns"", when clearly my headers match with the row's like headers are 4 and rows are also 4","â€‹
https://preview.redd.it/961ykc48nunc1.png?width=1385&format=png&auto=webp&s=17021658a1bf5773165bdfeac469930ba2873b0c",1,2024-03-12 06:42:38
Scraping Wholesale + Products for eCommerce,"Hey everyone!
I have a wholesaler that I need to scrape because they don't have price lists or marketing materials easily laid out in .csv or any usable way for eCommerce.
I""m putting all the brand names behind spoilers cuz of the subreddit rules around linking to paid services. I just need whatever information is relevant I'm not trying to sell anyhtig, but I am willing to pay for a service that does what I need because I'm not a developer of any kind and just need this to keep going on my business. 
Website to scrape: Magenta Wholesaler Website
Info to get: descriptions, pricing, photos (multi photo products are key!), hopefully minimum quantity if it can figure out from order page.
Tools using: I'm looking at Shopify & Ecwid for my site, Scraping w/ Apify potentially? I need it to login, get the information, not DDOS or otherwise freak out the site owner with requests, and get the information I need. I am open to more laborious ways of doing this. The website itself is set up really poorly from the perspective of monitoring in any way, which is particularly difficult for an ecommerce site using them as a wholeasler triyng to get the product lists, the different categories and subcategories, the minimum quantities, and all the photos to at least get started (Eventually we'll replace with our own original assets but you need to have the default manufacturer ones to look legitimate and to compare across vendors). 
Anyways, all help is appreciated!",1,2024-03-12 06:41:24
Instagram Profile Scraper,"The ""Instagram Profile Scraper"" is a Python script designed to extract multimedia content from Instagram profiles using Selenium and BeautifulSoup libraries. The script provides a user-friendly interface leveraging Tkinter, enabling users to input the target Instagram username, password, profile URL, and desired download directory. It automates the process of logging into Instagram, navigating to the specified profile, and collecting images and videos.
Key Features:
User-friendly Interface: Implemented with Tkinter, facilitating ease of use and interaction for users.
Automated Login: Utilizes Selenium to automate the login process, enhancing efficiency and convenience.
Content Extraction: Extracts images and videos from Instagram profiles, ensuring comprehensive content collection.
Dynamic Scrolling: Utilizes dynamic scrolling to load additional content, ensuring comprehensive data retrieval.
Download Management: Organizes downloaded content into respective image and video directories for easy access and management.
Error Handling: Implements robust error handling to gracefully manage exceptions during the scraping process.
Password Security: Incorporates password hiding functionality to enhance user privacy and security.
Technologies Used:
Python: Core programming language for script development.
Selenium: Web automation library for browser interaction and navigation.
BeautifulSoup: HTML parsing library for extracting data from web pages.
Tkinter: GUI toolkit for creating the user interface.
Requests: HTTP library for making web requests.
JSON: Data interchange format for parsing Instagram API responses.
Chrome WebDriver: Web browser automation tool for Selenium.",1,2024-03-12 05:11:36
Scraping Info From Individual Listing Profiles After Scraping the Listing Site? Help Needed!,"Hey all, I'm trying to build a workflow and tech stack for list building and need some advice. I'm trying to scrape businesses from Houzz, but am struggling with how to get the right data.
Here's the problem, when you scrape Houzz with a tool like Instant Data Scraper or Easy Scraper, you just get the company name + Houzz profile URL. The website URL, company phone number, and address is tucked within each of the Houzz profile URLs, so I need a tool that can visit each scraped Houzz profile URL and extract the data.   
So the process would essentially be:

Scrape listing page for company name + Houzz profile URLs
Add Houzz profile URLs to workflow to scrape website, phone number, and address

Anyone know of a tool that can do this? Or an alternative solution?   
I was also planning on using Clay to enrich the data further, but it would be pretty cost using it for this entire process, so I'm hoping to solve at least the basic data needs in a more cost-effective way.   
Thanks!",1,2024-03-11 19:26:31
ðŸï¸ We are looking for the perfect scraping tool for travel sites. Help requested!,"Hi!
We are currently developing a portal on the subject of ""Travel and Sport"". For this we need data from various tour operators. The tour operators have a certain pool of trips that is updated once or twice a year. We have to scrape this basic data once and keep it up to date. In total, there are certainly 20 smaller websites that we have to scrape and keep up to date, although - as described - there are not changes every week. In addition, every trip has certain availabilities. These can change daily.
To summarize again:
We have around 20 websites, each with around 300 trips. We have to scrape this basic data once and then check it for updates once every two weeks. The availability of the trips is only a very small part and must be checked daily for updates.
Do you have any recommendations on the best way to do this? We have a developer with Python experience. However, it would be nice if non-coders could also use the tool.
A small budget is available for this. If it costs money, it should also be easy to use and not require an insane amount of training.
So far, my colleague has only tested Selenium very briefly. As he is the developer, he would certainly be able to cope with it. As a non-developer, I would of course prefer a web tool. We are simply looking for the best compromise.
Many thanks for your help!",6,2024-03-11 09:08:54
Need Help Bypassing Nike's Akamai,"Hi! I know basic Selenium. When it gets too complicated then I get lost. I want to build a Bot that can automatically purchase a product. I am using undetected Chromedriver, changed headers, and donÂ´t know what else to do. Is there maybe a standard Selenium script to bypass Akamai specifically? Can anyone help me?
â€‹
options = uc.ChromeOptions()
        options.add_argument('--start-maximized')
        options.add_argument(""--no-sandbox"")
        options.add_argument(""--disable-dev-shm-usage"")
        options.add_argument('--disable-popup-blocking')
        options.add_argument('--disable-extensions')
        options.add_argument('--ignore-ssl-errors')
        options.add_argument('--ignore-certificate-errors')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('log-level=3')
        options.add_experimental_option('prefs', {'intl.accept_languages': 'en,en_US'})
        # options.add_argument(f""--user-data-dir={user_dir}"")
        prefs = {f'profile.default_content_settings.popups': 0,
                 ""credentials_enable_service"": False,
                 ""profile.password_manager_enabled"": False,
                 'useAutomationExtension':False,
                 ""excludeSwitches"": [""enable-automation""]}
        options.add_experimental_option(""prefs"", prefs)
        #options.add_experimental_option('useAutomationExtension', False)
        #options.add_experimental_option(""excludeSwitches"", [""enable-automation""])
        #options.add_experimental_option('excludeSwitches', ['enable-logging'])
        #options.add_experimental_option(""excludeSwitches"", [""enable-automation""])
        headers = {
                'Referer': 'hhttps://www.google.com/',
                'sec-fetch-site': 'cross-site',
                # Add any other headers as needed
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',
                }
        options.add_argument(f""--user-agent={headers['User-Agent']}"")
        # if proxy != ""No"":
        #     options.add_argument(f""--proxy-server={proxy}"")
        if headless:
            options.add_argument('--headless')
            options.add_argument(""--window-size=1920,1080"")
            options.add_argument(""--disable-gpu"")
            options.add_argument(""--disable-blink-features=AutomationControlled"")
        driver = uc.Chrome(options=options)
        driver.execute_script(""Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"")
        return driver

â€‹",3,2024-03-10 15:49:28
How did OpenAI scrap the entire Internet for training Chat GPT?,"Out of curiosity, how did OpenAI *scrape the entire Internet for training ChatGPT?",114,2024-03-09 19:29:18
Shein Products based on sku list,"Hi guys, anyone knows how can I scrap shein products based on a sku list I have?",1,2024-03-10 11:07:11
How scrape with the selenium group post?,"hi,
I tried scrape with selenium group post and I have problem with python code.

posts = self.browser.find_elements(By.XPATH, ""//div[@class='xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs x126k92a')]"")
print(posts.count())

This code not  you have any idea?",0,2024-03-10 10:07:45
How to scrap all the messages from a really big whatsapp chat?,"Hey guys!I need to get all the messages(including audio, images and videos. Stickers if possible) from a really old chat. IÂ´ve searched methods to make this, but only found two ways:  

Suspicious Software.
Extract data from whatsapp backups.

â€‹
I don't want to download any suspicious software, and extracting data from WA backups requires downgrading to older versions of Whatsapp (I'm afraid of losing data) and I don't think it saves media either. And the last solution for me was to scrap the WA web. But I have some questions:

Is this possible to make? (pls say yes ðŸ¥¹)
Do you guys know any better solution for this?
Wich is the best lib/software/api for this? (I just used Selenium a couple times)
By the amount of messages and media, will my web driver crash?",1,2024-03-10 08:55:12
"I need to scrap 1M+ pages heavily protected (cloudflare, anti bots etc.) with python. Any advice?","Hi all,
Thank you for your help.",35,2024-03-09 07:02:56
APIs to bypass akamai? Cost?,"I am familiar with jevi, any other APIs that are able to bypass akamai, datadome, K4sada etc?",1,2024-03-09 23:09:07
Hawaii DCCA Disciplinary Actions Scraping Help,"I have some code here to scape hawaii dcca disciplinary actions. The structure of the html tags are not ideal and about half of the reports are pdf docs, so the approach I thought best was to extract the text from the links and use a regex search to get matches and put them in a data frame. I am able to get respondent, case number, sanction, and effective date, but I am having some trouble collecting the case descriptions. Additionally, I only want records under ""Medical Board"". The current code collects all of the cases on the page/pdf. 
Any ideas on how to approach this? Relatively new to we scraping so open to any and all tips as well. 
links = []
#loop though site pages
for i in range(0, 26):
base_url = f'https://cca.hawaii.gov/oah/category/news-releases/page/{i}/'
page = urlopen(base_url)
soup = BeautifulSoup(page, 'html.parser')
â€‹
# Find all <h3> 
h3_tags = soup.select('h3 a[href]')
# Extract links from <h3><a> tags
for a_tag in h3_tags:
link = a_tag['href']
links.append(link)
# initialize regex pattern 
pattern = re.compile(r""Respondent:\s*(.*?)\s*Case Number:\s*(.*?)\s*Sanction:\s*(.*?)\s*Effective Date:\s*([\d\s-]+)\s*"", re.DOTALL)
#description_pattern = re.compile(r'Effective Date:(.*?)(?:Respondent:|$)', re.DOTALL)
â€‹
#create empy data frame
columns = ['Respondent', 'Case Number', 'Sanction', 'Effective Date']
df = pd.DataFrame()
â€‹
#loop through links
for link in links: 
#check if pdf
if '.pdf' in link: 
#create temperary pdf
read = requests.get(link)
with open('temp.pdf', 'wb') as pdf_file:
pdf_file.write(read.content)
pdf_document = fitz.open('temp.pdf')
#extract text from document
text = """"
for page_num in range(pdf_document.page_count):
page = pdf_document[page_num]
text += page.get_text()
text = text.replace('\n', '')
#find matches and append results to dataframe
matches = pattern.findall(text)
#description_matches = description_pattern.findall(text)
#description_matches = description_matches if description_matches else ['']
#comment_df = pd.DataFrame(description_matches)
temp_df = pd.DataFrame(matches, columns=columns)
#temp_df['Description'] = comment_df
df = df.append(temp_df)
â€‹
else: 
#open html and extract text
page = urlopen(link)
html_bytes = page.read()
html = html_bytes.decode(""utf-8"")
soup = BeautifulSoup(html, 'html.parser')
text = soup.get_text().replace('\n', '').replace('\xa0', '')
#find matches and append results to dataframe
matches = pattern.findall(text)
#description_matches = description_pattern.findall(text)
#description_matches = description_matches if description_matches else ['']
#comment_df = pd.DataFrame(description_matches)
temp_df = pd.DataFrame(matches, columns=columns)
#temp_df['Description'] = comment_df
df = df.append(temp_df)",1,2024-03-09 19:41:53
Passing value with Puppeteer to field not working,"I'm trying to pass data to the field with Puppeteer (website: https://przegladarka-ekw.ms.gov.pl/eukw_prz/KsiegiWieczyn/znajnieKW) using a scrapper made in Puppeteer, but I failed when entering the value into the first field. I am talking about this:
<input id=""kodWydzialuInput"" class=""required"" type=""text"" maxlength=""4"" aria-label=""Lista rozwijana wybierz kod wydziaÅ‚u"" aria-required=""true"" autocomplete=""off"">

I cant find element with getElement(), cant type with focus()+keybord.type() or page.type().",1,2024-03-09 10:00:29
Could you give me some resources?,"Can you recommend any resources or channels for people new to web scraping? I just wanted some things to consume, like: communities, videos with some tutorials, some good sites for scraping, etc.",4,2024-03-08 17:11:10
Is scrapping the latest tweet from multiple Twitter account is possible?,"I have scrapped twitter few years back. But i heard that since August scrapping X is nearly impossible.
My use case is that i have bunch if twitter handles and i want to get thier latest tweet. 
If i can scrape the last tweet asap thats awesome but i don't mind a bit of delay.
So coming to my actual query that which resources should i look to handle this use case. 
I know python and bit of selenium but i read somewhere that you cant get last post from X. 
Any lead or direction will be super helpful. Thanks .",1,2024-03-08 20:37:25
I need help with web drivers.,I have recently got into web scraping and need some advice. I have installed selenium and now need to install a web driver (chrome). Problem is I don't know which one to install. Chrome version is 122.0.6261.95 (official build) 64 bit. But I can't find any web driver for it on the chrome web driver site. Would appreciate any advice.,0,2024-03-08 17:45:26
Which undetected/stealth headless scraper for multi-processing/threading?,"I am working on a project where I need to scrape product prices in online shops. Still trying to figure out the tech-stack for the project. 
Requirements:

bypass Cloudflare
render Javascript
run in parallel
run in Docker

â€‹
Not really 100% happy with any of the tools I tried so far:
â€‹
undetected-chromedriver:

can run with multi-processing, however, doesn't support proxies with authentification... selenium-wire does, but it doesn't work that well with multi-processing

â€‹
hrequests:

doesn't render Javascript correctly in some cases for me
also the library isn't very popular

â€‹
puppeteer-extra / puppeteer-extra-plugin-stealth:

hasn't been updated for over a year

â€‹
ulixee hero:

very early stage of development and not very popular

â€‹
Happy for any advice. Thanks!",14,2024-03-07 15:42:43
A Service/Proxy for Bypassing Cloudflare and Captcha stuff,"Do you know any good proxy that works well in bypassing cloudflare and captcha, a paid service, with IP rotation or Residential Proxy or anything that works?",5,2024-03-07 17:26:49
Paid project put on back burner..,"So admittedly I  have only web scraped a handful of times, I normally pull my data from databases and load them in their desired location. In this situation I had a client reach out that wanted data (current and potential customer) pulled from one CRM and placed somewhere else. This would run nightly updating the newest customers, the best way for me to do this was web scrape via python script. I was able to extract the exact data I needed and was working on the second portion of my process, after reaching out they informed me that ownership had gotten an alert on back end work and they were not happy, and there would be extra charges if this continued. This is now on the back burner till further notice. My question, I read term and services and no where did it mention anything about data scraping or requests...Is it best practice to reach out to the company with your intentions? Is this something that the owners should do or myself? I am just trying to see if this is something I can avoid in the future.",1,2024-03-07 18:58:04
How to search for a term within a Twitter list?,"I was referring to twitter documentation and it says we need to search on that way to find all tweets from that list:
list:NASA/astronauts-in-space-now 
I suppose to find a term within that list we just need to add the term in the command, so I tried that for the term 'earth':  
list:NASA/astronauts-in-space-now earth 
but both search commands didn't work, it keeps showing other contents even from accounts outside the list. For the first command it even showed some porn result.  
Did it change? Does anyone know how to do that?",1,2024-03-07 15:35:40
"How to hack websites behind WAF, cloudflare, akamai, imperva","Report
https://drive.google.com/file/d/1RdssR9XpbQGVSaWtmyvZP_jeN7T0CQjN/view
Hello Everyone, I found a way to bypass these WAF systems, they way to bypass them is to get the real IP from the server
So this is before:
â€‹
https://preview.redd.it/m12zugrx1qmc1.png?width=1750&format=png&auto=webp&s=805d1b49f7cf6f98f8f9c909a215104d30d21730
This is after:  
https://preview.redd.it/4umg8vzy1qmc1.png?width=1506&format=png&auto=webp&s=fff446748d0b8dd84c9150db1130847d21a8caf5
The fundamentals  to get the real IP is to send HTTP request to every possible IP until the real server responses back.
The full report is here:
you will need to have Go installed on your systems, here its is the code:
https://github.com/johnbalvin/marcopolo/
Btw, this is my first time making reports like this , so be kind.
I'm probably not following any good design pattern, also I don't have enogh experience teaching, so probably the videos won't have a good audio, or good teaching practices.  
This is not just for ""hacking"" but it's also to create web scrappers using the real IP from the host",72,2024-03-06 14:13:08
Best way to scale up my Playwright-Python webscraper?,"Hey all
I have a Playwright-python script set up that asynchronously runs several headless browsers in parallel on one machine. The input is a csv file containing a list of URLs to crawl on. The URLs need javascript to display properly so scraping a single page takes around 5 seconds each in an ideal situation.
The issue is, that I want to scale this approach up to get through more URLs faster. My understanding is that the best approach is to parallelize the work across several instances, but I have never attempted this before and am a bit lost on how to proceed.
â€‹
I have access to AWS and all its features for reference. Any starting points would be helpful.
â€‹",7,2024-03-06 14:57:59
How to scrape the videos of a private online course?,"Hello recently, i've bought a 2600$ online course, with different modules and videos with my friend, i paid for about 1000 among it and my friend payed the money left. My friend is the only one with the logins.
And he dont want the staff to know that 2 people bought it, and potentially ban us  for it. 
We would like to scrape the videos in the course and download it then transfer it to my pc in different folders so i can follow it peacefully, without having to lose a grand invested.
I have no idea how to do that so far, i belive it's doable. 
Could a kind soul help me out, or redirect me",1,2024-03-06 22:57:33
Scraping job boards (failed miserably),"I'm currently working on a website to aggregate jobs from various job boards such as indeed, wellfound, ycombinator etc. 
I'm using beautiful soup to do the scraping, using css selectors to get the elements.
But if I try going to the actual sites to scrape, it either blocks me or gives a 404 error. How should I change my approach to this (currently a beginner), should I use selenium and automation tools, is the lazy loading affecting my approach.
I'll be grateful for any tips you have.",1,2024-03-06 18:21:36
"Scraping old YouTube archive ""Related Videos"" for specific keywords?","I'm a noob to this - I wanna make a Python script that scrapes through the related videos section in a set of multiple old YouTube archive links and looks for mentions of videos created by a certain user or videos with a certain word in the title. I'm not sure how to do this.
( example page: https://web.archive.org/web/20070601075842/https://www.youtube.com/watch?v=jNQXAC9IVRw - Let's say I specify that I want videos with ""Jaguar"" in the title, or videos from the user ""alokhorst"". It would spit out the link to this archive and tell me what was found, something like that)",1,2024-03-06 12:10:22
"Looking to quickly archive some lost websites from the wayback machine however when I try to pull via wget I don't seem to be getting a full backup, some links are missing and not all css is coming with, it's more complete on IA's side. any help is greatly appreciated.","Command being used,
wget --recursive --no-clobber --page-requisites --convert-links --domains web.archive.org

I've also tried using this Ruby Project on Linux, every attempt it's pulling the html data back corrupted (possibly encoded?), I'm using a DNS service that isn't rate limiting me, and have even modified the code to use ""sleep3"" per download initialized to prevent Wayback machine from rate limiting me.
https://github.com/hartator/wayback-machine-downloader
https://github.com/hartator/wayback-machine-downloader/issues/273
only was able to recover the web images, surely there's a better way to pull entire websites down locally?
I see web services that claim to do this perfectly for a fee, but their software must be similar?
https://archivarix.com/
what's the magic I'm missing here fellas, I'm new to Web Scrapping.",1,2024-03-06 06:24:03
Can websites spot crawlers by no-mouse-movement?,"Hey folks, I've been wondering if websites like LinkedIn can spot crawlers due to entirely ""keyboard-like"" behavior. As a web developer myself, I understand that websites can detect mouse movement on their pages but do you think LinkedIn would have something crazy that can detect the ""type"" of mouse movement (including acceleration and movement paths) to distinguish between a crawler and human?
I tried my hand at Selenium but see that some websites throttle me even when I'm at a tad slower pace than a human. Perhaps their bot detection system is able to spot tha difference as bot interactions = no mouse movement?
All of that said, my idea is to perform some healthy crawling for self-use without abusing web servers and also nothing commercial.",2,2024-03-06 01:30:39
WebScraping Spotify,Can someone help me do web scraping of artists on Spotify safely? I specifically need to get the monthly listeners,1,2024-03-06 04:23:49
"Scraping 100,000 product pages from a single website: what's conceptually the strategy?","I'm using Python and have a bunch of Premium proxies. I want to scrape product details, prices from a dozen or so websites. Each website has about 100,000 products in my niche.
But how do I structure my workflow?

I do not want to hammer their site with requests (play nice)
I do not want to crash my own system because I am making too many requests at once (if that's possible)

Conceptually, in my head, without any experience, I would think that:

Segment the 100,000 URLS in batches per CPU core
I though that parallel woud be best, but the actions are not heavy in computing, so maybe async/await is fine?
Add some delay (for the not hammering part)

Can you share some conceptual examples of this workflow?",15,2024-03-05 11:12:26
How to Not get DETECTED by social media? I use 4g proxy and stealth browser! Rate my setup,"I use gologin as stealth browser
I use 4g mobile proxy 
Usually Not do more than 200 ip call for account
Usually scarpe 500 users a Day using different social media and different account and ip....
I use Nodejs, Playwright and Firebase
Should i change tÃ² puppeteer stealth ?",9,2024-03-05 09:36:46
I'm looking to scrape google maps for new service businesses,"I want to scrap google maps for new service based businesses such as tow trucking, landscaping, plumbing, hvac etc. Can anyone lead me in the right direction towards a scraper that does this, so the businesses it scrapes do have to be new. Maybe there is something on github or someone here might have an answer, thank you!",1,2024-03-05 21:01:20
WebScraping in no GUI environnement,"Iâ€™m working on a project for real estate, i started scraping data, started from facebook marketplace, and it was successful.
I used browser driver, and worked with chrome ( because it is installed on my laptop ).
But Iâ€™m wondering if someone else wanted to use my script and he doesnâ€™t have any type of browsers , for example someone using Arch, Debian, CentOs, would it be feasible ?
If it is, how i can do that !",1,2024-03-05 20:28:53
What are the best selectors to use when scraping Google search results?,"I am building an SEO Chrome extension for a client that will scrape search results and display them in a formatted window. I just want to know which selectors Google doesn't change often, such as classes or data IDs, what I should use. If anyone have experience of building such thing would love to know about their experience",2,2024-03-05 15:45:26
Fetching addresses from geospatial view,"I am planning on fetching all addresses from below URL for training a downstream recommendation system.
If you open below link, zoom in and them click on any property you can see a small popup with address. When we click it sends a get request to API with x & y points and brings the address. Problem is I cannot find all the addresses without brute forcing X & Y.  
https://www.planningportal.nsw.gov.au/spatialviewer/#/find-a-property/address 
Is there any other way to bring in addresses? Any service or API.",1,2024-03-05 18:47:08
google ReCaptcha - no Api,"I am trying to scrape data from this website. https://servicesenligne2.ville.montreal.qc.ca/sel/evalweb/index
where I enter the lot number through ""Lot rÃ©novÃ©,"" for example: 1381520. 
It will take you to the property details webpage. 
The issue is that the site is using Google ReCaptcha to check every request; there is no API. 
Is there a way to scrape data without using Selenium?",2,2024-03-05 11:48:12
I was using scrapy to web scrap a page but the css selector always gives empty output,"IDK if it's a security  thing or me making a mistake 
I do Css selectors in the terminal and it yields  nothing",2,2024-03-05 11:22:46
Scraping products from shopee using python requests returns 90309999,"Since 2022, I was able to scrap shopee listings using python requests with headers User-Agentand From only. However, since last week, I've been receiving error code number 90309999 from {""is_customized"":false,""is_login"":false,""action_type"":2,""error"":90309999,""tracking_id"":""583f2b10-6b13-4402-8b3b-a6b8f0fead62""}
Also, I've checked this stackoverflow thread and added headers x-api-source and af-ac-enc-dat, but isn't working. Any suggestions?
from requests import Session

base_url = 'https://shopee.com.br/search?filters=9&keyword=funko&locations=Nacional&noCorrection=true&page=0&sortBy=relevancy'

headers = {
    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0',
    'From': '',
    'af-ac-enc-dat': 'null',
    'x-api-source': 'pc',
}

with Session() as s:
    s.get(base_url, headers=headers)",1,2024-03-05 03:43:17
"Scraping customer logos / testimonials from company websites, is this legal?","Is it legal to scrape company logos from websites? Similarly, company names from client / customer testimonials, case studies, etc. I know a lot of the time companies don't consent to their logos being used on websites but case studies and testimonials obviously require their consent/participation.",1,2024-03-05 01:19:32
Best lenguage to create web scraper  100 instagram users Daily,"There are some cheap scrap services in the marketplace
no point on pay for  software when I can do that by myself, only users no point on adding other targets when there's still room to improvement.
What i want acchive Is :
the flow would be the next Scrap x amount users from your targets -> Follow only from that list and move them to another list once you've followed
Once you've followed everyone in the list scrap new users and delete the duplicates from both lists, with that you're always following recent followers from your targets and removing duplicates/followed people
Best lenguage to create web scraper  100 instagram users Daily?
Puppeteer stealth extra or playwright?",3,2024-03-04 19:11:03
Linkedin Automation Bot with every possible scraping! (2024),"Built a LinkedIn scraper for automating connection requests, follow messages, profile visits, post likes, profile endorsements, and more. Find onÂ GitHub
If you want it hosted on Railway, here's aÂ docker image",2,2024-03-04 09:24:16
"List of links, a pdf within each link","Hi i have a list of links
and in each link there's a pdf in there that needs to be download  (not a direct link)
what should i use to do this in batch
thanks",1,2024-03-04 14:26:45
Test exams with the correct answer from a website,"I am wondering if its possible to get all the questions and correct answer of this website and the easiest way of doing it: 
https://lifeintheuktestweb.co.uk/test-2/ 
There are 24 exams, I need them just to study, copy paste will take me ages. 
Thanks for your help.",1,2024-03-04 12:21:51
Need help with building webscraper for job board site,"I hope I can post this. but i need some help to build a webscraper for a job board I have. I used to use automated point and click tools, but the job board I get jobs from obviously has done some retooling and either I get a 404 page when i load the saved job search page in my automated program, or all of the pages are just repeats of the same 100-200 jobs. If i go to the same page on another computer then I see the jobs as they should be listed with the dates going down as the jobs get older. I'm looking for a pretty automated setup if anyone can point me in the right direction. I'm trying to give as much info as I can without getting my post removed. Thanks. BTW the job board is flexjobs",4,2024-03-03 21:36:21
"New to webscraping, would appriciate Rstudio help.",Sorry if this is not the correct place to post but I was l wondering if anyone had any knowledge on how to use a proxy service with RStudio?,2,2024-03-04 01:21:37
What to do when View Page Source does not show the data I want to scrape?,"Hello - I am fairly new to web scraping, but thought it would be fun to try to figure out how to scrape some data from a trading site in a game I'm playing (Elder Scrolls Online).
For reference I am using the requests module from python to try to scrape search result data from here:https://us.tamrieltradecentre.com/pc/Trade/SearchResult
My goal is to be able to look up a specific item to see if there are any new results since the last search.
The problem I'm having is that the requests module does not return the data from the search result which seems to be injected into the page in a way that is not 'scrapeable'.
When I click 'view page source' on a page with results (for example this one: https://us.tamrieltradecentre.com/pc/Trade/SearchResult?IconName=staff.png&ItemID=25784&ItemNamePattern=Phoenix+Moth+Restoration+Staff), the ' search-result-view ' shows as empty even though there are clearly results on the page and if I use inspect I can see them.
I'm trying to understand why this occurs - if perhaps this is intentionally designed to prevent people from scraping site data? I know I could use selenium to visit the site manually and get the data but it's a lot slower and more overhead so just want to see if anyone could help educate me a bit on why this approach doesn't work.
â€‹",3,2024-03-03 18:50:17
Sport Betting API,"Hello, Iâ€™ve built an api with websocket to get real time odds & events has also an endpoint for pregame.
I do the major of sport nba, nhl, etc.
I was wondering how much would this be worth per month?
Should I also make the api send the a telegram bot and sell this as another service too?
Thanks",3,2024-03-03 18:07:46
Private FB Group Post Comment Scraping HELP!,"I have a large Facebook group, we produce merchandise.   
We wanted to produce an item that included as many members as possible so we asked members to drop a comment on a post to be featured on the tee design - 3.6k people commented   
I need to now somehow scrape all those names from the post and export them into a list so they can be used in the design.   
Can anyone help with this? Thanks in advance",1,2024-03-03 23:39:55
Trying to use selenium and chrome driver to take screen shot of googles search result.,"Hi. Iâ€™m not a coder but had ChatGPT made me a python script to do a search on Google, then take a screen shot of the results and save it to a file. After a little while it blocks me with a captcha. Is there a way around this? If I used googles api would it allow me to look at search results and take a screenshot. Iâ€™m very new and trying to learn.",4,2024-03-03 15:16:38
Creating a job board for specific job titles,"I'm quite new to this area but wondering if some of you could guide me.
The idea I have is to create a job board for myself which searches the web for a specific job title every 15 minutes > puts the result on a sheet or some location that I can find
I'm also based in Europe so want to sort by location if I can. My initial thought was to run advanced searches on Google like the ones below and automatically add them to Google sheets:
""sales manager"" site:jobs.ashbyhq.com
""sales director"" site:jobs.ashbyhq.com
""sales manager"" site:apply.workable.com
""sales director"" site:apply.workable.com
For a bigger version of this I was thinking to collect career page URLs on Linkedin for the companies that hire in Europe. 
Can this be done? If yes, can you tell me where to start? The goal is to find jobs as soon they are published and apply to them on companies' career pages. Willing to spend some money on 3rd party tools like Zapier and Make. I have access to Microsoft Power Automate, Copilot, ChatGPT if that helps.",6,2024-03-02 13:30:39
Which VPN service allows users to connect programmatically by python?,Most of popular VPN services come with GUI software. This isn't what I'm  looking for. I need to be able to connect to their VPN service by  python programming language. I need to connect and disconnect several  times to multiple different VPN servers around the world. Are there any  services that come with an API for python?,1,2024-03-02 17:17:12
How to use playright py with vercel,"Where or how, ""playright  install"" should be writen so that I can install it in vercel, and I am using flask",1,2024-03-02 15:48:33
Open street map,"Hi, I need help with scraping data from the map after pressing the button, request is sent and now I would like to know how to get the BBOX value to make 1 request for all possible points.
https://internet.gov.pl/geoserver/public/wms?SERVICE=WMS&VERSION=1.3.0&REQUEST=GetFeatureInfo&FORMAT=image%2Fpng&TRANSPARENT=true&QUERY\_LAYERS=address\_points&LAYERS=layers&INFO\_FORMAT=application%2Fjson&FEATURE\_COUNT=5&I=229&J=30&WIDTH=256&HEIGHT=256&CRS=EPSG%3A3857&STYLES=&BBOX=2659855.7102675624%2C6585814.357050786%2C2660008.5843241327%2C6585967.231107356 
it returns 
{
""type"": ""featureCollection"",
""features"": [
{
""type"": ""Feature"",
""id"": ""s_lubelskie_address_points.3856808"",
""geometry"": {
""type"": ""Point"",
""coordinates"": [
2659993.05772531,
6585948.52715873
]
},
""geometry_name"": ""geometry"",
""properties"": {
""terc"": 604011,
""simc"": 987800,
""city_name"": ""HrubieszÃ³w"",
""streets"": 17011,
""street_name"": ""field_street"",
""house_number"": ""28A"",
""summary"": 4
}
}
],
""totalFeatures"": ""unknown"",
""numberReturned"": 1,
""timeStamp"": ""2024-03-02T13:46:48.149Z"",
""crs"": {
""type"": ""name"",
""properties"": {
""name"": ""urn:ogc:def:crs:EPSG::3857""
}
}
}
and I only need the id from this",1,2024-03-02 13:48:31
Evaluate the maintenance cost of a scrapper in terms of changes in the website source code,What is the way to go if you want to evaluate how often things change on a website you are interested in crawling?,2,2024-03-02 09:11:32
How much do you make from web scraping?,I'm pretty skilled at web scraping. Can you make living from web scraping?,21,2024-03-01 15:45:36
Getting detected using all measures,"Hi! So I'm completely lost. I am trying to scrape leboncoin.fr, and I'm using playwright (python) and 200 rotating proxies. I make a new chromium browser for EVERY request, have the correct headers, I'm using playwright_stealth and checked with several bot detection tests. However, I still seem to get detected 98% of the time. Any suggestions?",4,2024-03-01 22:33:15
Question on too many requests error: 429,"Hi guys, 
I'm scraping a website that requires login, and it will return 429 errors (too many requests error) when I scrape for 60 pages consecutively.
For your info, I am using python library requests to run this scraper synchronously.
An interesting finding is that every time I faced a 429 error, I could reset this via clicking the ""Disconnect and delete runtime"" and I could scrape the website again for another 60 pages. 
Not sure if I could automate this ""Disconnect and delete runtime"" for my web scraper. I don't mind to pay a fee for Google Cloud or proxy. Just that my budget is around $3 monthly because it's just personal hobby project.
Thanks in advance.
https://preview.redd.it/mmlkmz7lrtlc1.png?width=1450&format=png&auto=webp&s=ba87fec95a8cf6112af091591ff871460b34550b
Found this reference but not sure which scraping techniques is suitable for my case: https://stackoverflow.com/questions/22786068/how-to-avoid-http-error-429-too-many-requests-python",1,2024-03-02 01:42:49
How to properly use multi-threading for web-scraping?,"How do you approach this?
In Python, it is easy to use multiprocessing-package. In Node things get a little more complicated.
Is it the recommended way to spawn processes from your code or would you rather build a scraper that runs only one process and instead run this scraper multiple times on the same machine?
How to make sure that the CPU power is actually put to good use on Linux? Or inside Docker?
So many questions! I hope some of you already went on this journey and care to help out. Cheers :)",5,2024-03-01 14:25:42
Use-cases of web scraping,"What is your primary use-case for building web scrapers? Most of my scrapers have been for tracking odds on sports betting sites, but I imagine the most common use-case is probably e-commerce price scraping since there's no other real option for this. However, the articles I've read put lead generation at the top, which seems silly to me since there are plenty of lead gen services that aggregate data from thousands of places, and would probably be more economical and trustworthy than building/maintaining a ton of scrapers.",8,2024-03-01 04:54:11
Puppeteer on AWS - 15 seconds to execute a simple scrape of 1 URL. How can I speed it up?,"Hey guys. My code simply visits a website, scrapes the first 200 words of text, then itâ€™s done. 
This is currently taking in excess of 15 seconds. And if I add another URL in (E.g the same website, but navigating to the /blog) - it takes up to 30 seconds. 
I can share my code/answer any questions. Iâ€™m using @Sparticuz to deploy it to Lambda from an S3 bucket.",2,2024-03-01 11:44:49
"Mass web scrapping for a company, am I doing it wrong ?","I did some web scraping freelance job for a company few years ago.
The job was to scrap 10-20 public websites to get public tenders information. It was very much straight forward, I used selenium, xpath, click on some buttons, done.
They contacted me again, and this time, it's ~1000. I have some budget, so I can hire people on upwork to do the scraping scripts and focus on a software architecture to handle all the scrapers (which is my main skill, not scraping).
But, I realized that I didn't knew about scraping that much. I know selenium, but that's about it. I didn't used things like beautiful soup, etc...
I always assumed that selenium was the only good solution, because it's running in a web browser, loading the js, and I can click on buttons.
I tried beautifulsoup and scrapy, and it seems to be the case. I can't click on buttons and interact with the webpage.
But I am feeling that I might missing something. Am I ?",15,2024-02-29 22:26:30
Scraping reactive content (button click),"Hello,
Iâ€™m attempting to harvest pricing from petco, using python and hrequests.
Iâ€™m running into a problem where the website loads the pricing data via html and a reactive component button alters the price when you select 1 month, 6 month or 12 months of product.
Is there away to toggle this using requests? Or am I better off using a browser emulator?
https://www.petco.com/shop/en/petcostore/product/heartgard-plus-chewables-for-dogs-26-to-50-lbs
Thanks!",1,2024-03-01 07:27:37
"Webscraper, Bot, or Automation?","I'm looking to figure out what the hell to call my thing lol. The context is this:
I've been building basically an automation that does in fact collect data using Python playwright, and I had a long conversation with Google's Gemini about what to call it.
It called it a bot, but it's not the full automation. 
Basically, I'm trying to find a clean separation between the part that collects data and take actions on the website and the part that'll actually do automations.
What term would you use for something that doesn't ONLY scrape data, but also can take actions on a website? Like ""MySite.apply_to_job(job)""?
Gemini calls it a bot, but to me a bot could have more things like a database and what not built in.",1,2024-03-01 07:11:44
"Just joined, still a noobie.","Im just an enthusiast, learning all of this in my spare time. Ive probably spent about 300 hours or so now studying c, c++, html, python and such as well as writing my own code.
My main project is to build an ai capable of scraping charts, and the internet to find patterns and trends. Then having it choose stocks based off of those predictions and executing those trades through my preffered trading platforms api, then selling once they either succeed or fail.
Scripy and beautiful soup are my preferred tools for making spiders. 
My ideal plan  is to use them with a set of functions defining the strategy i used myself to make money, as well as a set of deep learning to improve upon that strategy.",10,2024-02-29 18:41:20
Web scraping in excel/google sheets - as a total beginner,"Hello guys, I'm in the e-comm business and recently i'm thinking of more and more automation.
I generate 50-60 tracking numbers daily and they contain name, phone number, tracking number and product. I would like all this data to go into an excel/google sheets automatically.
I have no experience in coding ( besides a little bit of HTML, CSS and JS ) and it's my second day of thinking about this ideea.
Would this be hard to do ? What tools should i use ?",12,2024-02-29 14:03:58
best way to go around nginx rate limits?,"I am scraping a website from which I know is using nginx and rate limits each IP addresses to 2MB, at the moment proxies are a little scarce, what I love about this site is it does not block your IP, there's just a speed limit, how can I go around this?",1,2024-02-29 16:21:02
Built A LinkedIn Scraper,"LinkedIn scraper that scrapes specific profiles or scrapes multiple profiles based on a keyword.
Check it out scraper",6,2024-02-29 04:25:55
Facebook Post liker list scraper,"Hi guys, I'm on a project crawling a Facebook post's liker list data.
I've been using the mobile web version and it was fine the whole last year. But recently, Facebook is getting tough and blocks my account. I don't crawl like crazy, maybe around 1k liker per 30 minutes with a residential proxy. 
Anyone knows a more solid solution to this? Thanks",1,2024-02-29 03:31:06
Tweety project - 24/7 Twitter Scraper,"Hi all, I run an OnlyFans Agency, and have an ongoing Twitter Scraping project. Aim is to scrape a subset of profiles as close to 24/7 as possible and action each one.
Recently I built a pretty manual Twitter scraper which opens chrome browsers to get elements. Realising how labour-some that is, I got a dev from Fiverr to build an API scraper for me.
He built a code using Tweety, which supposedly scraped twitter 24/7 and notifies me when one of the target users posts a tweet.
I'm hoping to find someone who has experience with Tweety who can help me understand how to use this code - happy to share",4,2024-02-28 18:21:36
Any demo website that blocks scraping bot by IP,"Hi guys,
I am learning scraping, and I did some scraping previously mainly via python library requests. What I usually did is to find the AJAX API from the website and scrape the data from it.
Personally, I haven't had encountered IP blocking unless that site requires login access to get the website.
Any demo website which I could learn scraping and bypass IP block?
Thanks in advance.
(Notes: I know some website like quotes.toscrape.com and httpbin but they're not blocking me if keep scraping with same IP or account login)",1,2024-02-29 01:00:38
Advanced Web Scraping With Python: Extract Data From Any Site,"Learn how to manage cookies and custom headers, avoid TLS fingerprinting, recognize important HTTP headers, and implement exponential-backoff request retrying in Python!
â€‹
https://jacobpadilla.com/articles/advanced-web-scraping-techniques",19,2024-02-28 00:34:07
Manual email alias list,"I've started webscraping with one specific site, for which I needed a ton of mail accounts. Since this was one of the hardest parts and I didn't and still don't have much experience I opted to create mails manually using different services. Here are my experiences.


Service
Free Mails (Amount)
Paid Mails
Notes



Apple Icloud+
0
Unlimited (0.99$)
Quickly detects fraudulent use and blocks incoming mails by specific addresses.


Mozilla Relay
5
Unlimited (0.99$)
Free tier does not allow answering mails. Might be considered low quality as it's an obvious spam address


Duck Mail
50 - 500
No paid tier
The privacy G.O.A.T comes to our rescue once again. By far the best service, but I feel a bit guilty using too many of this because of this exact reason, as they will slowly get blacklisted on popular sites (Twitch already did so). Also don't create too many aliases at once.


Proton Pass
10
Unlimited (2.39$)  Around 5 aliases / min
You can't verify your forwarding address and relaying from your Proton Mail to another provider is Pro ""Mail"" Tier only (3.99$)


Yahoo
0
500 (5$)
Working with Yahoo always feels a bit sketchy. Forwarding is a bit harder, since you need to use IMAP, but it should do the trick with Pro tier.


Outlook
10
10
High quality mail addresses, very hard to create. You get blocked very fast, but it works.


Gmail
~1000
~1000
You can create Gmail aliases by adding random dots in your address. What you might consider is GOOGLE and how easy it is to detect.


33Mail
Around 10 mails / hour; Limited bandwidth
Higher rate limits (1$ - 50$)
You will get your own subdomain, for example mail(@)brentspine.33mail.com. Very limited bandwidth and no replies included.",2,2024-02-28 12:42:39
"Is there a way to bypass the ""Unauthorised Client"" error while webscraping?","""https://www.99acres.com/api-aggregator/discovery/srp/search?area\_unit=1&platform=DESKTOP&moduleName=GRAILS\_SRP&workflow=GRAILS\_SRP&page\_size=25&page=2&city=15&preference=S&res\_com=R&seoUrlType=DEFAULT&recomGroupType=VSP&pageName=SRP&groupByConfigurations=true&lazy=true""
I could previously scrape this website, however I'm getting this error all of a sudden.
{
""message"": ""Unauthorized Client"",
""responseStatus"": ""UNAUTHORIZED""
}
Is there a way to bypass it?
The code that I'm using -
#Scrape the Data

chromedriver_path = 'E:\\chromedriver\\chromedriver.exe'
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument(f""webdriver.chrome.driver={chromedriver_path}"")
driver = webdriver.Chrome(options=chrome_options)


#Load the URL

scrape_prompt = input(""Should you scrape the data? \n1. Yes \n2. No"")
if scrape_prompt == ""1"":
Â  Â  print(""The scraping process has begun"")
    url1 = ""https://www.99acres.com/api-aggregator/discovery/srp/search?area_unit=1&platform=DESKTOP&moduleName=GRAILS_SRP&workflow=GRAILS_SRP&page_size=25&page=""
    url2 = ""&city=15&preference=S&res_com=R&seoUrlType=DEFAULT&recomGroupType=VSP&pageName=SRP&groupByConfigurations=true&lazy=true""""
Â  Â  headers = {
Â  Â  Â  Â  Â  Â  ""Accept-Language"": ""en-US,en;q=0.9"",
Â  Â  Â  Â  Â  Â  ""User-Agent"": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"",
Â  Â  }
Â  Â  
Â  Â  page_number=0
Â  Â  all_data = []
Â  Â  data_list = []
Â  Â  previous_data = None

Â  Â  max_pages = 3000
Â  Â  for page_number in range(0,max_pages+1):
Â  Â  Â  Â  base_url = url1 + str(page_number) + url2 Â  Â 
Â  Â  Â  Â  response = requests.get(base_url, headers=headers)
Â  Â  Â  Â  
Â  Â  Â  Â  if response.status_code == 200:
Â  Â  Â  Â  Â  Â  data = response.json()
Â  Â  Â  Â  Â  Â  all_data.append(data)
Â  Â  Â  Â  Â  Â  print(f""Page {page_number} data retrieved."")
Â  Â  Â  Â  Â  Â  # Check if the data of the current page is the same as the previous page
Â  Â  Â  Â  Â  Â  if previous_data and all(prev_item == curr_item for prev_item, curr_item in zip(previous_data, data)):
Â  Â  Â  Â  Â  Â  Â  Â  print(f""Data retrieval is complete."")
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  previous_data = data
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  print(f""Failed to retrieve data for page {page_number}. Status code: {response.status_code}"")
Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  time.sleep(3)
Â  Â  Â  Â  page_number +=1",1,2024-02-28 11:33:43
Best tools for stealth/undetected web scraping in 2024?,"I'd like to develop a tool for price tracking of certain products. This will be a big project, so my choice for the tools I will use is important. I am happy about any hints, thank you!
Here is what I gathered:
Javascript:
puppeteer-extra-plugin-stealth

ðŸŸ¡ last updated 1 year ago
ðŸŸ¢ high popularity
+ ghost-cursor

ðŸŸ¢ last updated 1 week ago


puppeteer-real-browser

ðŸ”´ low popularity

Python:
undetected-chromedriver

ðŸŸ¢ last updated 1 week ago
ðŸŸ¢ high popularity

selenium-stealth

ðŸ”´ last updated 4 years ago
Python

â€‹
Which tool do you think is best for scraping websites that are protected by Cloudflare or similiar tools? Which of them is better regarding configurability?
If you were to start all over now with your project - which route would you take?
THANKS!",40,2024-02-27 15:13:21
Reliable non dedicated phone service,"I need to create many PVA (phone verified account) bots for my scraper. Is there a service that allows me to verify a lot of accounts for a specific site? I will not have to use the number outside of this one time use, so buying dedicated numbers would get very expensive very fast. Is there any website I can try out that you might be able to vouch for?",1,2024-02-28 09:42:20
Is it illegal to write code that just replaces me clicking like a monkey every day?,"â€‹
I've written a couple of very simple node js / playwright scripts to get interesting car deals and one for searching scientific papers.
They aren't used in any commercial way.
I know about the ""robots"" field in the websites' manifest, but... is this automation (i.e web scraping) merely for personal purposes illegal?
I am in the UK (but can easily use a VPN, although I doubt this changes anything ?) 
I unfair for this to be illegal, since it's just ones' automation of typing. 
What is the reality?
â€‹
â€‹
â€‹
â€‹",59,2024-02-26 23:15:57
"How to handle intermittent clicking behavior in Selenium where sometimes the click works and sometimes it doesn't, without throwing any errors indicating that the element to click is not found?","from selenium import webdriver
from bs4 import BeautifulSoup
import time
import requests
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.keys import Keys
import random
options = webdriver.ChromeOptions()
options.add_experimental_option(""detach"", True)
options.binary_location = ""F:\webdrive\chrome-win64\chrome-win64\\chrome.exe""
PATH = r'F:\webdrive\chromedriver.exe'
#driver = webdriver.Chrome(PATH)
service = Service(PATH)
driver = webdriver.Chrome(service=service, options=options)
# Adres URL strony
driver.get(""https://www.youtube.com/results?search_query=abcde"") 

def alwayssleep():
     time.sleep(10000)
     return """"
def firstvideo():
    result = driver.find_elements(By.CSS_SELECTOR, ""ytd-video-renderer"")
    result[random.randint(1,5)].click()
    return """"
def cookie():
    driver.find_element(By.XPATH,'//*[@id=""content""]/div[2]/div[6]/div[1]/ytd-button-renderer[2]/yt-button-shape/button').click()
    return """"

def videodata():
    views = driver.find_element(By.CSS_SELECTOR, ""span.style-scope.yt-formatted-string.bold"")
    title =driver.title
    ch_name=driver.find_element(By.XPATH,'//*[@id=""text""]/a')

    title=title.replace("" - YouTube"", """")
    #title > h1
    print(views.text)
    print(title)
    print(ch_name.text)
    return """"
def randsleep():
    random_number=random.randint(3,5)
    time.sleep(random_number)
    return """"


print(randsleep())
print(randsleep())
print(cookie())
print(randsleep())
print(firstvideo())
print(randsleep())
print(randsleep())
print(videodata())
print (alwayssleep())

â€‹
â€‹",1,2024-02-27 17:36:21
Scraping Deleted Reddit User Page,"Hello!
Forgive my lack of knowledge. I majored in computer science but have very little web dev knowledge. 
Anyways, I woke up today and found myself in an interesting position. Last night, I was looking through the comments of a certain reddit user, who has suddenly become a person of interest. Maybe you can figure out who it is, but it's not that important. Today, all of the comments are deleted, but I can still access them because I had the page open in Chrome. I donâ€™t know how this stuff works; I can duplicate the tab and still see everything. Iâ€™m worried I'll suddenly lose access to it all.
I'd like to scrape the page as it is in my browser, without a refresh. Is there a way to do this? I've done some googling, but haven't found anything promising yet. Thanks!",0,2024-02-27 16:57:21
Ikea kitchen planner - adjust kitchen and gather new url + price + image using python,"Greetings all,
Background:
Ikea offers a kitchen planner where potential customers can design their own kitchen based on for example their floorplan and personal wishes.
Ikea also offers some pre-signed kitchens that are public. See for example:

I'm from the Netherlands, hence the 'nl/nl' in the weburl, but I bet similar example urls are available for the US website ('us/en').
What is nice about this feature is that it automatically generates 8 example kitchens based on the floorplan that is provided. Hence, when you adjust the floorplan manually (under 'Bepaal je ruimte'), 8 new kitchens will be generated (under 'Kies een favoriet'), each of which has a price and 'standard' image coupled to it (under 'Realiseren'). Creating a floor plan is manual by default and the url within the browser is not updated by adjusting the floorplan.
Question:
How could we use the Ikea kitchen planner to automatically generate 8 kitchens, along with their url's, prices and images after inputing the length and width of the floorplan using python?
Kind regards,
Victor",1,2024-02-27 13:35:24
Need Help with Scraping Detailed Financial Reports from ekrs.ms.gov.pl,"Hello r/webscraping!
I'm struggling with a project where I need to scrape detailed financial reports of Polish companies from a specific website. I'm looking for advice on how to do this more efficiently using Python.
Website URL: I'm aiming to scrape data from the search form results page at https://ekrs.ms.gov.pl/rdf/pd/search_df. Specifically, I'm interested in the results page that comes up after entering a KRS number, like KRS 0000402267 for example.
Data Points: My goal is to extract specific information from the financial reports, such as the company name, KRS number, fiscal year, and detailed financial data presented in the reports, like revenue, profits, assets, and liabilities.
Project Description: I'm working on a financial analysis of Polish companies and need an automated way to gather their financial reports for further analysis. I started experimenting with Selenium in Python, but I'm encountering difficulties related to the dynamic nature of the site, such as AJAX and dynamically loaded data, making my current script inefficient and unreliable.
So far, I haven't been able to find a hidden API that might simplify the process, so I'm open to any suggestions on techniques and tools that could help in this situation.
Could anyone share tips or similar experiences related to scraping sites with dynamically loaded data? What tools or libraries could help with this? What strategies can I apply to deal with the dynamic nature of this site?
I would be grateful for any advice and suggestions.",1,2024-02-27 12:15:40
Webinar Invite: Overcome Web Scraping Barriers,"Unlock the Secrets to Effortless Web Scraping - Join Our Webinar!
Hey everyone!
Frustrated with web scraping limitations? Join NetNut's exclusive webinar and discover how to bypass anti-bot systems like a pro. Our experts, Eitan Bremler and Pierluigi Vinciguerra, will guide you through advanced strategies to mimic real users, navigate CAPTCHAs, and rotate IPs seamlessly.
ðŸ“… Date: March 28, 2024
â° Time: 9:00 AM EST
ðŸ”’ Limited spots available - Secure yours now!
Learn to collect data efficiently without getting blocked. Perfect for data scientists, SEO professionals, and anyone eager to refine their web scraping skills.
Reserve your spot here: NetNut Webinar Registration
See you at the webinar!",1,2024-02-27 10:07:03
Scraping food publications,"Hey everyone,
I'm working on a project where we're trying to scrape restaurant information from hundreds of food publications like Bon Appetit, Spoon University, Eater, The Infatuation, etc...
Right now, we're currently using Python and Selenium to manually build scrapers for each publication. While this works for a few publications, it's not ideal when you want to scrape food publications across hundreds of cities in the US. Do you guys know an ideal solution, service, or software that can help us with this?",1,2024-02-27 00:06:34
Scraping PGA Tour Leaderboard`,"Hello everyone,
I am looking for assistance in scraping PGA tour leaderboards (example: https://www.pgatour.com/tournaments/2024/wm-phoenix-open/R2024003). I would like to be able to pull strokes gained metrics. I have included a screenshot on what I want extracted and the html. Anyone have a good starting point?
â€‹
https://preview.redd.it/i88ywptp1zkc1.png?width=1987&format=png&auto=webp&s=67f40354b6f846cd751c00b3b895853d7e69f9cd",2,2024-02-26 18:18:33
Decode / Decrypt request response,"Hello,  I'm diving into a project where i need to decrypt and understand the data returned by the Australian Open's CourtVision API: https://itp-ao-sls.infosys-platforms.com/prod/api/court-vision/belowCourt/year/2024/eventId/580/matchId/MS701/pointId/0\_0\_0.
API requests like this are make by pages like the AO Open WebSite, this is the page for the  Jannik Sinner vs. Daniil Medvedev match.
In the past people were able to access the CourtVision data to scrap, directly via the API like shown above used similarly in this GitHub Repo (the API used to return JSON)
I'm able to view the data in the website looking at the div.CourtContainer properties for the CourtVision data, however i need a reliable way to build a database of as many years as possible and many matches as possible, of CourtVision data.
If the Frontend is translating the API response to JSON, there must be a way to reverse engineer this decryption, to make a scraper like this GitHub Repo work again.",1,2024-02-26 21:54:17
Need Help with Accessing INCI Beauty Database,"Hi all,
I'm looking to access or collect the INCI Beauty database, which has open info on product ratings and ingredients. Does anyone know of a method to scrape this data or if there's an existing export available? Interested in using it for research purposes.
Appreciate any tips or resources you might have.
Thanks.",1,2024-02-26 14:46:08
"Where can I get Historical Data on the Options index: ""OMX30"".","â€‹
Where can I get Historical Data on the Options index: ""OMX30"".
It it listed on the NASDAQ Stockholm Exchange.
I need data from between 2018-2023, and minimum 50,000 observations.
â€‹
Data needed:
- Daily closing prices of option premiums
- Daily volume of the contracts
- Different expiration contracts (30,60,90,180 day contracts)
- Different strike prices (in-,at, out- of the money contracts)
â€‹
On this website https://www.nasdaqomxnordic.com/optionsandfutures there's single data",1,2024-02-26 13:59:50
Library I Created to Migrate to Cloudflare with Puppeteer,"Hi, I just released a new update. It's free and open source. It is not caught by Cloudflare. I hope it will benefit you.
https://github.com/zfcsoftware/puppeteer-real-browser",10,2024-02-25 17:17:32
Web scrapping,"I want to write a python code to scrape the website  https://www.bls.gov/news.release/cpi.t01.htm  and return value of Food , Gasoline and Shelter at 2023-Jan.2024 and find their average
â€‹
output should be like this
â€‹
Food : 0.4
Gasoline : -3.3
Shelter: 0.6
average is : 0.76
â€‹
Here's my code so far, but I'm getting ""Failed to fetch data. Status code: 403"", any modification in my code? Thanks
â€‹
import requests
from bs4 import BeautifulSoup

def scrape_inflation_data(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}

    # Send a GET request to the URL with headers
    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        print(""Successfully fetched data."")

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find the relevant table containing the data
        table = soup.find('table', {'class': 'regular'})

        # Extract data for Food, Gasoline, and Shelter for Jan 2023 to Jan 2024
        data_rows = table.find_all('tr')[1:]  # Skip header row
        values = {'Food': None, 'Gasoline': None, 'Shelter': None}

        for row in data_rows:
            columns = row.find_all('td')
            category = columns[0].get_text().strip()

            if category in values:
                # Extract the inflation value for each category
                values[category] = float(columns[-1].get_text().strip())

        return values

    else:
        print(f""Failed to fetch data. Status code: {response.status_code}"")
        return None

def calculate_average(data):
    # Filter out None values and calculate the average
    valid_values = [value for value in data.values() if value is not None]
    average = sum(valid_values) / len(valid_values) if valid_values else None
    return average

if __name__ == ""__main__"":
    url = ""https://www.bls.gov/news.release/cpi.t01.htm""
    inflation_data = scrape_inflation_data(url)

    if inflation_data:
        for category, value in inflation_data.items():
            print(f""{category} : {value}"")

        average_value = calculate_average(inflation_data.values())
        print(f""average is : {average_value}"")
    else:
        print(""No data retrieved."")

â€‹",0,2024-02-26 04:27:29
Working Instagram scraper without logging in!,"This my first working Instagram scraper. It works perfectly all you need is a list of Instagram account you want to scrape. 
This is my first major project, and very proud how far I've come in leaning python!. 
I'm open to here about how it can improved.
This is my repository in git hub: https://github.com/jhontotomato/Snapscrape.git 
â€‹",2,2024-02-25 19:59:49
Puppeteer is being detected,"Hi guys,
I have a project that was originally written in Selenium in windows, but later I decided to move it to Puppeteer.  I shifted from Selenium to Puppeteer because I have 3-4 network adapters. I read somewhere that I can select which adapter the program uses in Puppeteer.
I'm attempting to scrape a website, performing actions such as creating an account, signing in, posting content, and messaging someone. I'm using the mobile view feature with page.emulate(phone), where the phone model is obtained from Puppeteer's knownDevices list.I've tried various approaches like Puppeteer Extra, the Stealth plugin, and even using Puppeteer with a real browser, but the website always detects my scraping attempts. This happens even when I manually input credentials and perform other actions.
My first question is, why am I being detected while using Puppeteer? And secondly, can I switch between network adapters when running the browser? For example, running three processes with each one using a different network adapter. Thanks",1,2024-02-25 20:09:57
Web Scrape clothing website,"Hey everyone,
I'm looking to scrape data from Sellpy, an online marketplace that uses Algolia for its search functionality. I'm new to web scraping and could use some guidance on how to approach this effectively.
Specifically, I'm interested in extracting product information such as names, links, measurements, and prices from Sellpy's listings. However, navigating the Algolia-powered search feature presents some challenges.
If anyone has experience with scraping websites that use Algolia or have done similar projects, Id appreciate any advice  you can offer. Any suggestions for libraries or tools to use, insights into Algolia's API, or any tips for scraping dynamic websites, im open for any help!
Thanks!",1,2024-02-25 18:48:51
Some neat Powershell code to traverse tables,"This gives you all the text and the right childnode for that particular text.
$url = ""https://finviz.com/quote.ashx?t=TCMD&p=d""
$page_result = Invoke-WebRequest $url
$html = ConvertFrom-Html $page_result
$tables= $html.SelectNodes('//table')
cls

function TraverseNodes($node, $indentLevel, $tablenumber, $totaltext) {
    Write-Host ("" "" * $indentLevel) $node.innertext.trim()
    Write-Host ("" "" * $indentLevel) $totaltext
    if ($node.HasChildNodes) {
    $i=0
    foreach ($childNode in $node.ChildNodes) {
        $newtotaltext = $totaltext + "".childnodes["" + $i + ""]""
        TraverseNodes $childNode ($indentLevel + 2) $i $newtotaltext
        $i++
    }
    }
}

if ($tables -ne $null) {
    $j=0
    foreach ($table in $tables) {
    Write-Host ""Table:""
    $newtotaltext = ""tables[""+$j + ""]""
    TraverseNodes $table 0 $j $newtotaltext
    $j++
    }
} else {
    Write-Host ""No tables found in the HTML document.""
}",3,2024-02-25 09:49:25
please help to scrape odds,"i have this code
or try any other live running match from this website  and when i try to scrape the odds it gets error :
NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""css selector"",""selector"":""div.pending-odds.ng-tns-c185-4.ng-star-inserted""}
Navigate to the website
driver.get('https://www.satsport.io/x/#/2/home/exchange/event?sid=4&eid=33039646&mid=225125561&mtype=MATCH\_ODDS')  
Find all elements by XPath
elements = driver.find_elements(By.XPATH, '//div[@class=""pending-odds ng-tns-c185-4 ng-star-inserted""]')
odds_element = driver.find_element(By.CSS_SELECTOR, ""div.pending-odds.ng-tns-c185-4.ng-star-inserted"")
print((odds_element))
i guess this is a very small issue . iam trying to scrape this website in google collab using selenium.
can anyone try to help how do i do this using selenium or any other library",1,2024-02-25 15:23:36
Automating Browser processes in background,"Hello Iâ€™m trying to build a web application that does web automation in the background.
Say for example a user enters a product name and submits, this should kick off a background process that opens a bunch of websites and searches for the product, finds and pre-order the product.
Since itâ€™s a web app, I need this to be able to run efficiently without slowing down the server.
Whatâ€™s the best way to proceed with this task and what tool could be able to do this at scale",2,2024-02-25 09:43:40
I have a job that I could easily automate completely with web scraping LinkedIn,"Hi all, I have a freelance job that makes me find specific profiles of people from a list of companies they give me. If I could make a simple webscraper that simply takes all of the employees and a description of their role from a company's linkedin profile I would be able to completely automate my work. I would have to do only 100 or so requests per week. I've tried on my own but LinkedIn's anti scraping tools are too good for me. Do any of you know of any tool online I can pay for to do this for me? Any help appreciated. Its frustrating because it would be such a simple and stupid scraping I would need to do to just automate everything.",4,2024-02-25 03:33:48
Faster than Python?,"I had a webscraper tool built in python & puppetmaster but it is slowwwww. What would be the better option?
Edit: I am creating 100s of PDFs with a product page & a checkout page showing that an item will deliver to a specific location",7,2024-02-24 23:26:27
Scraping Wowpedia,"I am trying to scrape several galleries on Wowpedia for my own archives (mainly art from the Warcraft TCG) but my traditional methods of using python and bs4 are not working, yet this seems to work on other Fandom style sites. WFDownloader does not seem to work either.
Could anyone provide any help and the proper way to scrape? The closest I've gotten is just getting the thumbnails.",2,2024-02-25 00:26:05
Amazon web scraping,"I want to Write python code to scrape amazon data from search results, python code should read each url of search result and scrape the following contents
â€‹
Product Name,
Categories,
Product Description,
Price,
Discounted Price,
Variant SKU Code,
Variant Label 1,
Variant Value 1,
Variant Label 2,
Variant Value 2,
Tax Value (%),
HSN Product Weight (kg), and 
Image links (up to 24 for each product group)
â€‹
and save data in csv file
â€‹
â€‹
here are search keywords
â€‹
flower bulb
flowers seeds -namdhari
imported seed sygenta taaaki bennery asia pacific
vegetabe seed
namdhari seed
flora valley
unique seed
team seed
sakaata
frenchi seed
sunrise seeds
cocopet
Fertilizer -NPK DAP UREA BIOVITA (1KG TO 5 KG)
â€‹
PESTICIDEE & FUNGISIDE
PLANTS
BIGS PLANTS
SESAONL SEEDLING
PERMANENT SEEDLINGS
LAWN COVER PLANTS
CARPET GRASS
ARTIFICFICAL GRASS
SECULLANTS
INDOOR PLANTS
OUTDOOR PLANTS",0,2024-02-25 10:17:18
Storing data,"Hi Reddit. This is my first post here and I would like to ask you for advice regarding data storing. I'm currently working on an image scraping project, so far I'm saving the images to my filesystem and doing some manipulations with them. All data has a certain structure and file architecture. However, I can't decide how to store the images properly after manipulation step. Initially, I thought about storing it in a database, but it seems to me that storing a large number of blob objects is not the best approach. I also thought about S3, however, the project I am working on is academic (related to my thesis) and I really wouldnâ€™t want to spend extra money. Or is storing data on a local file system also a pretty good solution? Does anyone have any advice on what can be done about this?",3,2024-02-24 15:46:02
Finding if thereâ€™s any news article about list of vendors related to fraud.,"I have a list of companies, about 200,000 I want to write a script to find if thereâ€™s a news article about these businesses regarding fraud. 
Basically it would input the name of one article is found, returns true if not returns false. Thatâ€™s all Iâ€™m looking for. 
Reading about scraping google, I see thereâ€™s a lot of work around for that. What else can I do to work around this?",1,2024-02-24 16:31:38
Scraping Large Amount of PDFâ€™s - General Advice,"I have a someone who wants me to extract PDFâ€™s from a judiciary website that contains court documents.
It has a simple captcha that was easy to crack with some binarization and denoising techniques using OpenCV and PyTesseract. The inner HTML, the content I am after, is generated and retrieved via AJAX calls.
The problem is that thereâ€™s an upward amount of 12+ million PDFâ€™s on the site alone which as you all can obviously guess takes a lot of storage to hold all that. 
Anyone have any general advice on what youâ€™d do in this situation?
Website is attached below:
https://judgments.ecourts.gov.in/",11,2024-02-24 02:10:59
Puppeteer supremacy?,"Hello I just wanted to get your guys' opinions on which if puppeteer is the end all of browser automation. From what I've read puppeteer seems to mimic the browser the best, even loading tiktok livestreams which selenium and playwright are unable to do. That is not even including pupetteer extra and stealth plugin. I guess the only wonder is if I can prevent RTC leak on pupetteer as I was unable to do that with selenium and playwright. But it seems like at least from what i've read is that puppeteer runs the game and selenium and playwright are just mistakes who need to be erased from existence.",0,2024-02-24 07:23:10
Need Advice for a Large-scale Web Scraping Project: VPNs vs. Proxies,"Hi everyone,
I'm embarking on a substantial web scraping project and could use some guidance. I'm targeting a website with around 4 million product pages, available in two languages, which brings the total to about 8 million pages. The silver lining is that these pages can be accessed via .json links, offering a way to minimize traffic impact. The site is protected by Cloudflare, but I've managed to bypass this using VPN provider so far.
In my tests, I've successfully run 5 Docker containers concurrently under a single VPN account, each using different IPs. However, this is just the start. I plan to scale my operations to scrape up to 100 million pages. This scaling brings me to a crossroads: should I invest in 3-4 VPN accounts, or would proxies be a better route? I've never used proxies before, so I'm particularly interested in insights about their effectiveness, cost, and how they might compare to using multiple VPN accounts for a project of this scale.
Any advice, experiences, or tips you can share would be incredibly valuable, especially regarding handling large-scale scraping projects, managing IP rotation, and staying under the radar of protections like Cloudflare.
Thanks in advance for your help!",7,2024-02-22 16:51:03
Looking for iMacros add-on alternative for Google Chrome's browser since in June 2024 they will update to manifest 3.0 and iMacros is with manifest 2.0 is no longer supported to be updated?,"Hello,
I'm currently using iMacros add-on for my Google Chrome browser which works fine for the latest version of it. I have some automations recordings I have with it, and it's very fast and light, compared to using something like chromedriver watir automations. And I like that you don't need any special mode of the browser to run it.
I read that Google plan in June 2024 to update their browser to require manifest 3.0 which means many browser add-ons will stop working unless the add-ons are updated to manifest 3.0, and iMacros is end of life and no longer supported.
Will there be another way to continue to use the iMacros add-on despite of that?
Any ideas or alternatives?
Thank you.",0,2024-02-22 16:56:50
Best way to scrape truepeoplesearch without getting blocked,I am using python selenium to scrape through truepeoplesearch.com and also using proxies from anyip.io but still getting detected and cloudflare check pops up after some requests,1,2024-02-22 16:05:31
Cant accept Cookies,"Selenium cant find â€žacceptâ€œ-Button
I cant accept Cookies on immoscout24 via a Click() with Selenium uc. My selectors are fine but still get â€žBad selectorâ€œ errorâ€¦ I tried scroll to view and Click or Switched iframes. Any other ideas?",5,2024-02-22 06:09:19
Making a fetch request to Google search URL,"Hi all! ðŸ‘‹
Iâ€™m trying to deploy a very simple Express.js API that, given a few query parameters, constructs a Google search query and performs a simple fetch request on that URL (since Google pages are server side rendered, you can try this by using Postman and sending a GET to any query url). This works totally fine locally and Iâ€™m able to manipulate the returned HTML, but when I deploy this (using fly.io) my request gets blocked by Google and I get a captcha. 
All I need is the HTML - I do all of the parsing logic using Cheerio and donâ€™t see the need to spin up a headless browser instance (using Puppeteer or Playwright) for this. 
Any recommendations for how to get around the blocked fetch request on my deployed API? Maybe I should try a different deployment strategy? Open to any and all recommendationsâ€¦ been scratching my head all day on this one.",3,2024-02-22 05:32:37
Best way to get a value (mailing address) from a list of URLS,"I have a list of URLS. I need to collect a mailing address that is located on each page. I know there are many ways to go about this, but figure, why reinvent the wheel. So what i'm looking for is a tool where i give it a URL, it spits out the mailing address. Better yet, I give it a list of URLS and it spits out a list of addresses. (Csv, excel, whatever)",3,2024-02-22 04:43:38
No code scraping w/ No Scraping ToS Questions,"Curious on some no-code scraping questions I am having a difficult time verifying on google (for a report Iâ€™m writing, of course). 

Are the no-code scrapers able to be utilized on sites with no-scraping in the Terms of Service? Or do they actively not allow their own program to prevent legal issue?
If you have to log into that site w/ credentials ~ is it pretty much a guarantee that the website is going to be able to tell youâ€™re scraping?
Any scrapers that can go undetected and act very human like? Is it best to scrape low levels, will low levels of data be easily detected? 
Any no-code scrapers that are good for handling a one time scrape that align with items above ?",1,2024-02-22 04:41:09
Guys could you point me out in best direction to periodically get price for several international stocks?,"Title. International stocks (UK, Sweden, Hong Kong, etc)
I just need quote/price. Historical is fine (like 15-30 min delay). Very low call volume (like 10 stocks with 4 calls per day each).
As far as i undestand:
-, apis likely won't work for this. Cause most cover only US. Or some specific international market. Since i need lots of international markets i will need to use several apis (if i manage to find them). Plus international are mostly paid.
-, scraping yahoo finance or trading view seems best option (with such low volume). Am i right?",2,2024-02-21 21:01:07
Local Business Scraping,"Is there a way to scrape local government data bases to find contact information for lead generation purposes?
I've tried multiple Google Map scrapers including Outscraper & most have been shitty.
Other businesses other than what I put come out, foreign country listings appear, wrong phone numbers, etc
Most accurate sources seems to be some sort public registry like county information or something similar",1,2024-02-21 21:11:32
What are some other ways to pass path parameters?,"Hello, I am scraping this website that has a json endpoint. I am able to request the site as https://site/.json and I get returned a json file of the site's content.
I am trying to scrape a page on that site that requires some path parameters such as https://site/?param1=value1&param2=value2, I tried these paths but didn't work:
https://site/.json/?param1=value1&param2=value2
https://site/?param1=value1&param2=value2/.json
I also tried passing those parameters in my HTTP request in a params dictionary, still did not work.
The site has no documentation.
Anyone has any idea how to accomplish this?
Thanks.",1,2024-02-21 19:21:20
How do you notify yourself when something is going wrong?,"In my job, I often have to write small scrapers to gather information. Usually, I can create and deploy them easily across different servers. However, setting up notifications for when something goes wrong is not so straightforward. I prefer using email as a channel to notify myself, clients, and others. But, to set up email notifications, I need to use services like Mailgun or something similar, and configuring them for different domains can be challenging.
Do you encounter similar issues? How do you deal with them? Is there a very simple API available that allows sending 50-100 emails per month without the complex setup required by Mailgun, Mailchimp, or other services?",5,2024-02-21 09:22:42
How do pros handle giant scale webscraping projects?,"I'm interested in learning how you organize your scraping?
I can't imagine people writing scripts for each website themself so there has to be some level of abstraction and streamlining no?
Say you want to scrape a million websites. Do you first download the Html/css/js files and look whether it is static, ssrendered or dynamic to categorize into requests vs render needed? 
Do you cluster Selectors/classes etc and then build out a general scraping script for each cluster?
Maybe all of that is obsolete now and you just take the html/css/js and give it to an AI to spit out a scraping script(?)",18,2024-02-21 00:36:18
Need to understand how influencer marketing platforms scrape Instagram,"I'm developing a SaaS platform for Influencer Marketing, but I'm facing challenges with scraping data from Instagram and identifying influencers. Despite creating tools to scrape and store data efficiently, my Instagram account gets suspended quickly, leaving me uncertain about the next steps. 
However, I've noticed platforms like Modash and InfluencerMarketingAI have vast databases of influencers and offer their data through APIs. Unfortunately, utilizing their APIs is costly for me as I'm operating on a bootstrap budget. Can someone provide guidance on how to proceed? 
Additionally, these platforms offer insights into influencers' followers, including metrics like fake followers and audience demographics. I aim to create a similar system but need assistance on where to begin.",6,2024-02-21 05:16:37
Scraping College Faculty Pages,"Iâ€™ve been reading a lot of history books lately, and sometimes itâ€™s super annoying to find non-pop history books on more niche topics. So I want to scrape ~20 different university History faculty pages for a local database to make it easier to find potential work on Native American history or whatever. 
A lot of them are just static HTML pages, and Iâ€™ve scraped those before but Iâ€™d like to avoid it this time if possible. Unfortunately though, when I go into devtools theyâ€™re not making any API calls as far as I can tell so Iâ€™m sort of at a loss. They all run a bunch of JavaScript files which I think may be executing some queries, but I donâ€™t know the language. Hereâ€™s the University of Michiganâ€™s page as an example https://lsa.umich.edu/history/people/faculty.directory.html 
Is the solve at this point to just suck it up and go through the HTML or learn JavaScript so I can read those files?",3,2024-02-21 07:08:22
Looking for help to scrape an employee only ecommerce website,"Hi, 
I am looking for a method to scrape only the new products added to an employee only ecommerce website.  Actually I am looking for the new listings only.
The new products are added at random times during the day but they have limited sizes and units so trying to find a way to get quick notifications as soon as new products are listed. 
Not sure if scraping is the solution to happy to get any ideas. 
Thanks in advance. 
â€‹",2,2024-02-21 06:36:42
Seeking Advice on Integrating Human Captcha Resolution with Scrapy,"Hey everyone,
I'm currently working on a web scraping project using Scrapy, and I've encountered some challenges when dealing with websites that have captchas. I've been trying to figure out a way to integrate human intervention for resolving captchas within my scraping workflow, and then delegate the control back to Scrapy to continue with the scraping process.
My goal is to create a program that can detect when a captcha appears during scraping, pause the process, prompt a human to resolve the captcha manually, and then resume scraping once the captcha has been resolved.
I've looked into various approaches, such as using external captcha solving services, but I'm interested in creating a more custom solution where the user can interact with the captcha directly.
If anyone has experience or suggestions on how to achieve this integration between Scrapy and human captcha resolution, I would greatly appreciate any insights or pointers you could provide.
Thanks in advance for your help!",2,2024-02-21 04:17:09
Any advice on how to take my scraping to the next step,"Recently I have been using programs such as taper monkey and selenium to scrape. I have explored utilising the XHR api key for scraping when available.
My question really is, how can I learn more technologies as the skills I have now are vulnerable to anti-bot software. Any good books or resources, I have only looked into undetected-chromedriver and proxies but is there anything else I need to know to scrape websites. Currently looking to scrape Vinted if that helps :)",6,2024-02-20 22:30:11
Web Scraping by Selenium,"So I am trying to get a text from this website https://dexscreener.com/solana/7bzzop3qb2zk3r7wqrzjs5fpeeergdy3hgzxxrn97aey There is a button that when you click automatically copies the contact address of the cryptocurrency. I use selenium. Why? Because the website has cloudflare antidetection and after a long time I finally found code that can bypass it. So I need code that would be able to navigate to that page and click that button and print the pasted text. copy button
I only managed to write code for bypassing the cloudflare antibot detection, I tried using chatgpt to write me the code for getting the text but I keep getting errors. I am not that experienced with coding though.
Here is the code;
from time import sleep from DrissionPage import ChromiumPage
p = ChromiumPage() p.get('https://dexscreener.com/')
sleep(5)
https://preview.redd.it/v5qfxdy4tvjc1.png?width=1914&format=png&auto=webp&s=4bc9b143d30064451de942eb7927763903455849",1,2024-02-21 06:21:21
Deduplication via previews,"I am trying to download large quantities of images from Reddit, but many of the images are duplicates. I could deduplicate them by requesting all of the images and putting them in a hash table after they are downloaded, but this would require me to send a ton of requests, and the whole purpose of deduplication is to keep the execution time low in the first place.
Because Reddit loads low-resolution previews of most images before loading the full image, I thought I could find some way of grabbing these previews from the local storage or cache because they have already been loaded in. This would keep me from having to send requests for the images. However, I am unable to find any information on this technique, and I don't even know if it's possible. I would think the data exists in local storage, but I don't know how to get to it.
For reference, I am using a JS script in the Firefox dev tools console.",1,2024-02-21 06:09:53
VPS Provider for high scale scraping project,"I'm looking for a reputable VPS provider for a highly ambitious scraping project. 
When my product is done, i'll be making roughly 3000 HTTP requests per second. I'm aware this will require multiple VPS' but I want to find servers that will keep my overhead low. 
I want the results from the requests to come in near-instantly, and they already do from the endpoint so any other slowness will be coming from my end. 
I need a provider with reasonable prices that can give me unlimited bandwith and ideally 1-5gbps internet speeds. 
It can't be less frequently than per second, and I will be using caching to lower the load on my servers with redis. 
I've done a lot of research and for pricepoints OVH looks okay but i've heard horror stories about their customer service. IONOS also offers the specs and pricing I need but it caps off at 1 gbps. 
Looking for any better options, i'm curious what you guys use. 
Requests can return anything from 50-500KB in JSON data, averaging probably the 300 range.",1,2024-02-21 02:31:37
Need Help for Web Scraping a Government Website,"Hi! I have never tried web scraping before but this is my last resort since the website I need to download data from does not have API. What it has is its Quick Report Plugin which cannot accommodate my needs.
â€‹
Basically, the website is https://nsap.nfrdi.da.gov.ph/explorer-light . It is where I can look up Fish Landings Data in the Philippines per Region, Province, Species, Gear, or a combination. The data I need is the monthly catch data in my province for the fish Sardinella lemuru under the family Clupeidae. The only way I can get this is from the Data Explorer>Catch Data By Species>Per Province>Specify Year, Species Family, and Species Name. This method would mean that I need to do this for 6 times to cover 6 years and manually copy each month's value since I can't export it.  
Can anyone help me with this? I thought of web scraping but I don't have any experience on this. Go to the website if you want to try it for yourself.",4,2024-02-20 12:06:49
All in one solution for web scraping for devs?,"Letâ€™s say youâ€™re a dev? Are there any frameworks available to start web scraping out of the box for technical users? (Taking scalability, configurability etc into account)
I am currently building this tool, may go open source. 
Like how is great expectations for data quality but its uses libraries like pandas as its core probably. A web scraping framework that uses libraries like scrapy, beautiful soup etc? 
Anything like this available?",1,2024-02-20 19:02:24
What are bots built with?,"so I'm new to web scraping, and I found this list of bots:
https://radar.cloudflare.com/traffic/verified-bots
I assume that not every bot has a dedicated team that builds them from scratch, so what kinds of frameworks/stacks do they use? Any place where I can read up on the systems design of people who have first hand experience building these?",4,2024-02-20 04:36:30
"When i'm Scraping with Scrapy Sometimes i have a 200 and can access, sometimes i have a 403 and can't access","I'm quite new and I'm trying to scrape the website https://fr.ra.co/
My problem is the following: I succeeded yesterday to have a 200 response with faking user agent, but after some other tries I've been getting 403. The next day (today) the first time I tried it was showing 200, then after some tries it became 403, but then after some retries again 200, and now it says 403 and it looks definitive for at least the day. What could happen to produce that result?
I have mostly followed this tutorial to begin: https://scrapeops.io/python-scrapy-playbook/scrapy-403-unhandled-forbidden-error/
Here what I've done in my setting.py
ROBOTSTXT_OBEY = False  DOWNLOADER_MIDDLEWARES = {     'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,     'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,     'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,     'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401, }  FAKEUSERAGENT_PROVIDERS = [     'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try     'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us     'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value ]  USER_AGENT = '<Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36>'  

And this is my header:
  HEADERS = {     ""Accept"": ""*/*"",     ""Accept-Encoding"": ""gzip, deflate, br"",     ""Accept-Language"": ""fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7"",     ""Content-Length"": ""4122"",     ""Content-Type"": ""application/x-www-form-urlencoded"",     ""Origin"" : ""https://fr.ra.co"",     ""Referrer"" : ""https://fr.ra.co/"",     ""Sec-Ch-Ua"" : ""'Not A(Brand';v='99', 'Google Chrome';v='121', 'Chromium';v='121'"",     ""Sec-Ch-Ua-Mobile"" : ""?0"",     ""Sec-Ch-Ua-Platform"" : ""Windows"",     ""Sec-Fetch-Dest"" : ""empty"",     ""Sec-Fetch-Mode"": ""cors"",     ""Sec-Fetch-Site"": ""cross-site"",     ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"" } 

And this is the response I have when I shell and request response.text
:
""info"": {     ""version"": ""2"",     ""statusCode"": 403,     ""statusMessage"": """",     ""headers"": {       ""date"": ""Tue, 20 Feb 2024 06:31:24 GMT"",       ""content-type"": ""text/html;charset=utf-8"",       ""access-control-allow-origin"": ""*"",       ""cache-control"": ""max-age=0, private, no-cache, no-store, must-revalidate"",       ""set-cookie"": [         ""datadome=os6pDl~6tWinZAoaxnbXzIYtz_zu8pK_6kz~JZPZa~ieOXSiWcULss3_cjXiupa5~rlgrIh7OHy7XnQWKhyA7H0gmskyWCuT3Bclk8c9NlsnxabSXpnrQR9ScqcWlwIt; Max-Age=31536000; Domain=.ra.co; Path=/; Secure; SameSite=Lax""       ],       ""accept-ch"": ""Sec-CH-UA,Sec-CH-UA-Mobile,Sec-CH-UA-Platform,Sec-CH-UA-Arch,Sec-CH-UA-Full-Version-List,Sec-CH-UA-Model,Sec-CH-Device-Memory"",       ""access-control-allow-credentials"": ""true"",       ""access-control-expose-headers"": ""x-dd-b, x-set-cookie"",       ""charset"": ""utf-8"",       ""pragma"": ""no-cache"",       ""x-datadome"": ""protected"",       ""x-datadome-cid"": ""AHrlqAAAAAMAx6PttWoNTCAAwMZ8vA=="",       ""vary"": ""Accept-Encoding"",       ""server"": ""cloudflare"",       ""cf-ray"": ""8584b4d86ac04bc7-BUF"",       ""content-encoding"": ""gzip""     }   },   ""body"": ""<html><head><title>ra.co</title><script src=\""/cdn-cgi/apps/head/tp1ZHJ6G4oyYkj6qAewQ1BNJKD4.js\""></script><style>#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}</style></head><body style=\""margin:0\""><p id=\""cmsg\"">Please enable JS and disable any ad blocker</p><script data-cfasync=\""false\"">var dd={'rt':'c','cid':'AHrlqAAAAAMAx6PttWoNTCAAwMZ8vA==','hsh':'107A2F9ACF118F5EFF46550CD47084','t':'fe','s':41462,'e':'9002aa6e033fb2de28bfcfd7685eb8bda998528879bf79b1a51783b0125365db','host':'geo.captcha-delivery.com'}</script><script data-cfasync=\""false\"" src=\""https://ct.captcha-delivery.com/c.js\""></script></body></html>\n"",   ""extractor"": {     ""result"": {       ""items"": []     }   } } 

So I set ROBOTTXT
 to false and use Scrapy-Fake-Useragent and I also use my own User_Agent.
I doubt it could come from an IP ban simply because with my computer, I can access the website. (And I'm not sure I understand why I could get 404 if I use the same IP and user agent as my computer and figured out it works on my computer.)
Using https://checkforcloudflare.selesti.com/, it shows me that the website is using Cloudfare, do you think it's where I should have a look?
I tried to modifiy the user agent, to set a header (but I think the Scrapy-Fake-Useragent already do that work?) to modify the delay time, to disable the robot.txt
.",1,2024-02-20 07:26:50
How can I scrape a dynamically loaded page blocking third party cookies?,"Iâ€™m trying to scrape a fairly well known maritime website. Theyâ€™re known for being tough to scrape, and obviously people that have figured it out will not publicly post how, so Iâ€™m determined to figure it out myself. 
I bought a premium account to see more details (the publicly available free details arenâ€™t enough). But when I set my headers and cookies in my program, when it runs it returns â€œThird party cookies will be blocked.â€ And doesnâ€™t end up scraping. Any advice on how to achieve this? 
Iâ€™m using C# with selenium & chromedriver. Iâ€™m open to python as well if thereâ€™s better packages there, but for me C# would be ideal",6,2024-02-19 21:44:45
Scraping with selenium and pyautogui,"Sometimes, it just seems like the simplest solution would just be to take a screenshot, find the element I want based on pixels, have my pointer move to that location, and click.  Obviously, this will make my scraper run way slower and be heavily platform dependent, but for my purposes, I don't care.  The issue is that if I use pyautogui, I can't use my computer for anything else while scraping, because it will interfere with screenshotting and cursor movement. Does anyone have or know of a solution to this?  I'd prefer to avoid any kind of containerization or pushing to the cloud, because that will come with its own issues.",4,2024-02-19 21:27:22
TruepeopleSearch.com scrapper,"I am developing a truepeoplesearch.com scrapper using Python selenium but facing issues with the captcha and tried everything to bypass it like undetected-chromedriver and selenium-stealth, but none is working.
If someone has a solution for this, I would appreciate the help. And if there is no way around it, then where I can buy residential proxies to integrate into bot.",1,2024-02-19 23:51:00
Auto-scrape web with given words by automatically going into each nested links?,"Thinking of scraping all data in particular news article websites with a given word by automatically going into each nested links. For instance, if I want to scrape data about 'COVID-19', I want the automated system to go into each nested links (ie. New York Times) and output each URL of the news articles that has the word 'COVID-19' in the article. Is this even possible?",0,2024-02-19 19:39:46
No-code Web Scraping (real-time too!),"Am I the only one who is often in the same situation?  
```
I want to find/copy some information from public websites. For example from a Notion database or a private (authenticated) dashboard, or whenever from the internet. I want to paste that info/values into my code/excel/pdf/website and have them update in real time. Something similar to variables and integrated APIs, a developer would say.
```
â€‹
Yet I haven't found a reliable solution. Usually, most web scraping solutions (even no code tools) only allow me to send the data to a Google Sheet or Airtable. I want to copy data from anywhere on the internet and use them wherever I need to, in a no-code way. Any suggestions? Have you faced similar problems? 
Thank you in advance!",1,2024-02-19 19:29:03
Clicking ALL Buttons in Web Page,"Hi all,
I am doing this scaping project where I am trying to get all text in a given web page. The way I am trying to do this is using Playwright package and BeautifulSoap in Python. Firstly,  I use BeautifulSoap as follows:
soup = BeautifulSoap(html_content, 'html_parser')
text1 = soup.find_all('p')
text2 = soup.find_all('span')

These obtain the <p> tag and <span> tag text. However, there are text hidden under <button> tags, and the way to do this - as far as I understand - is to click the  buttons and then use soup.find_all() method to get the text. For this, I am doing the following commands:
elements = page.locator(""button"")
n = elements.count()
for i in range(n):
   elements.nth(i).click()

However, when executed, it takes long time before it throws a runtime error due to exceed time.  I tried different timeoutvalues but to no avail. Would you please help in this?
â€‹
Many thanks",1,2024-02-19 18:51:52
"Chrome's ""Copy as cURL""","I was wondering if anyone knows if Chrome's ""Copy as cURL"" command generates exact same copy of the HTTP request made in the browser? Are there any differences that could make browser request fail, but cURL request work?
Here's a bit of background of what I'm trying to achieve.
I'm writing a scraper for https://www.toyota.com/owners/vehicle-specification/ website to pick up vehicle specs for the given VIN. I did manage to make my script log in without problem, but requests to fetch vehicle specs fail with Cross-Origin Resource Sharing error: PreflightMissingAllowOriginHeader. Interestingly if I pause the scripts execution, use ""Copy as cURL"" command on the same failed request and run it in the terminal, I get 200 response with the data.",1,2024-02-19 17:36:22
Ts it possible to parse all website on google for email address with python?,I don't want to go one by one to website and scrap emails. How is it real to parse through google for specific website and only then scrap email.,0,2024-02-19 19:19:10
Any idea how I could scrap a list on that page?,"I'd like to scrap all items from that list page: https://debank.com/proposal 
I've tried multiples alternatives on python scripts, I've tested with those libraries approach already:
- BeautifulSoup: it struggles with JavaScript-rendered content.
- HTMLSession (from requests_html): also didn't work
- Requests: same
- Selenium WebDriver: I had issues trying locally, I use google chrome 121.0.6167.184, and it seems there is no version of ChromeDriver for that, only up to 114 I guess, also I don't think it would work on heroku.  
Since I will deploy it on heroku, it shouldn't need my manual action to do something manually on each interaction. Does anyone know what I should do for that?",1,2024-02-19 15:17:07
How not to get banned scraping large amounts of data.,"I have created a selenium script that scrapes every property of rightmove.com and information for each one. For a bussiness I am creating I ideally want to update this every day. There should be around 800,000 properties. I would need to have around 5 instances of my script running at the same time. When testing my scraper i have not been banned yet. How could I do this and not get IP banned. when running this script pretty much 24/7. I am planning to run it on a vps.",1,2024-02-19 10:22:40
Anyone own an online business that uses web scraping?,"Just curious about the potential costs if I were to create a website to display the data Iâ€™m scraping. Iâ€™m currently programming something to use for myself as it has made me a significant amount of income over the past couple weeks, but thought I might be able to sell it as a monthly subscription to other people. 
Note: The data Iâ€™m scraping must be displayed to users as close to instant as possible. Thanks!",3,2024-02-19 01:20:31
Online Course,"Hi all.
I'm hoping someone could kindly help.
I have paid 2.5k for an online course alongside my employment, and after requesting a PDF copy to make it easier for me to study, it was declined therefore I'm having to read a prominently text-based course from a screen (I'm a bit old fashioned). Although there are several video based links this is not important.
Now, forgive me as I don't really understand scraping, yet I was advised that it might be an option. Would anyone please point me in the right direction and offer some advice whether scraping is suitable to extract all text and images to a PDF?
Maybe I'm asking in the wrong place so apologies if I am.
Thank you in advance",3,2024-02-19 00:10:33
Subreddit scraping help,"Hello everyone, I need to scrape all subreddits related to tea for a  research project. Initially, I used PRAW but discovered that the  submission limit cannot exceed 1,000. I intended to download data from  this source, but it is too large for my computer, and I'm not sure if it contains the subreddits I want either. I also don't know how  to use a virtual machine to process it. Can anybody help me retrieve all  the data from /tea and /puer, starting from January 1, 2023, up to the  present, in the following format.
'Post ID': post.id,
 'Post Title': post.title,
 'Post Content': post.selftext,
 'Post Date': post_date.strftime('%Y-%m-%d %H:%M:%S'),
 'Comment ID': comment.id,
 'Comment Body': comment.body,
 'Comment Date': comment.strftime('%Y-%m-%d %H:%M:%S')
Thanks a ton!",1,2024-02-19 01:39:39
scrapy only gives the proper output sometimes,"i am trying to scrape old.reddit.com videos and i am not sure what could be causing the inconsistency.
my xpath:
//a[@data-event-action='thumbnail']/@href

â€‹",2,2024-02-18 18:03:51
Fun scraping ideas,"Hello,  
I am doing a course in social data science, and I have an assignment in web scraping. However, there is full freedom as to what to scrape and visualize (I could do the weight and height of tour de france cyclists or inflation levels in different countries). Do you have any fun and creative ideas? I am getting a bit lost in the freedom, hehe..   
Thank you!",1,2024-02-18 20:48:59
Puppeteer extra detected by cloudflare,I am using puppeteer extra with puppeteer-extra-plugin-stealth but I get detected by cloudflare even when I set up the user agent and some other args i keep getting to the cloudflare page and I tried to check the human input but it keeps redirecting to the cloudflare iframe. Is there a solution for that ?,19,2024-02-17 20:21:50
Trying to scrape website text gives only a tiny bit of data,"I'm trying to get text from the website ligand.com
When I use
import urllib.request as urllib2
req = urllib2.Request(link, headers=hdr)with urllib2.urlopen(req) as response:rawtext = response.read()

All I get is
b'<html><head><link rel=""icon"" href=""data:;""><meta http-equiv=""refresh"" content=""0;/.well-known/sgcaptcha/?r=%2F&y=ipc:174.81.32.128:1708229047.782""></meta></head></html>'

rather than the page html. What is going on here?",1,2024-02-18 05:14:57
Trying to scrape website text gives only a tiny bit of data,"I'm trying to get text from the website ligand.com
When I use
import urllib.request as urllib2
req = urllib2.Request(link, headers=hdr)with urllib2.urlopen(req) as response:rawtext = response.read()

All I get is
b'<html><head><link rel=""icon"" href=""data:;""><meta http-equiv=""refresh"" content=""0;/.well-known/sgcaptcha/?r=%2F&y=ipc:174.81.32.128:1708229047.782""></meta></head></html>'

rather than the page html. What is going on here?",0,2024-02-18 05:14:57
How to tackle elements that change its id periodically.,"I've noticed some elements that changes its ids in a certain period of time, to avoid easy scraping i guess. What is the best approach to deal with this?",1,2024-02-17 23:12:13
"What modules should I add to an open source project that combines headless browsing with LLMs, without reinventing the wheel?","Hi,
I have been building a project that helps you do any browser automation, scraping, or functional testing tasks with the help of combining headless browsing with LLMs that can identify the UI elements of a webpage and make actions on top of it. Currently the project has only done an early release, but I think this one could have several modules that makes is easy for a web scraping user to have a customised LLM agent in their web scraping tasks. What modules should I further add, as the current landscape of LLMs  and scraping has a lot of new components in many popular projects?   
Here is the project link and I have explained pretty much everything I am doing in the project readme and description: https://github.com/kindsmiles/pyvigate",3,2024-02-17 09:50:02
Can I we scrape my college?,I started a business in college and want to get the emails of the students here through the directory. It does require me to sign in with my college email to access it. Iâ€™m using webscraper.io . I just want to send ads for my business. Iâ€™m still studying here so I donâ€™t want to get in trouble. Would this cause problems?,0,2024-02-17 09:44:14
Scraping / Automation with user creds,"Hi I am working on an automation / scraping tool and i need help with what i am trying to accomplish. I plan to allow others users to use my service. The service I am building requires login to third party websites. The websites utilize regular email pass login and the normal signin with google, microsoft etc.
â€‹
My first thought was to just store hashed or encrypted user creds in db and use that way. Is there a way i can use oauth or something else to do the same thing?",5,2024-02-16 21:35:29
How to increase reliability?,"I have a fairly basic python script using BeautifulSoup and requests that is grabbing a small amount of public data from a large archive, but it always fails after about 20-30 successful grabs. I wait a while (like an hour), try again, and it gets a few more. it seems like maybe there's some sort of protection on the server end, or maybe there's something else going on? I've tried using tenacity, but it seems like I have to wait too long before retries for it to be useful.",1,2024-02-16 17:23:56
How to scrape Twitter comments,How can I scrape 20000 twitter comments from a post? Do you know any tools? I'm having problems using API,1,2024-02-16 14:05:43
Accessing node inside particular div that might occur elsewhere with same attribute value. How to narrow find_element to particular region,"How to select specific nodes under specific div. I want a pointer to the img node under div with class=level2 in the following html. The whole html is here[1]
Why level2_div.find_element() keeps points to img present in parent node. How to narrow the focus of find_element to nodes under its content?
BLUEBOX = '//div[@class=""bluebox""]'
COLLAPSE_BTN = '//div[@class=""collapse-arrow""]/img]'
LEVEL2_DIVS = '//div[@class=""level2""]'
LEVEL3_DIVS = '//div[@class=""level3""]'

# for purpose of this question the html contains only one bluebox div,
# but actually there are multiple hence level1_divs
level1_divs = driver.find_elements(By.XPATH, BLUEBOX)
for i in range(len(level1_divs)):
      level1_divs = driver.find_elements(By.XPATH, BLUEBOX)
      level1_div = level1_divs[0]

      button = level1_div.find_element(By.XPATH, COLLAPSE_BTN)
      button.click() #<<< this will populate the level2 divs

      level2_divs = level1_div.find_elements(By.XPATH, LEVEL2_DIVS)
      for j in range(len(level2_divs)):
            level2_divs = level1_div.find_elements(By.XPATH, LEVEL2_DIVS)
            level2_div = level2_divs[j]

            try:
                  #this always points to collapse img under level1_div even
                  #though i use level2_div as the starting point. Why?
                  button = level2_div.find_element(By.XPATH, COLLAPSE_BTN)  # <<<<<<<<<
                  button.click()

                  level3_divs = level2_div.find_elements(By.XPATH, LEVEL3_DIVS)
                  for k in range(len(level3_divs)):
                        level3_divs = level2_div.find_elements(By.XPATH, LEVEL3_DIVS)
                        level3_div = level3_divs[k]

                        process_link(level3_div)
            except:
                  process_link(level3_div)  #process link under <div class=""link""></div>

Example HTML fragment
  <div class=""bluebox"">
      <div class="""">
        <div class=""collapse-arrow"">
          <img src=""collapse.gif"">
        </div>
        <div class=""link""> <a href=""A01"">A01</a> </div>
      </div>
      <div class=""clearfix"">
        <p>  Highest level text</p>
      </div>
      <div class=""additional_links"">
        <div class=""level2"">
          <div>
            <div class=""collapse-arrow"">
              <img src=""collapse.gif"">                 <<<<<<< This one 
            </div>
            <div class=""link""><a href=""A01_0.txt"">A01_0</a></div>
            <div class=""additional_links"">
              <div class=""level3"">
                <div>
                  <div class=""collapse-arrow"">
                  </div>
                  <div class=""link""><a href=""A01_00.txt"">A01_00</a></div>
                  <div class=""clearfix"">
                    <div>
                      <p> second level text </p>
                    </div>
                    <div>
                    </div>
                  </div>

â€‹
[1] https://pastebin.com/XfmLkHva",1,2024-02-16 12:54:22
Hook post server endpoint to send updates,"How can I establish persistent connection with a server endpoint using a POST request to receive updates? I donâ€™t want to send repeated request to see if any new update has come, instead I want the server to automatically push me updates. I played with sse and web sockets, but connection always end with a single request/response. If thereâ€™s anyone who knows any advance technique make server send updates automatically to me when it comes instead of me making multiple post request to them. Some info: The endpoint is powered by PHP and Plesklin. Thanks",1,2024-02-16 12:45:31
Resources for fictional news articles for scraping,"Hi,
I am working on a project for my dissertation involves webscraping. I am looking for resources to get URLs to news articles (fictional or none fictional) that allow scraping. Topics should rage from political to opinion pieces.
Whenever I find recourses they are always for finance but I just want to find websites that allow scraping for news articles. I just need to get the URL (you might be able to tell this is stressing me out).
Thanks in advance for any help.",1,2024-02-16 03:00:50
Legally Scraping from Ebay,"If I were to scrape the price and date of sold items from Ebay, does that violate Ebay's scraping policy? I just want it for a personal project. Thank you",1,2024-02-16 01:29:07
"Hi everyone , I am trying to figure out how to webscrape from the redirected links .","So the page i want to webscrape https://www.searchfunder.com/deal/exchange page but i think we need to first pass the login through https://www.searchfunder.com and pass through some initial redirected links and some how move to https://www.searchfunder.com/deal/exchange page and get the full grown html of it.  
So i have figured this much out that i need to use request_html to render the page/deal/exchange as this page does dynamic js loading. But the current task is how to land to this page first and then scrape ????  
Please let me know of any tryouts i can do",3,2024-02-15 17:37:43
Twitter scraping,"Hi, does anyone know how the find  URLs of videos on X/Twitter? I'm trying to scrape some tweets but I can't find video links in their code using XPath.",1,2024-02-15 15:21:02
Bun for web scraping?,"I always use puppeteer for scrap alternative websites, but I feel to slow about it, do you think bun makes it faster or it's not gonna be so different?",1,2024-02-14 20:51:57
Text classifier built in scraper,"Hi Everyone! 
I have used web scraping only once, but need to use it for my research again. Iâ€™m going to scrape public channels in Telegram (I need only postsâ€™ texts and date), but the problem is I do not need all posts from these channels. What I need is only posts about one specific topic. So my main question is: is it possible to build in the text classifier in scraper? So the scraper goes to the channel, scrapes it, identifies if the topic is relevant or no, and puts into csv table only those posts that are on a specific topic. Is it possible to do? And if yes, any advices on it would be extremely appreciated. 
Thank you so much in advance â¤ï¸",2,2024-02-14 16:00:09
Playwright or Puppeteer?,"Hey. I'm in a situation where I have to choose either Puppeteer or Playwright. I'm interested in nothing else but maximum efficiency and stability, knowing that my scripts take hours/days to finish.
Thanks.",3,2024-02-14 11:56:16
Trying to scrape some manga but the image are JS and shuffled,"Trying to scrape some manga (https://mangareader.to/read/akira-46/en/chapter-1) but using something like JDownloader, I get the images shuffled into a grid. Python script I used doesn't seem to parse any images. Using inspector I found the images follow a like this (an example of one of the pages):
<div class=""iv-card shuffled loaded"" data-url=""https://c-1.mreadercdn.com/_v2/0/0dcb8f9eaacfd940603bd75c7c152919c72e45517dcfb1087df215e3be94206cfdf45f64815888ea0749af4c0ae5636fabea0abab8c2e938ab3ad7367e9bfa52/2c/65/2c65696bd8504c362d8b01db021b15b2/2c65696bd8504c362d8b01db021b15b2_1100.jpeg?t=515363393022bbd440b0b7d9918f291a&amp;ttl=1908547557"">
                    <div class=""card-loading"">
                        <div class=""c-l-area"">
                            <div class=""paper-loading""></div>
                            <p class=""mb-0"">Loading...</p>
                        </div>
                    </div>
                <canvas width=""727"" height=""1100"" class=""image-vertical""></canvas></div>
Any help?",1,2024-02-14 14:49:03
Instagram hashtags,"Hey I'm very new to this, is it possible to scrape the number of times a specific hashtag has been used? How hard/ easy is it to set up? What tools are the best to use to use?",5,2024-02-14 07:15:11
What to do with 503 code?,"Hey all! I'm kinda new to web scraping. I need to scrape data from millions of URLs. But when I start making the requests, I sometimes get a 503. I know for one that 429 is the code for being rate limited, but what's up with 503? What is its purpose and what do I need to do with that? Should I wait for sometime and request the same URL again. Or is the URL broken? Also, is it an error on my end or the server's end? Thanks!",1,2024-02-14 05:16:30
Web scraping multiple sites,"Iâ€™m trying to develop a scraper that takes in a link of news site and returns a list of article urls for articles present in the homepage. It should be able to handle a variety of sites so I canâ€™t hardcode specific html structures. 
The approach Iâ€™m thinking of is extracting all links from the html then excluding any links that are present in the header/footer or point to external sites. This approach was successful in eliminating a lot of links but there are still some false positives. (For example, not all websites have a header/footer so I still get links from there and sometimes there are footer links outside the footer structure) Does anyone know how I can exclude other links or has a better approach?",1,2024-02-14 01:26:39
Low Level Useful?,Hey I was just curious whether low level languages are useful to you in any specific scenario when you are scraping a huge volume?,1,2024-02-14 00:41:42
Web Scraping Instagram simulating the official app,"I am developing software through which, after users log in to Instagram and obtain a bearer token enabling private calls to Instagram, they can synchronize their Instagram data.
This synchronization procedure consists of a series of consecutive API requests to Instagram: obtaining user info (username, fullname, etc.), obtaining followers, obtaining following, and obtaining posts. I have replicated the requests exactly as they are made in the official app down to the smallest detail.
I need this procedure to be performed by the user as many times as they want without having to re-login each time to obtain a new token.
The issue is that every 2-3 synchronizations, Instagram responds with a 'challenge required' error due to suspected scraping, forcing the user to log in again and effectively invalidating the bearer token. I have also tried introducing a delay between each request, but the result is the same.
I suspect that some requests made by the official app allow the token not to be invalidated because they are considered 'secure' and 'not-scraping' , but I cannot figure out which ones.
Does anyone have any information on this that could be helpful? Does anyone have an idea of how Instagram identifies certain API requests as suspicious scraping, leading to the invalidation of the session?",4,2024-02-13 15:10:59
Scraping with the power of GPT,"Hey guys before a couple of months with working on a project one of the stages i needed a scraper to scraper a number of websites and to return a certain JSON format always. I didn't want to do it manually because its boring after looking for a lot of solutions I came up with this solution using OpenAI, instructor, Pydantic.
Give me your suggestion and how can i improve this project, any suggestion will be helpful.
https://github.com/Mamdouh66/Scraper-GPT",1,2024-02-13 18:16:56
Capture Entire Website with Screenshots ?,"Hi ! I'm trying to screenshot all the pages of a website, but there are so many pages that it's taking me forever to do a web capture one by one. Is there any way to extract these pages quicker ?
I'm open to paying for it if there's a good website that does this kind of stuff....
Any help is greatly appreciated !
â€‹
â€‹",4,2024-02-13 09:36:19
How is your setup?,"I have taken upon myself (the job want to save money by doing it inhouse) to setup a scraping bot(s) scraping our three main competitors. I am using Python with requests (JSON) and Selenium to load Javascript(html, CSS selectors) and save it all to csv files. I have tried bot Scraping Bee and Oxylabs proxies and it seems Oxylabs are faster (For my geolocation maybe). Then some Python scripts for merging, backup and calculations. I am gonna run all on an local windows PC with windows scheduler for the scripts and powerautomate to copy the files to Sharepoint and give an small report on a dedicated channel on Teams.
This seems to me as an 'easy' way out. Maybe in the future it would involve for example an AWS/azure backend to handle compute, storage and an sql database. (Maybe an SSAS cube to PowerBI)..
My projectfolder looks kinda like this:
..backup
..csv
..log
..utils
..credentials
ScrapeStore1.py
ScrapeStore2.py
ScrapeStore3.py
Some questions I have..
1. What are your favorite proxyrotators?
2. Any good Utils? (Like ""I"" have programmed (thanks copilot) an script that fetches the URLs from the sitemaps in robots.txt that works pretty nicely)
3. Worth building an GUI for parameters? (Sleep time, specefic categories, etc)
4. How do you setup your infrastructure from scraping to delivery to visuals?
5. How do you setup your project folder?",5,2024-02-13 07:44:31
learning Web scraping,"Any recommendations for courses to learn web scraping using Python for data analysis?  
Can I master the skill in one month? I am really good with Python, and I have an IT background.",2,2024-02-13 11:10:54
Is it Normal for a Python Selenium Web Scraper to Take 4 Days for 40k Pages?,"Hey everyone,
I recently ordered a web scraper built with Python and Selenium, and I've been running it to extract data from approximately 40,000 pages. However, I've noticed that it's taking around 4 days to complete the extraction process, and I need to keep my PC always on during this time.
I'm wondering if this is normal for this sort of script to run for such a long duration. Is there any way to optimize it or speed up the process? Alternatively, would switching to a tool like Octoparse be more efficient?
I'm relatively new to web scraping, so any insights or advice would be greatly appreciated. Thanks in advance for your help!",18,2024-02-12 17:41:13
Enterprise Web Scraping: What to look out for,"Hello r/webscraping,
I see a lot of similar questions on this subreddit and thought I would add my 2 cents and try and cover a lot of the pitfalls I see when people start trying to scrape at scale. If you're asking the question ""how do I scrape 100 million pages in a month that run javascript/keeps blocking me/will be maintainable long term"", this guide might be for you.
Context
I'm a Senior Engineer who has specialized in specifically web automation for a few years now. I currently oversee about ~100 million requests a month and lead a small team in my endeavors. I've had the chance to research and implement most current tooling and hope to provide folks here with the most information I possibly can (while trying to stay inside the sub's rules ðŸ˜ƒ). This ""guide"" will mostly cover high-levels of requests, Websites that utilize Javascript, and bot detection (as these are what I have the most experience dealing with).
Tech Stack
There is a multitude of different options, but the ones I typically shoot for on a project are:
- Typescript
- Puppeteer (or puppeteer-extra depending)
- AWS (SQS, RDS, EC2)
Proxies
Proxies mask your origin IP address from the website. These are EXTREMELY important if you plan to make a bunch of requests to one site (or multiple). There are a bunch of proxy services that are fine to use, but they all have their downsides, unfortunately. If you have to cover a bunch of requests to a bunch of websites, and there is a chance they are blocking IPs or verifying the credibility of the IP through some online flagging database, then I would recommend going with a larger, more credible proxy service. The goal is to have clean and fast proxies. If they aren't clean, you can easily get blocked. If they aren't fast, they will increase your infra pricing and possibly cause your jobs to fail. I typically use services that have an IP pool in the millions and utilize a few at a time in case of outages or an uptick in failures.
Captchas
The ultimate robot stopper.... not. There are a ton of captcha-solving services on the market that you can just pay for API usage and never have to worry about again. Pricing and speeds vary. I've found that AI-based solvers are the best sometimes. AI solvers are the fastest and the cheapest, but the best ones I've used can't solve every kind of captcha (IIRC HCaptchas are the problem), so if you're solving for multiple sites, you may need a few different solutions. I'd recommend this anyway because if there is ever an outage (which does occur when there are captcha updates), then you have a backup for when jobs start failing. A little extra code will automatically switch over services when stuff starts failing ðŸ˜ƒ
Browsers
The one thing that probably matters the most when interacting with bot detection at scale. These solutions are somewhat new to the market. I've even made my own in some cases, and this is probably the one thing that I don't see mentioned frequently (if at all?) on this sub. There is a bunch of cool browser tooling out there that have their particular use cases. Some are licensed out containers, some are connection-based. That being said, they all do a somewhat similar job. Introduce entropy into the browser and mask the CDP connections to the browser. When interacting with the browser via a script (and technically without), there a leaks everywhere that make it easy for big bot solutions to figure out what's up. There's simple stuff that can be fixed with the scraping libs out there (user agents, etc), but there is also stuff like canvas/webgl fingerprinting that isn't as fixable with these libraries. Most large-scale bot detection tools use quite a few fingerprinting tools that get quite in-depth. I would not recommend trying to tackle these solutions solo if you don't have years to spend doing research and learning about the nuances of the space.
Infra
I've only found AWS to be ""the one"" in terms of being able to scale up to a level that I require. Sorry if this breaks rule 2, but this is what I've used and seen success with. Other solutions are going to be difficult to maintain and develop long term. I specifically utilize EC2/ECS for the scraping portion because tooling like Lamda/Fargate (although cheaper) doesn't offer the privileges that more ""aggressive"" scraping might require.has
Clustering
A must when trying to achieve millions of jobs a month. My solution for this is at a few different levels. Node has some built-in packages that allow for clustering which is great for maximizing machine usage and optimizing scale costs. Next would be utilizing ASGs in AWS to scale up the number of machines we are using. After that, we would accept requests from a queuing service) doesn't offer the privileges that more ""aggressive"" scraping might require.
Queuing
Queuing is great for this stuff. Jobs take an unknown amount of time and can run extremely long if there is an outage somewhere. I  would recommend this all day and if you don't currently have a queue for your jobs and you are looking to scale, do it.
Retries
Failures are inevitable, but you don't have to let all that precious data getaway. If you want to do this at scale, we need to determine if a job has failed and have a system in place for getting that data again. This is where queuing is important. Having tooling where you know if something has failed and being able to add it back into the queue is so important at a large scale that I shouldn't even have to mention it. Don't forget this.
Cost Savings
There are tons of places for you to save money on this. Negotiating infra, captcha, browser, and proxy costs down to understanding every single request you make. Proxies can get expensive. There is great tooling in Puppeteer (extra?) that lets you manage each request and even bypass your proxy and download it straight to you. I would say just make sure if you do this, know which requests your allowing, and which you are letting bypass or you could run into some issues. Essentially, we should look to optimize to have the least amount of requests, and the least amount of data downloaded as possible without jeopardizing our identity.
Metrics
It's easy to see if your scripts are working locally, but sometimes not everything is as easy in the cloud. This is one of the most important things if you plan to scale is understanding your requests. Please, please, please utilize reporting tools so you know that the data that you are getting is correct and is coming in at the size that you need. There are no ifs, ands, or buts. Especially if you are dealing with clients on your project.
Conclusion
There are a ton of variables in large-scale web scraping that need to be accounted for. Bot detection, rising costs, and cumbersome tooling are just a few you WILL encounter. I wish you the best of luck in your endeavors and hope this guide provided a little guidance into where you should start looking or continue your journey.
P.S. some useful open-source docs
Puppeteer-extra
Dark Knowledge",64,2024-02-12 05:23:41
Webscraping Nordstrom?,"Has anyone had any luck scraping a product page (e.g. https://www.nordstrom.com/s/sportswear-club-hoodie/6049642?origin=category-personalizedsort&breadcrumb=Home%2FMen%2FAll%20Men&color=765) from Nordstrom? I only need details like image, price, sizes, etc (product specific metadata).  
Ive tried various headers with requests, and with selenium (headless and not) without any luck whatsoever. Very new to webscraping, so any help hugely appreciated, and if anyone had a script that would be amazing!",1,2024-02-13 00:53:44
Chipotle Menu Price Web Scraping,"Hello All!
â€‹
I am new to this subreddit and I recently thought of an idea that I would like to explore. I want to gather Chipotle (and maybe other restaurants) data on menu prices and their corresponding rewards point values in order to see how much value one can get by redeeming various rewards. Ideally, the web/mobile app would update and values would change as well. I was also thinking this could be scaled and expanded to a point where users could check close-by locations and see which locations have better prices and/or rewards exchange values.
â€‹
I recently came across this GitHub Repo where there is code to search by location and find Chipotles near you, but I wasn't able to tell if I could get the specific data I am looking for just from the code in this repo. There is a Youtube Video here as well for reference if needed.
â€‹
Any help would be greatly appreciated. Thanks!",1,2024-02-13 00:08:20
Suggestion for Httpx/Aiohttp based web scraping framework for Python,"Hi folks,
Have You come across framework as mature as Scrapy based on Httpx/Aiohttp?
Scrapyâ€™s core is twisted. Architecture is great. Pipelines. Middleware specially.
Thank You",1,2024-02-12 22:27:50
Scraping ad websites for commercial use,"Could I actually be sued for scraping commercial advertisement website data from multiple websites, aggregating it, repackaging it in a different, more convenient display format, with linking back to the original websites, slapping an ad banner on and hosting it as a public  website?
Any cases of this?",1,2024-02-12 19:56:20
"Web Scraping the details of this page without having to press the ""expand"" arrow for each of the 700+ entries","How would I webscrape the details of this page without having to press the ""expand"" arrow for each of the 700+ entries?
Is it possible to view the source code of this webpage on Google Chrome? If so, how do I do this?",0,2024-02-12 17:10:45
Gold Standard Enterprise Proxy Provider?,"For those that have experience with large scale scraping operations (millions of requests per month), what specific companies or services do you use? There are a few Iâ€™ve used such as BrightData, OxyLabs, and IPRoyal, but is there a clear gold standard in your mind for what the best service is at a large scale?",1,2024-02-12 15:25:12
Airbnb scraper made pure in Python,"Hello everyone, I would like to share this web scraper.
The project will get Airbnb's information including images, description, price, title ..etcIt also supports full search support
https://github.com/johnbalvin/pybnb
I absolutely hate using tools like Selenium, Puppeteer, Playwright, and the like; that's exactly why I went with pure HTTP requests instead
Install:
$ pip install gobnb
Usage:
from gobnb import *
data = Get_from_room_url(room_url,currency,"""")
let me know what you think
thanks",11,2024-02-11 21:05:08
Wayback Machine/ Archive - Twitter hashtags,Is there any Python package to link to the wayback machine archive of twitter to search for particular hashtags rather than usernames?,2,2024-02-11 16:08:22
Seleniumbase UC and block specific urls,"Hello,
I am trying to block some urls from loading in order to have fast rendering of web page (css, js, images)
Eg, I want to load example.com and block all example.com/assets/* requests.
In order to have a proxyless system, are there any solution that I can follow?",2,2024-02-11 14:45:38
Robots.txt disallowed pages issue on an e-commerce website,"Curiosity kills the cat!

I'm quite new at web scraping. I wonder that what happens if i scrape data from robots.txt disallowed pages?

What is the mechanism of work this kind of e-commerce websites?
Can i use this scraped data in my personal web crawler or spider projects to compare the product prices?
Is there any restriction in server side?
Do server forbids my access  or directly blocks my IP or any other access info?
Here is the part of the robots.txt :

User-agent: *
Allow: / [ somepagehere]
Disallow: /m/
Disallow: /mc/
Disallow: /siparis-takip-durumu
Disallow: /kiyasla/
Disallow: /brand/
`",1,2024-02-11 17:56:39
How to bypass cloudflare in python?,"I was trying to scrape this website: 
https://www.truepeoplesearch.com/
But i got blocked.
I've tried some configs for selenium but it didn't work, i also try using Undetectable Chromedriver but i got the same result.
I'm new with cloudflare so are there other options that i could do?",8,2024-02-10 23:18:12
Getting 403 on every device except my machine,"Hi! I am having an issue that I can simply not explain. I am scraping kleinanzeigen.de using proxies, which seems  to work perfectly on my machine, but if I dockerize the application or have anyone else execute the code it willl return a 403 error. I know for a fact that the proxy is being used on every machine, since I can see the requests going out on the proxy dashboard. I have also tried adding several request headers with no succes.
Dockerfile:
FROM python:3.10.12-slim

# Set the working directory to /app
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed packages specified in requirements.txt
RUN apt-get update -y && \
    apt-get install -y postgresql postgresql-contrib && \
    rm -rf /var/lib/apt/lists/* && \
    pip install --no-cache-dir -r requirements.txt && \
    rm -rf /root/.cache && \
    apt-get autoremove -y

# STACKOVERFLOW
ENV PYTHONUNBUFFERED=1

CMD [""python"", ""main.py""]

â€‹
Python code fragment:
def request_with_proxy(url, headers={}):
    # Add random user agent to headers
    headers[""User-Agent""] = user_agent_rotator.get_random_user_agent()
    # Configure proxy
    try:
        proxy_url = f'http://{os.environ[""PROXY_USER""]}:{os.environ[""PROXY_PASSWORD""]}@p.webshare.io:80'
        proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
    except:
        raise TypeError(""MISSING PROXY ENVIRONMENT VARIABLES PROXY_USER AND PROXY_PASSWORD"")

    # Retry 3 times before crashing
    for _ in range(ATTEMPTS):
        try:
            response = requests.get(url, headers=headers, proxies=proxies, timeout=TIMEOUT)
            print(response)
            print(response.status_code)
            return response
        except Exception as E: print(E)

Any ideas? Thank you!",2,2024-02-11 00:18:12
How to scale webscraping with selenium?,"What is the best way to scale an application that uses Selenium? To be more specific, I thought of a backend application using Flask or Jango that starts, on an endpoint, a service using Selenium.
The problem is that this seems like a huge memory bottleneck to scale a project like this to hundreds or thousands of users.",1,2024-02-10 20:09:15
"Playwright browser getting detected after scraping 25,000 pages","Hello, I am scrapping a website protected by an anti-bot service, the service is very advanced. I managed to bypass it using common tricks + proxies + captcha solving services. Everything worked like magic and I was scraping using multiple instances. However, today the program stopped working and all the automated browsers are getting blocked. 
The weird thing is I can use my main Chrome browser and access the website with no problem. How do such services detect automated browsers? (I have checked all the headers sent/time zone/ locales... nothing special)",7,2024-02-09 21:09:48
Need to scrape 10 million links within a 28 day timeframe. Any Advice?,"As the title suggests im trying to scrape 10 million urls (from the same provider) in a month time frame. I run into 429's after about 1000 requests. I'm new to web scraping but not new to programming and I decided to just use python as it would be simple. Everything works besides the rate limiting. I am not opposed to spending money on proxies and whatnot, I just want to know the place im going to would actually be useful and not just get me locked out after 10000 requests.
If you guys have any advice on this as I really have no clue where to start in procy rotating. If you have any advice on how to make an IP last longer before being 429'd aswell that would be great cause as of right now im obviously bot like im doing 8 urls in a multihreaded batch and just grabbing the html with a python request.
Thanks everyone!
Oh, and its a rolling 28 days so I will run it again the month after etc. thats why the time constraint",7,2024-02-09 19:55:08
Which ai tools can scrape social media for new posts?,"Not exactly web scraping but I am interested to find an ai tool that can scrape facebook, instagram, quora, reddit and similar websites where people will discuss and mention particular keywords.
â€‹
Any suggestions?",3,2024-02-09 11:26:02
Is this possible,"I am new to scraping but I am having issues using the scrapy shell to view the site I want to get to. I am using the scrapy tutorial project but I just wanted to see if it was possible to get to this site. This is the robots.txt file. Should I abandon this project because so much is banned or is this just because I jumped the gun and need to figure it out? Thank you!
â€‹
User-agent: * 
#Prevent Bot Crawl of applied search filters 
Disallow: /search/*  
#Prevent Bot Crawl of deprioritised pages
Disallow: /*/selling/* 
Disallow: /*/sold/* 
Disallow: /*/likes/* 
Disallow: /*/other/*  
#Prevent Bot Crawl of Query Params 
Disallow: *?from=*  
#Prevent Bot Crawl category & brand filters 
Disallow: *categories=*
Disallow: *subcategories=* 
Disallow: *brands=* 
Disallow: *sizes=* 
Disallow: *priceMin=*
Disallow: *priceMax=* 
Disallow: *hasFreeShipping=* 
Disallow: *isDiscounted=* 
Disallow: *colours=* 
Disallow: *conditions=*  
User-agent: Yandex 
Crawl-delay: 2.0 
# sets a 2-second timeout 
Disallow: /search/?q=*& 
Disallow: *categories=* 
Disallow: *subcategories=* 
Disallow: *brands=* 
Disallow: *sizes=* 
Disallow: *priceMin=* 
Disallow: *priceMax=* 
Disallow: *hasFreeShipping=* 
Disallow: *isDiscounted=* 
Disallow: *colours=* 
Disallow: *conditions=*  
User-agent: GemIndexer 
Disallow: /  User-agent: 
PetalBot Disallow: /",1,2024-02-09 05:12:14
"store details from google, is it possible to scrape that data ?","I was given a task(intern) to scrape details of local businesses in different categories globally.
Can someone guide me.?",1,2024-02-09 04:32:20
Word Press. Scrape images from wp-content/uploads/,"Any suggestions to scrap all image files from a series of subfolders under domain.com/wp-content/uploads/
Inside /uploads/ are sequental years, 2018,2019 etc etc.   
Any advice appreciated!",1,2024-02-09 03:05:34
Seeking advice on my data subscription model and industry norms,"Hi. I'm offering a leads data subscription model and developing my own API to facilitate it. The subscription includes receiving 2000 new leads monthly for a fixed rate. With each passing month, a new set of leads is served through the API. As I'm relatively new to the concept of data subscription models, I'm wondering whether subscribers should only access the current month's dataset or also retain access to data from previous months during their subscription period.
To illustrate with an example:
- The subscription provides 2000 rows per month.
- Alice subscribes to the dataset in January and downloads batch #1.
- In February, Alice downloads batch #2, containing the new leads.
According to industry norms, would Alice anticipate ongoing access to batch #1 as well, perhaps through a versioning parameter or similar mechanism?
While I understand that the subscription terms are dictated by the seller, as a regular data subscriber what is more common?",3,2024-02-08 20:40:00
Scraping Tumblr image server,"Hello,
I'm not sure if web scraping would be the way to go. Looking for images that are lost on Tumblr but I know still exists on Tumblr image server. I've been able to find a couple photos through Internet archive and pulling the image source URL. But haven't been able to get them all. I am looking to pull find ever URL used by the image.tumblr.com and query/sort through them with the address parameters. 
Would this be possible or should I look somewhere else?",1,2024-02-09 01:13:54
How to approach this data scraping project,"Hi there!
So Iâ€™m a little informed but not great. 
My company requires us to scrape Amazon and capture supplements fact data from supplement product labels.
Thatâ€™s of course in addition to:

msrp
reviews
bought this month
type
count of capsules / weight of powder 
about this item
reviews 
ratings
title name
page link 

Iâ€™ve already got octoparse pulling most of it ok. But how do I approach the labels?
Literally the ceo wants to be able to look at all the competitors mg of vitamin c for example in a vitamin c supplement and then figure it out how it relates to retail price.
So he wants to be able to be like:

ok this has 1 mg of vitamin c
the retail price is $10
there are 10 capsules
serving size is 1 capsule 

$10 / 10 capsules = $1 capsule
$1 capsule / 1 mg of vitamin c = $1 per mg of vitamin c
And we want to pull the data at mass. As I said octoparse has been ok with some issues but the hand calculating takes forever.
There has to be a service right? Or a better way?",1,2024-02-08 23:58:39
Anybody figure out how to turn this into a money maker?,"I can whip together a pretty reliable scraper, pretty fast. Just wondering if anybody is making fun money out of this?",3,2024-02-08 15:20:02
Scraping Reddit for a public dataset â€” is it legal?,"Letâ€™s say that I want to create a dataset that connects users with their top-10 favourite communities. The dataset doesnâ€™t contain any information other than username:subreddit entries. The dataset contains 10s of thousands rows and will only be useful for educational purposes (trying out graph clustering algorithms). I collected the data using Reddit API or web scraping, and want to make it public (by uploading it on GitHub or Kaggle.com).
Is that legal? Does it come under any personal information protection laws? If itâ€™s legal, is it against Reddit TOC?",1,2024-02-08 17:32:57
How can I get past this captcha?,"Hey guys!
I have been scraping a certain website for a few months now. However, this week they added a very complex catpcha that I don't know how to get through now. 
Basically, you should align the image on the right with the direction the hand in the image on the left is pointing.
Does anyone have any way to resolve this problem?  
Thank you very much for all your help.
â€‹
â€‹
https://preview.redd.it/3xdj22v56dhc1.png?width=524&format=png&auto=webp&s=1c710cf0bae83ffd405396244e17d120d5098f97",1,2024-02-08 13:33:46
Trying to Solve captcha,"https://stackoverflow.com/questions/77961370/try-solving-google-captcha
â€‹
First off, excuse me, I'm still learning
â€‹
How he solved the captcha in this video starting at second 27
â€‹
https://youtu.be/0h504QFDyxc?si=RckXYelnrvWyWHFA
â€‹
Every time I solve the captcha on this site, it tells me that something is wrong. How do I overcome this problem?
â€‹
I tried the code api 2captcha It did not succeed in solving it I use a library seleniumbase To control the browser
â€‹
Continue changing between Centers for 30 minutes, then log out. You will find that Cloud Flare has disappeared and Google Captcha has appeared. If you fill in the data, it tells you that there are empty fields. I do not know how to bypass that.
â€‹
videos to the problem https://drive.google.com/drive/folders/1QLqdwmPSjhnmC17qmhCyKrx4pK-zNYom
â€‹
I stopped the api and installed a browser add-on that solves the captcha. What should I do? I am tired of this problem",1,2024-02-08 12:06:02
Healthcare Scrape Tool,"Hello, I'm looking for a Chrome extension app that will scrape a webpage (on-demand) to capture the data entered into a web form (mostly text, checkboxes and dropdowns), then parse and API that data to a 3rd party system.  The purpose of this workflow is to reduce manual data entry between two systems.  For example, a user will enter the information into the web form; then this scrape will automatically populate another application.  This is a healthcare application, so the app must be HIPPA compliant.  Does anyone have any recommendations?",1,2024-02-08 11:40:11
Bing package tracking - Carrier list,"The Bing package tracking website seems to work great for scrapping UPS shipment status: https://www.bing.com/packagetrackingv2?packNum=<TrackingNumber>&carrier=<CarrierName>
Anyone have an idea how to find the list of European carriers?
Tried: DPD, DHL ... but doesn't seem to be working
Thanks!",1,2024-02-08 06:36:18
Are Terms of Service legally binding?,"Can I show a future employer a webscraper I built with rotating proxies, or is that a no-go?
Thank you for any advice or resources.",2,2024-02-08 01:31:12
Scraping a Substack I pay for?,Would it be legal to scrape a Substack site that I pay for?,1,2024-02-08 03:42:51
Certified web scraper?,"I've been web scraping for a few years and currently doing so as a freelancer next to my regular job. My first introduction to web scraping was in a data analysis course, but this was a miniscule part of the whole course. I was able to learn almost everything from YouTube, trial and error (and a lot of hair pulling), and from asking people like in this growing community.
Since my original studies were not IT/CS based, I wanted to know if there are any courses and/or certifications that would help me stand out when applying for a web scraping engineer job, as well as hone in my skills better?
I know web scraping itself as a discipline is still fairly new but if I were to look for certificates that would help me stand out, what should I look for and where?",10,2024-02-07 14:11:52
Webscraping Instagram,"Hey, I really need help for an Project that is due Friday.
I need to scrape 1000+ instagram user account's last three Posts for hashtags includet in the caption.
How can i scrape this in Python? I need to make this around 1x per Day.
All of the Existing packages like instagrap, instaloader ... fail after ~ 20 Attempts.
Selenium too gets blocked after ~40 tries  
Does anyone know how do get around this?",0,2024-02-07 23:09:45
Hey I'm totally new to scraping. Been watching a couple videos and wanted some info regarding getting leads,I would love to partner up if this is possible as it has unlimited income potential. What I'm looking for is a way to generate leads through finding the organic info from visitors on other companies website. Such as Sunrun or Momentum solar and tapping into people who have logged into their website? Is this a real or totally dreaming over here?,0,2024-02-07 19:28:36
Legality of scraping House.gov pdf links,"Hello everyone. I am trying to download all financial disclosure reports for US house members. You can download an xml file from house.gov that has doc ID from their website, and if you look at the pdfs you're able to download from the website, theres a pattern that goes <year>/<docId>.pdf. I was wondering the legality of downloading all financial reports with a script by just iterating through their structures pdf links. I know it's public information but I am worried about querying so much data in a way that is not accessible via their API but instead through a tedious manual search function that they have. I just want to take some extra precaution since its a government website haha",3,2024-02-07 06:35:55
How do I scrape Twitter DM Status not logged in,"Been trying to scrape DM status on profiles. It's easy enough logged in using the dev tools or puppeteer. 
There's a ""can_dm"" field that can be scraped which is set to true or false. Problem is, need heaps of accounts to do this at scale due to rate limiting.
I tried pulling the DM status not authenticated but they hide ""can_dm"". I can pull almost every other bit of a profile not logged in except the DM status.
Trying to come up with a method so I can scrape this and just rotate proxies all while not being logged in. 
Appreciate any thought!  
https://preview.redd.it/8whuwvmhj3hc1.png?width=1278&format=png&auto=webp&s=10e35e852a139e8bb99df2b6eb2502b546743c90",2,2024-02-07 05:09:51
I need scrape my posts from FB group,"Hello guys,
My father is member of 2 FB group (1 is private, other is public) and he want to collect only his posts with date. Btw there is more than thousand post from him :). Is there a any automatic script for that? Easy way to collect as even text file. Post with date.
Thanks in advance",1,2024-02-07 07:53:15
Zillow Scraping,"I am beyond stuck at the moment:
I only need information from certain addresses
Where I currently am: high level code

utilizing python , webdriver ~> Seleniumwire
webdriver.firefox()

Driver get zillow/specific address
Time sleep(5)
For requests in driver: 
Try
Load(response body)
Except
Continue
Driver quit
Two captcha windows pop up and it keeps saying try again and again and again even though i click and hold and the check mark passes 
Appreciate any advice on how to move forward, im open to restarting and using another library in python, yes prob not the best way but i prefer python",1,2024-02-07 07:11:14
Job Listings Scraping,"Has anyone come across an API (paid or free) that has access to most job openings in the USA? This is to pull information and not push information to job boards.
I've only come across scraping solutions that run on sites such as Jooble - which is a lower quality site.",2,2024-02-06 18:24:27
Tiktok scraping: How to avoid captcha?,"Trying to scrape urls from the infinite scroll feed, before getting to that feed a captcha always pops up that I have to manually solve. 
I've tried using residential proxies to no avail. 
Any tips?",3,2024-02-06 14:37:23
Twitter Scraping Limits and how to extend them,"Hi all, we have recently built a Twitter Tweet and Profile scraper which deposits all scraped information into Sheets. We need to scrape approx 3000 tweets/profiles a day, and I am aware the limits are 600 per day per unverified account right?
I have bought 24 Twitter Accounts (Scraper runs hourly, planned one account per hour to be safe), however I'm having a little trouble finding a proxy solution.
My main question is - is it unsafe to run all 24 (scraper only) accounts from the one VPS IP, even though they are only navigating Twitter and not performing any actions (e.g. likes, tweets etc.)? If not safe, is anyone aware of any proxy services which offer cheap rotating (Data center if thats also safe!) proxys which can automatically rotate at least hourly? Much appreciated",2,2024-02-06 16:45:29
Any good open source alternative to Octoparse?,"I like the software but it can be a bit resource intensive and confusing at times. Nonetheless, the pricing is a bit steep lol
Does anyone else have other recommendations for similar no-code webscrapers that are open source?",2,2024-02-06 16:05:39
How I can get instagram graphql query_hash?,"Uncommon question but related to scraping. Instagram uses query_hash to improve GraphQL network performance by reducing the request size, query_hash is a random string of characters and numbers that summed up - random letters + random numbers (for example: 4sa6as5jld8k3ldf... ) gives ammount of exactly 32 characters. However few years ago query_hash was available in browser dev tools in network tab section. Now it's not (at least not named to anything related to query_hash) I wonder how to get it these days? I'm seraching for quey_hash for user post likers/ user followers so I can scrape their data. I did some reserach I was able to find few posibilities - they use minification to hide the query_hash, moved it somewhere else (for example hidden in javascript) or did something else. I hope somebody who may know that topic will share. Any information will be helpful!",3,2024-02-06 06:49:55
i am trying to scrape a website https://www.sama.gov.sa/en-US/RulesInstructions/Pages/FinanceRulesAndRegulations.aspx it has pagination in the button my code is not clicking on the button i am using this,"next_button= await page.locator('//a[@class=""ms-commandLink ms-promlink-button ms-promlink-button-enabled""]')         if not next_button:             break  # Exit the loop if there's no ""Next"" button          await next_button.click()",1,2024-02-06 13:57:11
Ever experience ISP provided routers blocking requests?,"In testing a scraping script, I noticed if I batched a thousand or more requests(to a proxy) at once I would get 98% failure rate with an ""ETIMEDOUT"" error- and more suspicious, other sites just from my browser failed.  After turning on a VPN all this disappeared.  The only thing I can think of causing this is my router is doing some kind of DDOS prevention.  Any idea on what could be causing it?",1,2024-02-06 05:32:31
Any experience scraping facebook groups?,"The facebook api does not expose public group posts and comments unless you're the group admin. Which is why I resort to a regular approach. To use the search, an account is needed, and I tried automating the registration to counter the draconian rate limits imposed by the website, but I haven't gotten any further than the confirmation view, which if any of the proxies got lucky and a confirmation code is sent, the account keeps being suspended unless confirmed from the browser (not selenium, it doesn't work). Got any useful tips in that regard?
I'm using a python requests approach with proxy rotation. Any idea why most if not all requests are detected despite emulating the browser's requests, cookies, headers, ...?",10,2024-02-05 14:05:37
Scraping the Samsung TV App Store,"Anyone have experience scraping the samsung TV app store? Wondering if there's any APIs to it, or a new website to a list of their apps. I used to this URL (samsung.com/us/appstore/browse) to grab info but now it forwards all apps to 404s. Any help/hints appreciated.",1,2024-02-06 01:06:55
What's the best web Scraping project you've done or thought of doing ?,"Hi ðŸ‘‹, I'm just wondering. Just drop your project. I don't care how stupid or genius it sounds.",10,2024-02-05 09:04:03
"Is it possible to scrape hidden data contained in JS object, which is cleared from the DOM after page load?","I'm developing a scraper which uses playwright and a proxy network to scrape company overview information and reviews from Glassdoor. Here is an example URL:
https://www.glassdoor.com/Reviews/NVIDIA-Reviews-E7633.htm?filter.countryId=1&filter.countryId=3
Strangely when I was testing it before on a different company with this URL:
https://www.glassdoor.com/Reviews/eBay-Motors-Group-Reviews-E4189745.htm?filter.countryId=1&filter.countryId=3
I was able to extract the 'apolloState' object that I'm interested in, which is contained within a script tag in the HTML directly. But now for NVIDIA it doesn't seem to be able to find it.
As far as I understand, Glassdoor uses GraphQL to populate data such as company reviews. I'm interested in the script tag which contains the object 'window.appCache' as shown in the screenshot in the element tab of the devtools.
elements tab, window.appCache
However, as you can see it is only javascript with no properties containing the reviews data, but when I check the source HTML in the sources tab of the devtools, as shown in the screenshots, the 'window.appCache' object has an 'apolloState' property which contains json objects with all the overview and reviews data I want to extract.
sources tab, window.appCache
sources tab, apolloState
Am I right in my understanding, that javascript runs the script on page load to populate the elements of the page with its corresponding key value pairs from the apolloState's json and then the script removes the â€˜apolloStateâ€™ object from the DOM? If this is correct, should I then be trying to extract the 'apolloState' object before javascript runs the script? And how should I go about doing this?
Also, not sure if it is correct to refer to the 'apolloState' as an object or property, apologies for that.
When I refresh the page and filter by document requests in the network tab of the devtools, I can also find the 'apolloState' object in the script tag, as shown in these screenshots:
network tab, window.appCache
network tab, apolloState
I've been searching for an answer to this question, but I haven't found anything that completely answers it for me. If anyone can shed some light on the issue it would be greatly appreciated, thank you in advance.
Also, here is the relevant python code which should extract the 'apolloState' object from the HTML.
I think the problem is in the `find_hidden_data()` function, or more accurately that once the page loads the 'apolloState' object is removed from the DOM so the function canâ€™t find it.
I left out the Url class that builds the URLs for me as well since this is already quite a long question. You'll notice it is called to test the script in the `if __name__ == ""__main__"":` block.
Here is the Github repository for the project, in case anyone wants to take a deeper look at the entire code: https://github.com/qma9/glassdoor-scraper
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
from typing import Optional, Dict, Tuple, List
from datetime import datetime
from enum import Enum
import json
import re
import asyncio
from dotenv import load_dotenv
import sys
import os

# Load .env file
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
load_dotenv()

from log.setup import logger, setup_logging

# Configure logging
setup_logging()

# Bright Data headless browser authentication credentials
cred = {
    ""host"": os.getenv(""HOST""),
    ""username"": os.getenv(""USERNAME""),
    ""password"": os.getenv(""PASSWORD""),
}
auth = f'{cred[""username""]}:{cred[""password""]}'
browser_url = f'wss://{auth}@{cred[""host""]}'


def find_hidden_data(result: str) -> dict:
    """"""
    Extract hidden web cache (Apollo Graphql) from Glassdoor page HTML.
    It's either in NEXT_DATA script or direct apolloState js variable.

    Args:
        result (str): The HTML content of the Glassdoor page.

    Returns:
        dict: The extracted hidden web cache data.

    """"""
    # Create a BeautifulSoup object from the response text
    soup = BeautifulSoup(result, ""html.parser"")

    # Find all script tags that contain the appCache data
    script_tags = soup.find_all(
        ""script"", string=lambda text: text is not None and ""window.appCache"" in text
    )

    data = {}

    for script_tag in script_tags:
        # Extract the JavaScript code from the script tag
        javascript_code = script_tag.string

        # Find the start and end of the appCache JSON object
        start = javascript_code.find(""window.appCache = "") + len(""window.appCache = "")
        end = javascript_code.find(
            "";"", start
        )  # find the semicolon after the JSON object

        # Extract the appCache JSON object
        app_cache_json = javascript_code[start:end]

        # Parse the appCache JSON object
        app_cache = json.loads(app_cache_json)

        # Extract the apolloState data from the appCache
        data.update(app_cache.get(""apolloState"", {}))

    if not data:
        logger.error(
            f""No Apollo Graphql or window.appCache js variable found"",
            extra={""soup"": soup.prettify()},
        )

    return data


def parse_overview(apollo_state: str) -> Dict[str, str | int]:
    """"""
    Parse overview from Glassdoor page HTML.

    Args:
        result (str): The HTML content of the Glassdoor page.

    Returns:
        Dict[str, str | int]: A dictionary containing the parsed overview data.

    """"""
    overview_data = {}
    cache = apollo_state  # find_hidden_data(result)
    if not cache:
        logger.error(f""No hidden data found in Glassdoor page html"")
    else:
        try:
            root_query = cache[""ROOT_QUERY""]
            overview_data = next(
                (
                    value
                    for key, value in root_query.items()
                    if key.startswith(""employerReviewsRG"")
                    and isinstance(value, dict)
                    and value.get(""__typename"") == ""EmployerReviewsRG""
                ),
                {},
            )
        except KeyError:
            logger.error(f""ROOT_QUERY key not found in cache"")

    try:
        # Extract company overview information
        overview = {
            ""employer_id"": int(
                overview_data[""employer""][""__ref""].split("":"")[1]
                if ""employer"" in overview_data
                else None
            ),  # comment out later once all companies are retrieved
            # ""employer_name"": overview_data[""employer""][""name""],
            ""number_of_pages"": overview_data[""numberOfPages""],
            ""all_reviews_count"": overview_data[""allReviewsCount""],
            ""rated_reviews_count"": overview_data[""ratedReviewsCount""],
            ""overall_rating"": overview_data[""ratings""][""overallRating""],
            ""ceo_name"": (
                overview_data[""ratings""][""ratedCeo""][""name""]
                if overview_data[""ratings""][""ratedCeo""] is not None
                and ""name"" in overview_data[""ratings""][""ratedCeo""]
                else None
            ),
            ""ceo_rating"": overview_data[""ratings""][""ceoRating""],
            ""recommend_to_friend_rating"": overview_data[""ratings""][
                ""recommendToFriendRating""
            ],
            ""culture_and_values_rating"": overview_data[""ratings""][
                ""cultureAndValuesRating""
            ],
            ""diversity_and_inclusion_rating"": overview_data[""ratings""][
                ""diversityAndInclusionRating""
            ],
            ""career_opportunities_rating"": overview_data[""ratings""][
                ""careerOpportunitiesRating""
            ],
            ""work_life_balance_rating"": overview_data[""ratings""][
                ""workLifeBalanceRating""
            ],
            ""senior_management_rating"": overview_data[""ratings""][
                ""seniorManagementRating""
            ],
            ""compensation_and_benefits_rating"": overview_data[""ratings""][
                ""compensationAndBenefitsRating""
            ],
            ""business_outlook_rating"": overview_data[""ratings""][
                ""businessOutlookRating""
            ],
        }

    except KeyError as e:
        logger.error(f""Key {e} not found in overview_data"")
        raise

    return overview


def parse_reviews(apollo_state: str) -> Dict[str, Dict[str, str | int]]:
    """"""
    Parse data from Glassdoor page HTML.

    Args:
        result (str): The HTML content of the Glassdoor page.

    Returns:
        Dict[str, Dict[str, str | int]]: A dictionary containing parsed review data.
            The keys are review IDs and the values are dictionaries containing
            various attributes of the review, such as review ID, employer ID,
            date and time, ratings, job details, location, pros and cons, etc.
    """"""
    cache = apollo_state  # find_hidden_data(result)
    if not cache:
        logger.error(f""No hidden data found in Glassdoor page html"")
    else:
        try:
            root_query = cache[""ROOT_QUERY""]
            reviews_data = next(
                (
                    value[""reviews""]
                    for key, value in root_query.items()
                    if key.startswith(""employerReviewsRG"")
                    and isinstance(value, dict)
                    and value.get(""__typename"") == ""EmployerReviewsRG""
                    and ""reviews"" in value
                ),
                [],
            )
        except KeyError:
            logger.error(f""ROOT_QUERY key not found in cache"")

    try:
        # Extract city and job title names
        city_job_title = {
            key: value
            for key, value in cache.items()
            if key.startswith((""City"", ""JobTitle""))
        }
    except KeyError as e:
        logger.error(f""Key {e} not found in cache"")

    try:
        # Extract reviews
        reviews = {}

        for review in reviews_data:
            extracted_review = {
                ""review_id"": review[""reviewId""],
                ""employer_id"": int(review[""employer""][""__ref""].split("":"")[1]),
                ""date_time"": datetime.fromisoformat(
                    review[""reviewDateTime""].replace(""T"", "" "")
                ),
                ""rating_overall"": review[""ratingOverall""],
                ""rating_ceo"": (
                    review[""ratingCeo""] if review[""ratingCeo""] is not None else None
                ),
                ""rating_business_outlook"": review[""ratingBusinessOutlook""],
                ""rating_work_life_balance"": review[""ratingWorkLifeBalance""],
                ""rating_culture_and_values"": review[""ratingCultureAndValues""],
                ""rating_diversity_and_inclusion"": review[""ratingDiversityAndInclusion""],
                ""rating_senior_seadership"": review[""ratingSeniorLeadership""],
                ""rating_recommend_to_friend"": review[""ratingRecommendToFriend""],
                ""rating_career_opportunities"": review[""ratingCareerOpportunities""],
                ""rating_compensation_and_benefits"": review[
                    ""ratingCompensationAndBenefits""
                ],
                ""is_current_job"": bool(review[""isCurrentJob""]),
                ""length_of_employment"": review[""lengthOfEmployment""],
                ""employment_status"": review[""employmentStatus""],
                ""job_ending_year"": (
                    review[""jobEndingYear""]
                    if review[""jobEndingYear""] is not None
                    else None
                ),
                ""job_title"": (
                    review[""jobTitle""][""text""]
                    if review[""jobTitle""] is not None and ""text"" in review[""jobTitle""]
                    else (
                        review[""jobTitle""][""__ref""]
                        if review[""jobTitle""] is not None
                        and ""__ref"" in review[""jobTitle""]
                        else None
                    )
                ),
                ""location"": (
                    review[""location""][""__ref""]
                    if review[""location""] is not None
                    else None
                ),
                ""pros"": review[""pros""],
                ""cons"": review[""cons""],
                ""summary"": review[""summary""],
                ""advice"": review[""advice""],
                ""count_helpful"": review[""countHelpful""],
                ""count_not_helpful"": review[""countNotHelpful""],
                ""is_covid19"": bool(review[""isCovid19""]),
            }

            # Add job title
            if extracted_review[""job_title""] in city_job_title:
                extracted_review[""job_title""] = city_job_title[
                    extracted_review[""job_title""]
                ][""text""]

            # Add city name
            if extracted_review[""location""] in city_job_title:
                extracted_review[""location""] = city_job_title[
                    extracted_review[""location""]
                ][""name""]

            # Add review to reviews
            reviews[review[""reviewId""]] = extracted_review

    except KeyError as e:
        logger.error(f""Key {e} not found in review"", extra={""review"": review})
        raise

    return reviews


async def scrape_data(
    url: str, max_pages: Optional[int] = None
) -> Tuple[Dict[str, int | float], Dict[str, Dict[str, str | int]]] | None:
    """"""Scrape Glassdoor reviews listings from reviews pages (with pagination).

    Args:
        url (str): The URL of the reviews page to scrape.
        max_pages (Optional[int], optional): The maximum number of pages to scrape. Defaults to None.

    Returns:
        Tuple[Dict[str, int | float], Dict[str, Dict[str, str | int]]]: A tuple containing the overview information and the scraped reviews.

    """"""
    logger.info(""scraping reviews from %s"", url)

    async with async_playwright() as pw:
        browser = await pw.chromium.connect_over_cdp(browser_url)
        page = await browser.new_page()

        # Only allow document requests
        await page.route(
            ""**/*"",
            lambda route, request: (
                route.continue_()
                if request.resource_type == ""document""
                else route.abort()
            ),
        )

        # Navigate to the first page
        await page.goto(url, timeout=120000)

        # Create a locator for the script
        script_locator = page.locator(""body > script:nth-child(4)"")

        # Wait for the script to load
        await script_locator.wait_for(timeout=60000)

        # Extract the apolloState property from the window.appCache object
        apollo_state = await page.evaluate(""() => window.appCache.apolloState"")

        if not apollo_state:
            logger.error(""window.appCache.apolloState is not present in the script tag"")
            return None

        overview = parse_overview(apollo_state)
        reviews = parse_reviews(apollo_state)

        total_pages = overview[""number_of_pages""]

        if max_pages and max_pages < total_pages:
            total_pages = max_pages

        logger.info(
            ""scraped first page of reviews of %s, scraping remaining %d pages"",
            url,
            total_pages - 1,
        )

        for page_num in range(2, total_pages + 1):
            page_url = Url.change_page(url, page=page_num)
            await page.goto(page_url, timeout=120000)
            script_locator = page.locator(""body > script:nth-child(4)"")
            await script_locator.wait_for(timeout=60000)

            # Check if the window.appCache object is present in the script
            app_cache_present = await page.evaluate(
                ""() => !!window.appCache"",  # This JavaScript code checks if window.appCache is defined
                script_locator.first(),  # This is the element to execute the JavaScript code in
            )

            if not app_cache_present:
                logger.error(
                    ""window.appCache is not present in the script tag on page %d"",
                    page_num,
                )
                continue  # Skip this page and move on to the next one

            result = await page.content()

            if result:  # Check if the page was successfully scraped
                new_reviews = parse_reviews(result)
                reviews.update(new_reviews)
            else:
                logger.error(""failed to scrape %s"", page_url)

            # Add a delay
            await asyncio.sleep(1)

        logger.info(
            ""scraped %d reviews from %s in %d pages"", len(reviews), url, total_pages
        )

    return overview, reviews


if __name__ == ""__main__"":
    # test script
    overview, reviews = asyncio.run(
        scrape_data(
            Url.reviews(
                ""NVIDIA"",
                ""7633"",
                regions=[Region.UNITED_STATES, Region.CANADA_ENGLISH],
            ),
            max_pages=1,
        )
    )
    print(""\n\n"")
    print(""Overview:"", overview)
    print(""\n\n"")
    print(""Reviews:"", reviews)

â€‹",1,2024-02-05 10:59:51
What are the best options for web scraping with low level languages ?,"I've scraped the Web using languages like Python and Typescript many many times. But these days, I'm kind of into low-level languages like cpp & rust. I wonder what other low-level languages and libraries people use to scrape the web. So can you do me a favour and drop what you know in the comments?",8,2024-02-04 21:32:08
Furniture scraping help,"Hii people, I am quite new to scraping, although a bit familiar with python and have also used beautiful soup for very basic stuff recently. Currently I am tasked with writing a python program to get images from ikea with short description of the images and with the filters which the ikea website provides such cost, colour, material etc.
Anyone ahs any suggestions on how to approach this or anyone has any readymade code for this?
Any help would be great, thanks",1,2024-02-05 08:13:32
"Puppeteer keeps timing out, why? [Docker, Puppeteer, NodeJS]","Right now I'm trying to archive a page with Docker and Puppeteer, here is my Dockerfile and here is my TypeScript code. My code works perfectly fine with Bun on my host machine so I dont know why its not working. I keep getting a custom Timeout error after 30 seconds, even with it disabled it just waits forever for google.com  
First I use Bun (also tried with node:slim) and I download Chromium
FROM oven/bun:latest as base

# We don't need the standalone Chromium
ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD true

# Install Google Chrome Stable and fonts
# Note: this installs the necessary libs to make the browser work with Puppeteer.
RUN apt-get update && apt-get install gnupg wget -y && \
  wget --quiet --output-document=- https://dl-ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor > /etc/apt/trusted.gpg.d/google-archive.gpg && \
  sh -c 'echo ""deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main"" >> /etc/apt/sources.list.d/google.list' && \
  apt-get update && \
  apt-get install google-chrome-stable -y --no-install-recommends && \
  rm -rf /var/lib/apt/lists/*

â€‹
Here is my docker-compose.yml just in case
version: '3'
services:
  m4carchiver-frontend:
    image: node:18
    working_dir: /app
    environment:
      - NODE_ENV=development
    ports:
      - ""3000:3000""
    volumes:
      - ./frontend:/app
    command: >
      sh -c ""npm install && npm run dev""
    networks:
      - m4carchiver_network

  m4carchiver-backend:
    build:
      context: ./backend
    #image: oven/bun:latest
    working_dir: /app
    volumes:
      - ./backend:/app
    ports:
      - ""80:80""
      - ""443:443""
      - ""4000:4000""
    command: >
      sh -c ""bun run index.ts""
    networks:
      - m4carchiver_network 

networks:
  m4carchiver_network:
    driver: bridge

Then I open up a new browser
    browser = (browser !== undefined) ? browser : await puppeteer.launch({
        executablePath: '/usr/bin/google-chrome',
        headless: 'new',
        args: [ 
            '--disable-gpu',
            '--disable-dev-shm-usage',
            '--disable-setuid-sandbox',
            '--no-sandbox'
        ],
    });

    //some other code...

    // Create a new page
    const page = await browser.newPage();
    //await page.setDefaultTimeout(0);
    //await page.setRequestInterception(true);
    //page.on('response', onResponse);

    // Navigate to the specified URL
    console.log(""Opening new page..."");
    await page.goto('https://www.google.com/');

    // Wait for a moment to ensure the page is loaded
    await Promise.race([
        new Promise(resolve => setTimeout(resolve, 60000)),     // Timeout of 60000ms
        page.waitForNavigation({ waitUntil: 'networkidle0' })   // Wait until there is no more network activity
    ]);

    // Close the page
    page.close();

â€‹",1,2024-02-04 23:13:05
Udemy courses web scrapping,"So there's a extension that's let you download any video you are currently watching (Video DownloadHelper) but there is a limitations to it you can download.
Obviously you can copy url and can do manually but when there is site protection such as cloud flare then you can't do it using JDownloader directly you have to use the extension but then again there is a limit for free users.
So is there a way to automate the scrapping the url and then somehow bypass cloudflare protection.",1,2024-02-03 17:26:07
Someone need to build llm based webscrapping,"I hate this!!!!   
need llm based webscrapping tool which has good ux  Someone need to build this,   
no one like to do this  
need llm based webscrapping tool that has good ux  Someone need to build this,   
https://preview.redd.it/gwc1qm3mtcgc1.png?width=898&format=png&auto=webp&s=7ae1f10351c577bbe49e69674dc8e9375de123c1
â€‹",0,2024-02-03 11:17:33
Scraping non documented apis,"What are your thoughts on scraping non documented apis? Is it okay to do?
What I mean by non documented is where certain data is displayed publicly on a page and instead of parsing the html, I open the networks tab and create a script to send a xhr request directly to the api and iterate on that using different parameters.",1,2024-02-02 15:14:18
Chrome extension for PDF download automation,"Hello everyone, 
Iâ€™m searching for a chrome extension (not an external app) that enables me to automate two specific actions without coding 

automatically navigate from one page to another 
clicking a button to download a PDF 

Any recommendations ? Thanks for the help !",1,2024-02-02 10:06:58
Need help creating xpath,"Hi, Iâ€™m brand new to webscraping and Iâ€™m doing some basic practice. Im stumped on creating xpaths for pos, age, G, Cmp, Att, and college. Here is link to data and what I have so far. 
https://www.pro-football-reference.com/years/2017/draft.htm",1,2024-02-01 23:34:47
Monthly Self-Promotion Thread - February 2024,"Hello and howdy, digital miners of /r/webscraping!
The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!

Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?
Maybe you've got a ground-breaking product in need of some intrepid testers?
Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?
Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?

Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!
Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",9,2024-02-01 11:01:56
Need help extracting locations from store finder page,"Hi there, I'm new here and am having a hard time navigating this page, specifically how to extract the locations without having to copy them one by one. Anyone have any tips for someone who has little to no coding experience / doesn't have budget to join any apps?
Thank you all for your help!",1,2024-02-01 20:34:48
Is there a zero budget way to scrape 20 TikTok links for public facing engagement metrics?,"For context, we outsource work to TikTok creators. For our reports, it is such a manual and tedious process to manually navigate to each link and pull the number of views, saves, shares and comments into a spreadsheet.  
It feels like such a simple, logical task to automate and wouldn't spam requests but I simply can't find success using ImportFromWeb/ImportFromXML formulas or even trying to create a GPT agent. 
Any advice at ALL would be appreciated!",0,2024-02-01 10:54:30
Self-built Python Webscraper not working... help?,"Hello,
I'm a beginner developer punching way above my weight.
All I want is to feed into my python script two inputs: 

a .csv containing URLs to crawl
a .csv to append scraped email addresses to

Such that if I run:
python3 pyScrape.py pyTest.webarchive-001.csv pyEmails.csv
â€‹
I should be able to open pyEmails.csv in x hours and have a list of emails.
â€‹
Unfortunately when I run the script, nothing happens. No error, no output.
I used Selenium because I read that it's best for navigating URLs and clicking more links.
â€‹
import csv
import re
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from concurrent.futures import ThreadPoolExecutor

# Function to validate URL
def is_valid_url(url):
  try:
    result = urlparse(url)
    return all([result.scheme, result.netloc])
  except ValueError:
    return False

# Function to append data to a CSV file
def append_to_csv(file_path, data):
  with open(file_path, 'a', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(data)
  print(f""New email added: {data}"")

def scrape_website(website, output_file_name):
  options = Options()
  options.add_argument('--headless')  # Run Chrome in headless mode
  options.add_argument('--no-sandbox')
  options.add_argument('--disable-dev-shm-usage')

  browser = webdriver.Chrome(options=options)

  if is_valid_url(website):
    print(f""Crawling URL {website}"")
    try:
      browser.get(website)
      page_content = browser.page_source

      email_regex = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+'
      extracted_emails = re.findall(email_regex, page_content)

      if extracted_emails:
        for email in extracted_emails:
          append_to_csv(output_file_name, [email.lower()])

      # Extract links from the page
      links = browser.find_elements_by_tag_name('a')
      for link in links:
        href = link.get_attribute('href')
        if href:
          if is_valid_url(href):
            print(f""Crawling Link {href}"")
            try:
              browser.get(href)
              linked_page_content = browser.page_source
              linked_page_emails = re.findall(email_regex, linked_page_content)

              if linked_page_emails:
                for email in linked_page_emails:
                  append_to_csv(output_file_name, [email.lower()])
            except Exception as e:
              print(f""Error accessing link {href}: {str(e)}"")
              continue
          elif href.startswith('mailto:'):
            email = href[7:]
            append_to_csv(output_file_name, [email.lower()])
    except Exception as e:
      print(f""Error accessing {website}: {str(e)}"")
      return

  browser.quit()

def scrape_emails(websites_file, output_file_name):
  print(""Script started."")
  with open(websites_file, 'r') as file:
    reader = csv.reader(file)
    websites = [row[0] for row in reader]

  with ThreadPoolExecutor() as executor:
    for website in websites:
      executor.submit(scrape_website, website, output_file_name)

  print(f""Scraping completed. The {output_file_name} file has been updated with new unique emails."")

â€‹",1,2024-02-01 09:02:51
How can I scrape all of the available URLs of a listing site?,"How can I scrape all of the available URLs of a listing site? for example, I want to get all available URLs of a marketplace site. I guess they have around 3,000 items. I'm a python & javascript expert. I know how to scrape data from URLs that I can access to but I don't know how to get all URLs. Should I scrape from search engines? or is there a way to see website URLs directory?",2,2024-02-01 03:55:58
How legal is it to scrape data from TikTok,"I'm working on a project where I need to access and display publicly available data from TikTok user profiles, such as engagement metrics and video statistics. I'm considering developing a web scraper for this purpose but I am uncertain about the legal implications.
Has anyone here undertaken a similar project, and if so, how did you navigate these legal waters?",9,2024-01-31 10:57:32
Wealth Manager Software Providers,"Within the Wealth Management space there are software providers that offer performance reporting (how is your portfolio doing over the past month, qtr, yr, etc.). 
Wealth manager websites have links to longin URLs. Within this URL it says the name of the service provider.
Iâ€™m wondering if you can scrape the web and pull the wealth manager url along with the url of the service provider.
Example
Company URL - https://gentrustwm.com
Client Login URL - https://gentrustwm.com/client_login
Link for login -  https://id.addepar.com/oauth2/authorize?response_type=code&scope=session&client_id=iverson&state=%7B%7D&code_challenge=9fccba72f75becdec848eeed4c8b1e8ce958685e054d3dcda2180cad563d8d81&redirect_uri=https%3A%2F%2Fgtwm.addepar.com%2Foauth2%2Fcb&firm=gtwm- 
Iâ€™m looking to grab the company URL and where it says Addepar. 
Is this possible?",1,2024-01-31 14:13:15
Bypassing ReCaptcha in 2024?,"I'm currently trying to scrape a webpage periodically which sadly requires me to solve a Google Captcha. Are there any free or paid working methods to bypass ReCaptcha? The endpoint requires captcha_response and visibleCaptcha values to allow the request.
Thanks in Advance",2,2024-01-30 22:46:01
Any reliable source for my Scraping project.,"I am looking to scrape all the  Optometry/Eye Care Clinics in the USA.
Data I want to scrape are Practice Name, business email, and website, for each clinic.
My initial thought for this was Google search paired with Clinic's website.
What are your suggestions?a  and are there any public directories where I can find all the data easily.",1,2024-01-30 17:51:23
Extracting data of a sub reddit.,"Hi all.
I am trying to extract all the data(data as in all posts) from a sub reddit. I am aware of PRAW but I think it has some limit. Is there any workarounds to do this?",4,2024-01-29 15:39:48
Python Web Scraping using asyncio (opinion needed),"I want to write an application that compiles links to national news bulletins from different sites using asyncio on Python and turns them into a bulletin containing personalized tags. Can you share your opinions about running asyncio with libraries such as requests, selectolax etc.?

Is this asynchronous programming necessary to write a structure that will make requests to multiple websites and compile and group the incoming links? Or is time.sleep enough?
Could it be more efficient to check links on pages with a simple web spider?
Apart from these, are there any alternative methods you can suggest?",1,2024-01-29 17:55:00
AWS,"Any one here has an experience with running Puppeteer, and puppeteer-cluster in AWS lambda and SQS?",0,2024-01-29 11:53:29
Booking.com scraping,"Hello,
I'm a student new to web scraping, currently working on a university project. We need to scrape data from booking.com, starting with 1 and extending to 37 pages.
We attempted using a loop (for i in range(0, 901, 25)), but it's not working as expected. Could you please provide guidance on scraping all pages effectively?
Here is the link if you need: https://www.booking.com/searchresults.en-gb.html?label=gen173nr-1BCAEoggI46AdIM1gEaIgBiAEBmAEZuAEXyAEP2AEB6AEBiAIBqAIDuAKQqd2tBsACAdICJDFiM2JmMTk4LTI0ZDQtNDQ1YS1iM2JkLTExMGMxZDFjNDBkOdgCBeACAQ&sid=3ac9bb671d8a29bbb469e531db5d2ffd&aid=304142&ss=Londonas&ssne=Londonas&ssne_untouched=Londonas&lang=en-gb&src=index&dest_id=-2601889&dest_type=city&checkin=2024-05-03&checkout=2024-05-05&group_adults=2&no_rooms=1&group_children=0&nflt=ht_id%3D204&offset=0&soz=1&lang_changed=1 
â€‹
Thank you :))",1,2024-01-29 09:28:39
scraping linkedin,I'm getting started with scraping linkedin. All I need is a local program to repeatedly process hundreds of linkedin URLs and determine if the respective URL account is active or not and fetching to three most recent posts. I don't want to use an API. One of the hurdles is surpassing linkedin blocking me because of repeated visits. Currently I'm running my code with vpn but that still requires me to reload an IP. How to surpass this hurdle and what other hurdles is it possible I face? any opinion is appreciated.,1,2024-01-29 04:45:13
Bet 365 SCRAPE,"Is there any website scraping the odds of bet365 or something similar? Showing the best odds or wtv?
I want to scrape bet365 but seems like a difficult task, should I scrape the odds from another betting site or there is website re showing odds of bet365 online?",0,2024-01-29 00:57:49
Looking to Scrape A List of Thousands of Titles & Artists from A Website,"Hello,
I have a couple of websites that contain a list of thousands of movie and book titles and their artists/authors. They can't all fit in one page so there are maybe 30-50 per page. I want to grab the entire multi-page list and put it into a CSV list.
My goal is to search my local libraries for these thousands of titles and see which are available for borrowing. It would take months or years to do this one at a time. I found a website where I could enter a book or movie title and it tells me if nearby libraries have it available. A great resource and it even accepts huge lists of titles to check, but the website requests an CSV file for that.
The webpages I want to scrape the lists from are not open webpages and I must login to access. I thought I'd mention that in case credentials are an obstacle in web scraping.",0,2024-01-29 00:44:19
"Looking to scrape websites for business name, address of business, city, state.","I've never done a web scrape before, and looking to pull a bunch of location info (just basic as described in the title) to an excel sheet. Most of the websites have filters, and will want to utilize those filters. I am not a programmer, and looking for an easy solution that I can just pay for. I read something about Octoparse being decent? Looking for suggestions.",2,2024-01-28 15:54:34
Amazon scraping,"I'm using https://www.scraperapi.com/ forscraping product data from Amazon, the problem i have is that sometimes the I don't receive fully rendered HTML. I know it sounds like the problem of this specific service, but It turns out that I can recreate this problem locally. I'm talking mainly about sections like: description and rich content. I'm scraping information once per day and I'm monitoring how those product data is changing, so I would like to get rid of random fluctuation through days.
What I want is to ask you did you have simmilar problem? If yes, how did you solve it? If not, can you give me tips, how can i reach very high performance in scraping Amazon Data? The most important thing is consistency and reliability of scrapped data. I don't mind using 3th party solutions and I also don't see problem in large number of retries.",1,2024-01-28 21:44:22
How to find clients to sell data to,"Hello everyone! I recently learned Web Scraping with Python and now I'm facing a challenge.
I don't need the data that I scrape, so I am looking to sell it.
I'm looking for advice on where I can find clients to either:
- Request certain data from me and buy it
- Sell the data that I already have
And which way is more preferable? Any help appreciated!",2,2024-01-28 16:09:34
I need to scrape bulk data of google business site URLs from the internet in my area. Is there any way to do that?,"Ideally I am looking for some tool that will scrape from GMBs, Google Maps or any other directory businesses which have an active Google Business Site will be on. So i will need some kind of a scraper which would help me sort specifically for those.",3,2024-01-28 08:23:19
Would like help scraping thousands of public domain stock images and sharing them for free,"This is a long story to bear with me. So for some extra context, finding old stock photos isn't very easy. You can find blogs with a few of them here and there or old CD ROMs from the 90s, but if you want to find 30s-70s stock photography in bulk you're kinda screwed. These photos are really useful when wanting to create vintage looking advertisements and researching commercial art and historical aesthetics. During my search for these, I found H. Armstrong Roberts, basically the first stock photographer who started selling his photos commercially as early as 1920. Roberts library was distributed by a company named Retrofile in the 2000s along with an assortment of other old stock libraries that are currently unknown in their origin. Retrofile was bought in 2005 by Getty Images, and while a good chunk of those libaries existed on ClassicStock and RobertStock, those were also bought out by Getty later on. As of right now, Getty owns the entire Armstrong Roberts/Retrofile archive.
Here's the thing: Roberts died in 1947. Not only did he die in 1947 but none of his photos had their copyright registered. Given that he died over 70 years ago, his entire body of work is public domain. Given that Getty also lists Retrofile's other archives under Roberts name even though he was already dead, theoretically these are also public domain until proof can be provided that they belonged to someone else. They are listed under his name, so they should be treated as such.
Alamy currently hosts the ClassicStock/Roberts archive on their website, and while it isn't as complete as Getty's, what makes this special is images can be downloaded freely from Alamy using downloader.la. While they aren't super high quality, they are usable when working on a 1080p canvas.
What I want to do is figure out a way to download the entire Roberts archive in bulk, watermark free, with the titles as their file names and then upload those to the Internet Archive (or some other file service). There is no reason why these photos should be behind a paywall and I think it would be of great use to many designers and researchers.
I cannot program in Python or any other language, but I do have a very weak understanding in how to run said scripts and have done so before. I request someone who is interested in this project and would be open to writing a script that can download the entire collection in bulk using the same system that downloader.la uses (or better if possible.)
The H. Armstrong Roberts collection can be viewed here.
I am not asking for you to store them yourself, I just need a python script or some other tool I can run and I'll take care of the files from there.",7,2024-01-28 02:33:23
How to bypass PerimeterX,"PerimeterX (now Human Security) is an anti-bot software that leverages advanced machine learning and behavioral analytics to accurately identify and block malicious bot traffic in real-time. There is currently 153,000 websites using PerimeterX.
Recognize PerimeterX
To identify PerimeterX's presence on a website, look for these characteristics:

Internal property: window._pxAppId property.
Collector XHR: PerimeterX can operate with or without any external server, in case of external collector, those domains can be used: px-cdn.net, pxchk.net, px-client.net In case of an internal endpoint being used, it's format will likely follow this format: /rf8vapwA/xhr/api/v2/collector
Cookies: PerimeterX set the cookies: _px3, _pxhd, _px_vid

PerimeterX's Device Fingerprinting
PerimeterX employs traditional techniques commonly observed in other anti-bot software, but with a particular focus on WebGL by assessing rendering capabilities. This approach goes beyond the usual strategy of merely collecting parameters and extensions, as seen in most anti-bot solutions.
General fingerprinting

devicePixelRatio
hardwareConcurrency
localStorage
indexedDB
openDatabase
sessionStorage
cpuClass
Navigator.plugins
window.performance

Behavior Analysis
PerimeterX observes various specified events and compiles them into a consistent payload for their collector API endpoint. This suggests that they utilize behavioral analysis. As a result, it's important to be careful when activating these events. To closely replicate human interactions with the page, it is advisable to employ simulation libraries.

touchstart
touchend
touchmove
touchenter
touchleave
touchcancel
mousedown
mouseup
mousemove
mouseover
mouseout
mouseenter
mouseleave
click
dblclick
scroll
wheel

mouse type events coordinates and details are tracked on the following attributes:

coordination_start
coordination_end
movementX
movementY
clientX
clientY

For touch type events, the following attributes are tracked:

touches
changedTouches

Canvas Fingerprinting
PerimeterX utilizes a technique involving the use of unicode special characters for fingerprinting canvas renderings. This method is based on the significant variability in how unicode renders high-entropy elements like emojis.
First test: Renders all characters using ""8px sans-serif"" default font from 0x1F600 to 0x1F64F.
a.font = ""8px sans-serif"";
for (var o = 1, c = 128512; c < 128591; c++)
   a.fillText(_(""0x"" + c.toString(16)), 8 * o, 8),
   o++;
n = Q(a.canvas.toDataURL())

Second test: Renders the following characters using ""6px sans-serif"" default font:
97, 667, 917, 1050, 1344, 1488, 1575, 1808, 1931, 2342, 2476, 2583, 2711, 2825, 2980, 3108, 3221, 3374, 3517, 3524, 3652, 3749, 3926, 4121, 4325, 4877, 5091, 5123, 6017, 6190, 6682, 7070, 11612, 20206, 27721, 41352, 43415, 54620, 55295

And then the characters from 0x2699 to 0x26FF.
Results: https://imgur.com/a/oCyUD48

Notice: Result original resolution has been preserved, notice how small and pixely the rendering is, this allow better fingerprinting of anti-aliasing technique, and produce more entropy while being lighter.

WebGL fingerprinting:
PerimeterX checks for specific Anisotropic extension to detect browser type, among those values:

EXT_texture_filter_anisotropic
WEBKIT_EXT_texture_filter_anisotropic (Safari)
MOZ_EXT_texture_filter_anisotropic (Firefox)

This method is highly efficient for identifying browser type spoofing because the properties involved cannot be falsified through traditional JavaScript proxy or Function override techniques.
Attributes fingerprint: PerimeterX check for the following WebGL attributes:

RENDERER
SHADING_LANGUAGE_VERSION
VENDOR
VERSION
UNMASKED_VENDOR_WEBGL
UNMASKED_RENDERER_WEBGL

WEBGL_debug_renderer_info:

ALIASED_LINE_WIDTH_RANGE
ALIASED_POINT_SIZE_RANGE
ALPHA_BITS
BLUE_BITS
DEPTH_BITS
GREEN_BITS
MAX_COMBINED_TEXTURE_IMAGE_UNITS
MAX_CUBE_MAP_TEXTURE_SIZE
MAX_FRAGMENT_UNIFORM_VECTORS
MAX_RENDERBUFFER_SIZE
MAX_TEXTURE_IMAGE_UNITS
MAX_TEXTURE_SIZE
MAX_VARYING_VECTORS
MAX_VERTEX_ATTRIBS
MAX_VERTEX_TEXTURE_IMAGE_UNITS
MAX_VERTEX_UNIFORM_VECTORS
MAX_VIEWPORT_DIMS
STENCIL_BITS

getShaderPrecisionFormat:

VERTEX_SHADER
FRAGMENT_SHADER
VERTEX_SHADER
FRAGMENT_SHADER
HIGH_FLOAT
MEDIUM_FLOAT
LOW_FLOAT

Rendering fingerprint:
Execute the following shaders for WebGL fingerprinting:
Vertex Shader:
attribute vec2 attrVertex;
varying vec2 varyinTexCoordinate;
uniform vec2 uniformOffset;

void main(){
   varyinTexCoordinate = attrVertex + uniformOffset;
   gl_Position = vec4(attrVertex,0,1);
}

Fragment Shader:
precision mediump float;
varying vec2 varyinTexCoordinate;
void main() {
   gl_FragColor = vec4(varyinTexCoordinate,0,1);
}

Both are executed on a single program and then dumped using canvas.toDataURL.
Clipboard data
Not sure how much this influence the bypass success rate, but PerimeterX does look for Clipboard Data as part of their fingerprinting process.
Fonts presence & rendering
PerimeterX will attempt to render the text mmmmmmmmmmlli for the following fonts list:
""Andale Mono"", ""Arial"", ""Arial Black"", ""Arial Hebrew"", ""Arial MT"", ""Arial Narrow"", ""Arial Rounded MT Bold"", ""Arial Unicode MS"", ""Bitstream Vera Sans Mono"", ""Book Antiqua"", ""Bookman Old Style"", ""Calibri"", ""Cambria"", ""Cambria Math"", ""Century"", ""Century Gothic"", ""Century Schoolbook"", ""Comic Sans"", ...

How to bypass PerimeterX
Bypassing PerimterX's security requires a nuanced approach, especially considering its reliance on GPU rendering information to determine the operating system and device type.
Here are some strategies:

GPU Rendering: Emulate consumer-grade GPUs rather than professional hardware, as Akamai's algorithms are tuned to recognize and differentiate between them.
Behavioral Analysis: Utilize tools like ghost-cursor (found at https://npmjs.com/package/ghost-cursor) to simulate human-like cursor movements and keystrokes. Timing is crucial here; movements or keystrokes that are too rapid can be flagged as suspicious.

Learn more:
We're talking about multiple ways to bypass PerimeterX, Akamai, NuData on the Web Scraping & Data Extraction discord server!
Come say hi!
https://discord.com/invite/fHbbHTq4CQ",12,2024-01-27 05:10:23
Downloading all pdfs (help),"Ive currently started working as a tutor and I'm trying to download all the resources (pdf files) from https://www.physicsandmathstutor.com/ e.g. past papers and quiz sheets
Ive tried beautiful soup but it doesn't seem to folder them but instead just download everything
e.g.https://www.physicsandmathstutor.com/past-papers/gcse-maths/aqa-paper-1/ all the pdfs from this page download to the folder \documents\past-papers\gcse-maths\aqa-paper-1\example-exam-paper.pdf
â€‹
â€‹
Could anyone tell me if there is an easier way?",1,2024-01-27 17:40:49
Any help on how can I scrape resumes online from sources like LinkedIn and any other sites,I want to scrape around 5000 resumes and I don't know from where to start any help,2,2024-01-27 11:32:51
Scrape Data From an Android App?,"I am looking for a way to scrape data from an Android App for educational purposes. There is an app I have in mind that sells groceries and I would like to scrape the items along with their prices. From research I understand that to do this I need an emulator + HTTP Proxy, reverse engineer the API call that receives this data and use python requests to capture it. I have never worked with Emulators and HTTP proxies before; so here are the questions:
1) Is there an easier way to do it? 
2) are there open source/free tools for HTTP proxy and emulators? I found ones online and they seem to be for a price.
3) Any proper threads/resource I can use for educational purposes? I basically learned web scraping through selenium, beautiful soup and API but all from web pages. 
â€‹
much thanks!",6,2024-01-26 18:30:10
"How to ""submit"" injected reCaptcha v2 solution when no button identified? (Selenium - Python)","Below is what I have so far, but it doesn't work.. I found a textarea marked g-recaptcha-response and injected into it a TwoCaptcha-generated solution. But I have no way to ""Submit"" it. Cannot locate any button and focusing on it and pressing Enter doesn't seem to work unless I'm doing something wrong.
Any solutions are welcome.. The site generating the reCaptcha v2 prompts the challenge upon clicking ""submit"" on my search request. No ""Submit"" inside the reCaptcha area was found, just ""Verify"".
target url: https://urlscan.io/
```
        # Solve the reCAPTCHA
        result = solver.recaptcha(sitekey, driver.current_url)
    # Inject the result into the hidden textarea
    driver.execute_script('document.getElementById(""g-recaptcha-response"").innerHTML = ""%s""' % result['code'])

    # Make the g-recaptcha-response textarea visible for interaction
    recaptcha_response_area = driver.find_element(By.ID, ""g-recaptcha-response"")
    driver.execute_script(""arguments[0].style.display = 'block';"", recaptcha_response_area)

    # Create an ActionChain to perform keyboard actions
    actions = ActionChains(driver)

    # Move to the textarea, focus on it, and simulate the Enter key press
    actions.move_to_element(recaptcha_response_area).click().send_keys(Keys.ENTER).perform()

```
edit: formatting",1,2024-01-26 20:16:00
How can I optimise my web scraper?,"I am a learning web scraping and want to build a python bot to track the prices for mobile phones on different websites like amazon, ebay etc.
I have a list of 20-25 websites to start scraping from, some of these websites load data on runtime so I will need to use something like a headless browser for scraping. How scalable is this approach ? Also, are theere any alternative approaches to this ?
Also the websites on my list are very different, is there a way  I can make a general scraper for all the websites?",3,2024-01-26 12:18:57
How to Build a Price Tracking Bot that utilizes real-time data 24/7,"I see many people on Twitter create a price tracking bot which tracks real-time data of when a product drops in price.
They get this data immediately, right when it drops. I'm not sure how this is possible for them to get real-time data without them getting rate limited.
The only way I see that's possible is that they are constantly making the HTTP Request to the specific product 24/7 every second. But this seems too expensive. Especially since their price tracking bots can track thousands and thousands of products.
So what technique are they using to get real-time data for when a product changes prices?
If I were to currently attempt to make one, I would be forced to check prices like every hour or something(so I don't go over the rate limit). How are they bypassing that?",13,2024-01-26 03:10:49
In need of a simple tool to search a website for words that are typically not searchable.,"Hello,
I need a tool that can pull up a search on a website for info that is typically shown only to logged in users but not searchable.
To be more precise, this is the website: https://ecase.justice.bg/Case
Every single day courts from all around the country upload new cases with parties involved in them. The info is â€žpublicâ€œ to logged in lawyers. To the rest of the public itâ€™s also visible but names are â€žblanked outâ€œ - James Brown would be J. B. 
In the website, one can search by numbers/years/courts and so on, not by name of a party involved in a case, and the info is there, itâ€™s just not searchable. 
Googling the website and a name sometimes works for a couple of results but itâ€™s not very helpful because itâ€™s cases from way back that google has indexed.
Is there any tool that I can sort of modify to search on the backend of the website and give me the case number/year that this person/company is involved in?
Something like the reddit camas search from years ago?",1,2024-01-26 04:21:56
Instagram's ?__a=1&__d=dis no longer working,"Instagram's 
?__a=1&__d=dis

no longer seems to work, anyone has workarounds? Thanks",4,2024-01-25 18:53:40
Betting website scraping,"Hello, I was wondering is it still possible to scrape bet365 with ease?
If not, what alternative website would you recommend I switch to?",1,2024-01-25 21:27:14
Livesource Sport,"Hello, I am looking for away to watch sport live, or the closest to live?
Is there any website streaming that is close to live without much delay, like 1-2 secondes delay?
If you also have an api that say the stats of the game I would be interested in live too or mini delay.
Thanks.",0,2024-01-25 21:26:39
Can someone help me reverse engineer this store locator map?,"I have never seen this before. I am trying to extract the store locations from this map and there is almost 0 indication of where the locations are stored or how they are fed into the map on the website.
here is the url:
https://www.waterdrop.fr/pages/revendeurs
â€‹
I have scoured the page source and the network tab for any indications but cannot find any. Any suggestions or recommendations would be highly appreciated!
â€‹
Tta",3,2024-01-25 14:44:56
How feasible is automatically scraping logo on large scale?,"Say I have a list of many websites, and I want to scrape the logo from each of them. This process will need to be mostly automatic since the number of websites is huge (i.e. I can't read the html for each site). How feasible do you guys think this task is?
I'm thinking about third-party solution. But not sure how reliable they are. Currently I'm considering hexomatic.com and ritekit.com. Does anyone have experience with these sites?
Do you guys  have any tips on how to approach this kind of scraping? Thanks",3,2024-01-25 08:03:40
Is there a way to scrape Facebook business pages by creation date?,I need to find businesses created in 2023 or 2024.,2,2024-01-25 05:20:43
Error: Scrapy : pyasn modules,"Ok so i dont know what happened , but this started popping out, i didnt use scrapy for like months ... and then when i started working on a new project this happened ,   
some info: 
On debian bookworm , using conda , even tried with python virtual environment, tried global installation tooo , python version 3.11.5.   
Tried googling and suggestions were to try force upgrading pyasn mod's and even after that nothing so .......anyone facign the issue ? 
https://preview.redd.it/pigd0c261jec1.png?width=958&format=png&auto=webp&s=b310e2f8cc96d7edf9e5e4319d390b5b1e5a5f78",1,2024-01-25 06:02:11
ShapeSecurity's VM,"Hello guys 8 days ago I started writing a series about ShapeSecurity's VM. This is one of the hardest antibots to reverse due to their dynamic Javascript VM that is a pain-in-the-ass to reverse.   
One of the last good open-source repos about about ShapeSecurity was almost 5 years ago
https://github.com/sonya75/starbucks-botdetection-cracked 
Anyways here are part 1 and 2
https://www.botting.rocks/shapesecuritys-javascript-vm-part-1/
https://www.botting.rocks/shapesecuritys-javascript-vm-part-2/ 
I still have a few more parts to write about and then I will start writing more about Incapsula.",10,2024-01-24 17:53:58
SeleniumBase and scrap speed,"Seems that SeleniumBase waits about 4 seconds after the page is fully loaded, and I can't see the reason
I have this demo code:
from seleniumbase import Driver
driver = Driver(uc=True)

def custom_get(url):
    global driver
    import time
    epoch_time = int(time.time())
    print(""starting..."")
    driver.uc_open_with_tab(url)
    if not ""mysite"" in driver.get_title():
        print(""Error, getting new tab"")
        driver.uc_open_with_reconnect(
            url, reconnect_time=2
        )
    if not ""mysite"" in driver.get_title():
        print(""Error, CF is still there"")
        if driver.is_element_visible('iframe[src*=""challenge""]'):
            with driver.frame_switch('iframe[src*=""challenge""]'):
                driver.click(""span.mark"")
                driver.sleep(2)
    # print seconds passed
    print(""!seconds passed:"", int(time.time()) - epoch_time)
    print(""------------------ end"")

custom_get(""https://mysite"")
custom_get(""https://mysite/"")
custom_get(""https://mysite/"")
custom_get(""https://mysite"")
custom_get(""https://mysite/"")
custom_get(""https://mysite/"")
custom_get(""https://mysite/"")
exit()

but i get one request per 5 seconds, even if the page is fully rendered under 1 second  
starting...
!seconds passed: 5
------------------ end
starting...
!seconds passed: 5
------------------ end
starting...
!seconds passed: 5
------------------ end
starting...
!seconds passed: 5
------------------ end
starting...
!seconds passed: 5
------------------ end
starting...
!seconds passed: 5
------------------ end
starting...
!seconds passed: 5
------------------ end

Process finished with exit code 0

â€‹",2,2024-01-24 22:52:34
Login on webpage using Playwright,"Hi all, 
I wrote some Python code to login on a website called sportsbet.com.au. This code works flawless in my Windows environment. However, when I run it in a Linux environment with xvfb (I need the headed mode) then the code is unable to login in since it is detected as a bot. 
â€‹
How can I resolve this?  Added the code below (with obviously, fake credentials)  
from playwright.sync_api import sync_playwright
import time
CHROMIUM_ARGS= [
'--no-sandbox',
'--disable-setuid-sandbox',
'--no-first-run',
'--disable-blink-features=AutomationControlled',
'--disable-dev-shm-usage'
]
playwright = sync_playwright().start() 
browser = playwright.chromium.launch(headless=False, slow_mo=200, args=CHROMIUM_ARGS,ignore_default_args=[""--enable-automation""])
context = browser.new_context(
user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
)
page = browser.new_page()
page.set_viewport_size({""width"": 1600, ""height"": 1200})
page.goto(""https://www.sportsbet.com.au/"")
time.sleep(3)
login_menu = page.locator('[data-automation-id=""header-login-touchable""]')
login_menu.click()
time.sleep(1)
username_button = page.locator(""#username"")
password_button = page.locator(""#password"")
username_button.fill(""user"")
password_button.fill(""pass"")
login_submit = page.locator('[data-automation-id=""login-button""]')
login_submit.click()
time.sleep(20)
browser.close()
â€‹",1,2024-01-24 18:58:07
Transform Your Android Phone into an Efficient Proxy Server,"First, you need to install the Localtonet app on your phone by download it from the Google Play Store here: https://play.google.com/store/apps/details?id=com.localtonet.localtonetapp
Next, go to https://localtonet.com and sign up for an account. Once you're signed up, go to the clipboard page and copy the AuthToken.
On the Localtonet app on your phone, paste the AuthToken in. You should see an Android icon on the ""My tokens"" page.
You can adjust the settings for how often the phone goes into airplane mode, and you can also generate a reset link.
If  your phone is not rooted, when you generate a reset link, trigger it  once and the default assistant settings page will open. From there,  select Localtonet as the default assistant.
Next, go to https://localtonet.com/tunnel/proxyserver and choose either HTTP or SOCKS proxy, then create it. Click ""Start"" to start the proxy server.
You  can connect to the server using the IP address and port number  provided. You can also set a username and password in the tunnel  settings.
Note that the SOCKS5 proxy now supports TCP/UDP.",5,2024-01-24 07:37:58
Confused about mobile Endpoints,"Hello,
I came across tiktok-scraper on github (no longer maintained) and was looking at how they coded up the scraper, afaic tell they were taking the session id FROM THE PC BROWSER session and putting them into a nodejs MOBILE server request  
a line in the script for const request:
https:\/\/m.tiktok.com\/node\/share\/user\/@\w+\?uniqueId=(\w+)&verifyFp=$/.test(uri):  
I'm kind of confused how this can work? Does this assume that the mobile api endpoint won't check the user-agent string and or what proxy/ip is being used (4g vs residential proxy for initial session for example)? I would assume something like this will get blocked immediatley if the sessionid doesn't come from a mobile session with the same user-agent?  
Since I'm new to bulk webscraping, why do they use the mobile endpoint (if I'm not mistaken)? Softer security measures and different limits? Does the response on mobile differ from the response on webbrowser? Do they cut out loading the MB heavy parts of the response in that way?  
Wouldn't I need a unique mobile footprint for each scraper in this case? I know there are services out there that can provide me with that, but my understanding is I would need to intercept each one of the request first for each instance of the mobile connection to get the unique fingerprint first and then  create a mitm connection->then scrape the response + I would need to mimic typical user session behaviour iiuc(?)   
Another thing that is puzzling to me is the use of a nodejs server, I thought birp et al are used for these types of scraping activity?  
Thank you for your time",4,2024-01-24 07:18:34
Article results based on keywords,"How would I go about scraping for articles and websites online that contain user given keyword.
Like a python program where you input a keyword and the output is all the article/website urls that contain the keyword.",1,2024-01-24 05:57:36
Web scraping Udemy with Scrapy and Splash,"Hi everyone, I'm having trouble scraping Udemy websites for courses data. For example, I tried to scrape /topic/web-development and extract its urls to courses. However, theses URLs are returned dynamically. So, I used splash and tried to write Lua script to wait for a few seconds to let the urls load but does not seem to work. Anyone has experience with this please?",1,2024-01-24 03:26:45
Anyone launching in container/kubernetes?,"Was curious if people here scrape in bulk via multiple docker container/kubernetes or similar setups (proxmox)?
I'm going to need to scrape dynamic content so can't really run headless or just request afaik.
How much ram/container would I need on average? How many container can run on one thread if there is a delay (say 5 seconds per container after action/load)?",4,2024-01-23 18:43:46
Want to capture regular updates from a list of 100 websites,"Specifically, I want to monitor a set of websites that publish updates to the law or any new laws. I just want for a tool to detect whenever there are updates/new additions to these sites and alert me and return the new content. I donâ€™t know anything about webscraping, I have a little bit of programming experience.
Is webscraping the right task for this? Are there any free to use webscraping tools that could do this for me?
Also, could there be a functionality that filters new content by topic of law like by keywords, so to update me whenever a new content is added to one of the websites with these keywords in it?
Is web scraping the right approach for this? Sorry, Iâ€™m not very knowledgeable.",3,2024-01-23 20:00:23
vbox7 scraper made pure in go,"Hello everyone, with the recent announcement of vbox7 shout down, I created a bot so you can download the videos on your machine
https://github.com/johnbalvin/vbox7
I already deployed to lambda you can test it:
https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=c6eee5c948&format=htm
https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=3edf5e0539&format=html
https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=9b5b912a3f&format=html
This will give only the final video url, I didn't deploy the full video download because the bill will increase a lot if I do it, but you can implement it yourself
source: https://www.reddit.com/r/DataHoarder/comments/19d8omx/this_videosharing_website_is_going_to_delete_99/
let me know what you want me to add to to the bot
Thanks",7,2024-01-23 14:32:44
403 Forbidden errors when requesting data from Indeed.,"I am looking to build a job posting web scraper. I plan on scraping data off of Indeed, Linkedin, and Monster. I am currently using Cheerio and Axios to request and parse the HTML from Indeed but I am always getting a 403 forbidden error. I am looking for resources or tools that are available to get around this error.",1,2024-01-23 17:55:57
Had anyone done scraping on iPad apps before?,need help or at least some discussions,1,2024-01-23 17:39:27
Launch ephemeral headed chrome instances in Docker,"Run your bots on Chrome in an isolated graphical environment with this Docker image and run script I wrote:

Prints out the WebSocket URL when the you run the launch script. This can be used with Playwright automation
Starts a VNC server in the Docker container, so you can enter and interact with the graphical environment.
TODO: Programattically modify the Xvfb/fluxbox parameters to produce reproducible/unique canvas fingerprints.

https://github.com/gbiz123/docker-chrome-vnc
â€‹",3,2024-01-23 05:50:04
Scraping a CDN of it's images and videos and etc...,Hey I want to scrape all the images video and other files from this cdn link.(https://cdncf.ahlan.live/goods_list/anim/) but i dont know how i should do it,1,2024-01-23 08:23:25
Can't interact with website on scraper that is using selenium and working on docker container,"I have a scraper that I want to run inside my pythton 3.8-slim docker container.
I want to use selenium to open a headless browser, enter data to input boxes, submit via clicking a button and get results from the response page.
I have everything ready and working on my local. But when I move them to the docker, I just can't seem to be able to click on the button.
When I make some search about this I think it's because my OS in the container doesn't have GUI available and that's why?
How can I do this, I can switch from selenium to playwright or similar stacks.
Anyone with past exp/ideas?",1,2024-01-23 07:26:50
I don't use Scrapy. Am I missing out?,"I tried out Scrapy some times ago, but I find it restrictive and not intuitive to me. I find the selector useful though. Hence currently my flow is request/selenium to get html > scrapy selector to parse > sql alchemy to transfer to db. And it works well. 
But I still have a nagging feeling that I may miss something, since Scrapy is the most common scraping framework. Hence I want to check with you guys if I miss out anything for not using Scrapy?",9,2024-01-22 11:49:35
Trying to get all pin ids in a pinterest board,"I'm trying to create a free Chrome extension (that ideally wouldn't rely on the Pinterest APIs) that would allow the user to cycle through the pins in a pinterest board by using the left + right arrow keys.
Seems like the ids in a board are already ordered correctly, so having ALL the ids in a board should allow for the navigation. 
Using the DOM, it seems like you can only get about 10 pins as it's dynamically generated. Any thoughts on how to proceed? Has anyone done this before?",2,2024-01-22 16:45:56
Selenium - issues on a scraper that runs on google-colab,"â€‹
i am trying to get data form a page
see url = ""https://clutch.co/il/it-services""
The website i am trying to scrap from probably has some sort of anti-bot protection with CloudFlare or similar services, hence the scrapper need to use selenium with a headless browser like Headless Chrome or PhantomJS. Selenium automates a real browser, which can navigate Cloudflare's anti-bot pages just like a human user.
Here's how i use selenium to imitate a real human browser interaction:
but on Google-Colab it does not work propperly
â€‹
â€‹
import pandas as pd
from bs4 import BeautifulSoup
from tabulate import tabulate
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

options = Options()
options.headless = True
driver = webdriver.Chrome(options=options)

url = ""https://clutch.co/il/it-services""
driver.get(url)

html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')

# Your scraping logic goes here
company_names = soup.select("".directory-list div.provider-info--header .company_info a"")
locations = soup.select("".locality"")

company_names_list = [name.get_text(strip=True) for name in company_names]
locations_list = [location.get_text(strip=True) for location in locations]

data = {""Company Name"": company_names_list, ""Location"": locations_list}
df = pd.DataFrame(data)
df.index += 1
print(tabulate(df, headers=""keys"", tablefmt=""psql""))
df.to_csv(""it_services_data.csv"", index=False)

driver.quit()

â€‹
i get back this.   
â€‹
SessionNotCreatedException                Traceback (most recent call last)
<ipython-input-6-a29f326dd68b> in <cell line: 41>()
     39 # Example usage
     40 url = 'https://clutch.co/il/agencies/digital'
---> 41 scrape_clutch_digital_agencies_with_selenium(url)

6 frames
/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py in check_response(self, response)
    227                 alert_text = value[""alert""].get(""text"")
    228             raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here
--> 229         raise exception_class(message, screen, stacktrace)

SessionNotCreatedException: Message: session not created: Chrome failed to start: exited normally.
  (session not created: DevToolsActivePort file doesn't exist)
  (The process started from chrome location /root/.cache/selenium/chrome/linux64/120.0.6099.109/chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)
Stacktrace:
#0 0x565c2a694f83 <unknown>
#1 0x565c2a34dcf7 <unknown>
#2 0x565c2a38560e <unknown>
#3 0x565c2a38226e <unknown>
#4 0x565c2a3d280c <unknown>
#5 0x565c2a3c6e53 <unknown>
#6 0x565c2a38edd4 <unknown>
#7 0x565c2a3901de <unknown>
#8 0x565c2a659531 <unknown>
#9 0x565c2a65d455 <unknown>
#10 0x565c2a645f55 <unknown>
#11 0x565c2a65e0ef <unknown>
#12 0x565c2a62999f <unknown>
#13 0x565c2a682008 <unknown>
#14 0x565c2a6821d7 <unknown>
#15 0x565c2a694124 <unknown>
#16 0x7bf3032caac3 <unknown>

any idea - what goes on here - i guess that i am facing selenium issues - while the headless-browser does not work propperly
â€‹
â€‹",3,2024-01-22 11:12:19
trying to get more insights into the use of selenium on google -colab - a handson approach,"â€‹
â€‹
i am trying to get more insights into the use of selenium on google -colab. 
for that i have read lots of documents and tried outsome steps on  colab: now  i w ant to discuss this with you ... 
â€‹
this is the beginning of a tiny - manual of what can go wrong ever  - if you want to run little scraper that does all that - just read on. 
â€‹
btw: this is a very rough manual - lots of  you can add thoughts ideas and more... so come on - add your ideas ...
â€‹
To use Selenium in Google Colab, you first need to install the necessary packages and set up the WebDriver. Here's a simple example of a Selenium web scraper in Google Colab:
â€‹
â€‹
â€‹
â€‹
# `Install necessary packages
!pip install selenium
!apt-get update
!apt install -y chromium-chromedriver

# Set up WebDriver
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

# Specify the path to the chromedriver executable
chrome_driver_path = ""/usr/lib/chromium-browser/chromedriver""

# Create a WebDriver instance
driver = webdriver.Chrome(chrome_driver_path, options=chrome_options)

# Example: Open Google and search for a term
search_term = ""Hello, Google Colab!""
driver.get(""https://www.google.com/"")
search_box = driver.find_element(""name"", ""q"")
search_box.send_keys(search_term)
search_box.send_keys(Keys.RETURN)

# Wait for a few seconds to let the results load
driver.implicitly_wait(5)

# Print the title of the page
print(""Page title:"", driver.title)

# Close the browser window
driver.quit()

```
â€‹
**important**:  Make sure to adjust the chrome_driver_path based on the version of the Chrome browser installed in the Colab environment.
**my guess**: well  i guess that i have to take care for the adjustment of the ""chrome _driver_path""  based on the version of the chrome-brwoser that is installed in the colab environment 
â€‹
see what i ve got back - how  to proceed?
`Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)
Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)
Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)
Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease
Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease
Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease
Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease
Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease
Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease
Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease
Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).
0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-87c7a8d826d3> in <cell line: 19>()
     17 
     18 # Create a WebDriver instance
---> 19 driver = webdriver.Chrome(chrome_driver_path, options=chrome_options)
     20 
     21 # Example: Open Google and search for a term

TypeError: WebDriver.__init__() got multiple values for argument 'options'
`

- how  to proceed?
â€‹
It seems like there's a version mismatch or an issue with the webdriver.Chrome initialization in your code. The error suggests that there's a problem with the arguments being passed to the WebDriver constructor.
Try modifying the webdriver.Chrome initialization line like this:
`
# Create a WebDriver instance`

driver = webdriver.Chrome(executable_path=chrome_driver_path, options=chrome_options)

â€‹
This ensures that the chrome_driver_path is explicitly provided as the executable_path argument. Please update your code accordingly and see if it resolves the issue.
If the issue persists, there might be a compatibility problem between the ChromeDriver version and the Chrome browser version. In such cases, you may need to download the appropriate version of ChromeDriver that matches the installed version of Chrome in your Colab environment.
well -  we can download ChromeDriver from the official website: 
https://sites.google.com/chromium.org/driver/
â€‹
After downloading, upload the ChromeDriver executable to the Colab environment and adjust the chrome_driver_path accordingly.
â€‹
well how to proceed - how to  test the usage of selenium on google-colab!""?",1,2024-01-22 16:07:17
Scrape articles from Apple News widget on iOS,"Hello everyone, 
even though Apple News has officially launched in very few countries, Apple still offers an Apple News widget (see screenshot below) that displays curated articles (as far as I know they are updated once a day). Does anyone have any idea how I can find out the data source for the widget in order to scrape it over a longer period of time and analyze it (topics, media, etc.)? 
Thanks in advance for any kind of hints! 
https://preview.redd.it/vt3903q41zdc1.jpg?width=1284&format=pjpg&auto=webp&s=2802e5489a7aeb8db4143dae664df634b271a481
â€‹",2,2024-01-22 10:46:57
"fun stuff, fu** captcha","working on https://inconsistentcontacts.com
this is getting NYSE company dataâ€¦ to be pushed to the project tonight.
did most of Fortune 500 yesterday - need to complete industry tags and a bunch of other shiâ€¦
Kinda sucks rn but #remindme",20,2024-01-21 18:23:13
Web Scraping Using the Network Tab,"Hey, I am trying to scrape data from a website, and I decided to use the network tab to search or the json response that is being rendered on the front end.
I found the request that yielded the response but when I copied and pasted this URL on my browser I got a response similar to this
 {""code"":40101,""msg"":""no permission"",""request_id"":""2024012207483102E5C956EFC333680D96""} 

I am looking for a remedy around this. I also found an access token response on the network tab that looks something like this
{
    ""code"": 0,
    ""data"": {
        ""tokenType"": ""Bearer"",
        ""token"": ""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwbGF0Zm9ybSI6eyJwbGF0SUQiOiI3MjMyMTEyNzEwNzg2MzUxMTA2IiwiZG9tYWluTGlzdCI6WyJhZHMudGlrdG9rLmNvbS9idXNpbmVzcy9jcmVhdGl2ZWNlbnRlciJdLCJ0cmlnZ2VyS2V5TGlzdCI6WyJjY19haWdjX3N1cnZleV9pbl9zY3JpcHRfcGFnZSIsImNjX2VkdWNhdGlvbl9iaVdlZWtseV90b3BBZHMiLCJjY19lZHVjYXRpb25fc2hvd2NhcmRzIiwiY2Nfc2hvd19zdXJ2ZXlfaW5fZGV0YWlsX3BhZ2UiLCJjY19zaG93X3N1cnZleV9pbl90b29sYm94Il19LCJpYXQiOjE3MDU5MDkyNzgsImV4cCI6MTcwNTkxNjQ3OH0.8mI5R8slfUP7Ie55HO2b26hip0daezfaUt3t09fnMzQ"",
        ""expiresIn"": 7200
    },
    ""message"": ""success""
}

I tried passing this into the url but I still get the unauthorised message.
Any help will be gratly appreciated",1,2024-01-22 07:52:43
What a more professional scraping project looks like?,"Hello reddit,
Currently I have a project where i have between 50-70 spiders in scrapy that i need to run throughout the year. I think my current set up is not bad, but I would like to see more professional pipelines looks like or maybe you have some suggestion for me. Let's use this thread to discuss, it may help more people.
â€‹
My current approach is more or less as follows
- First i collect all urls with needed filters from the websites, and store them neatly in a google sheets (i know, maybe db is better but google sheets allows me to quickly make any changes if need be, and this document is very alive)
- I have individual spiders for each website
- Go through all the items in the website, and then go through the pipelines to clean the data...etc and store the final data into a postgresql database
- Then each page is saved in .html format in my local file system, just in case the data inserted in the database is wrong and i need to restart again, i just scrape the offline files instead of doing it online agai
- Then i have several one-off scripts, but the main one is extract to excel, where i take the data i want, do some analysis of some missing attributes...etc and then produce a final excel file, nicely formated with the results, ready to be sent to a client.
â€‹
This is more or less a high level view, but do you have any other approach to this? do you maybe store the whole HTML in the database? or store the 'unclean' data in database and then have some ETL process?, in my current process i do all the cleaning before i insert into database, so that in database i always have the clean data
â€‹
Thanks!",16,2024-01-21 13:08:22
"New to programming and web scraping in general, in python how can I iterate over a complete site that has URLs end in /xyz/""some random text""","the /xyz/ is always constant however not sure how I can iterate over a site where all the URLs end in different and random text
This is the site in question:
https://wodwell.com/
â€‹
I want to get all the URLs for all the workouts for example
https://wodwell.com/wod/hotshots-19/
I want to scrape the following data
hotshots-19
CROSSFIT HERO WOD

and the entire description text so all the info below
6 Rounds For Time
30 Air Squats
19 Power Cleans (135/95 lb)
7 Strict Pull-Ups
400 meter Run
With a running clock, as fast as possible perform 6 rounds of the work in the order written: 30 Air Squats, 19 Power Cleans, 7 Strict Pull-Ups, and then a 400 meter Run.
Score is the time it takes to complete the 6 rounds.
Movement Standards
Strict Pull-Ups:Â Begin with your hands on the Pull-Up Bar just outside your shoulders, arms fully extended. Without any help/momentum from the lower body (no kip), pull so your chin gets higher than the bar. Complete at full arm extension.
Good Score for â€œHotshots 19â€ (estimated)
â€“ Beginner: 35+ minutes â€“ Intermediate: 30-34 minutes â€“ Advanced: 25-29 minutes â€“ Elite: <24 minutes Tips and Strategy Choose a steady, moderate pace (around 75-80% of your max pace). A fast pace out of the gate will likely lead to burn out and a poor overall score. When you complete round 1, look at the clock. Aim to complete each of the next 5 rounds at the same pace, +/- 30 seconds. The crux of this WOD is going from 19 Power Cleans directly into 7 Strict Pull-Ups: They are both pulling exercises, so youâ€™ll experience major fatigue in your grip, biceps, shoulders, and lats. Break up the Power Cleans early (5+5+5+4 or 8+6+5 for example) to ensure that you donâ€™t get trapped at the Pull-Up BarÂ for too long. Intended Stimulus â€œHotshots 19â€ should feel long and exhausting. The Power Clean load should feel relatively light. If this workout were 4 rounds, it would probably be â€œenjoyable;â€ the 6 rounds make this WOD brutally hard. Scaling Options This Hero WOD is long and high volume. Reduce the volume, the load, and or skill level (see: Pull-Up Scaling) to keep this workout under 40 minutes. Intermediate Option 6 Rounds for Time of: 30 Air Squats 19 Power Cleans (115/75 lb) 3 Strict Pull-Ups 400 meter Run Beginner Option 4 Rounds for Time of: 15 Air Squats 10 Power Cleans (75/55 lb)Â  5 Ring Rows 400 meter Run

Another example would be this url
https://wodwell.com/wod/robert-nagel/?ref=listmodal 
workout name is robert-nagel
555 FITNESS HERO WOD
and has the following info that I would like to persist 
â€‹
3 Rounds for Time
100 foot Overhead Walking Lunges (45/25 lb)
20 Wall Ball Shots (20/14 lb)
20 Box Jumps (24/20 in)
10 Push Presses (135/95 lb)

â€‹
I am doing this for an online course as my final project, want to scrape all workouts and persist them in a DB. Site doesnt offer APIs,
each workout has a name, so the url is always /wod/workout name/
just not sure how I can get all the names so I can use requests and beautiful soup to scrape them after using a base url of https://wodwell.com/wod and append the workout name 
I am very new to this so would appreciate some input and thanks for your help",2,2024-01-21 06:29:50
Advice on best method to scrape codebase documentation (ultimately into a format suitable for RAG for LLMs),"As title says, I'm looking into what methods I should research or test, to pull some codebase documentation. For example, the documentation on, say, how to use typescript for JS (just an example, as I know there are already LLMs that may know that).
I guess this would involve getting a scraping tool to crawl everything on a subdomain or subfolder, for example codinglanguageexample.com/docs/ or docs.codingexample.net  as fake EGs.
I would like to do this free or lowest possible cost. But the more I research scraping, the less possible anything free seems to be.
Anyway, formatting:
Been learning steadily - but most examples are ""how to scrape Amazon! How to scrape hotels!"" and that's all data to CSV.
Whereas coding docs will be paragraphs interspersed with code examples, not lists of tabular data.
Best method to clean and format may be dependent on the parent method of obtaining the data in the first place? EG if I could somehow pull it all via some basic python and puppeteer we'd clean with BS4, right? But if we obtained it with another method it may be cleaner/messier? (apologies for vagueness).
Thanks",1,2024-01-21 01:11:42
Is there a current solution to scrap tweets,"So I decided for my master project that I would like to use twitter data on python to get Sentiments on a certain subject and I am now starting to realise that it is not as easy as it used to be due to Musk. I found this YouTube video here https://youtu.be/MNEw3Mplm7E?si=Q7eGcZawsNvSu9zB. Has anyone tried this?
What I would realistically like to do is scrape twitter data by certain keywords during a certain time period. Is this realistically achievable?
Lastly, if not is there any other python libraries that allow to scrape other social media data that can be used for Sentiment analysis.",3,2024-01-20 15:45:38
Production level scraping,"For production-level scraping, does it happen in real-time or is the data stored in warehouses or maybe in vector DBs?",5,2024-01-20 08:54:04
Cloud Flare bypass,"Hello!I am doing a project that's main focus is to do some web scraping directly trough API calls (so no browser involved / no selenium or puppeteer).
My main problem is that when I make the API call to login to the website I want to scrape, I am only able to get the desired response (Bearer Auth token) when I send the request with my own IP (no proxy) and with the cookie header copied from the request sent trough a chrome browser.
Otherwise, if I try to send that request without the cookies and trough the a proxy, I get Cloud Flare protection screen as a HTML response.Any idea / solution to bypass this would be a lot of help, thanks!
this is the HTML I get as response from cf
â€‹
https://preview.redd.it/t19exrqugldc1.png?width=1812&format=png&auto=webp&s=e5e44163e18301fddfcd486079dc795a12f358b5",1,2024-01-20 13:06:06
Business/work e-mail address?,"I've been trying to sign up for various e-mail scraping services but they all seem to ask for a ""business/work e-mail address"", i.e. Gmail and so on don't work. Is there a way around this? If I create a generic website with a domain name, will using the e-mail from that domain work?",0,2024-01-20 11:20:10
Webscraping the optionschain from Euronext,"Greetings all,
I'm trying to scrape the optionschain from Euronext. For example, for the company Heineken, the weburl is https://live.euronext.com/en/product/index-options/HEI-DAMS
What I used is the following:
import requests
url = 'https://live.euronext.com/en/ajax/getPricesOptionsAjax/stock-options/HEI/DAMS/' 
req = requests.get(url).json()

However, this only gets the options for the upcoming expiry date. How do I generalize this such that I get the options for all expiry dates?",1,2024-01-19 13:02:13
Crawling and scrape a huge amount of domain,"Hi there!
I would like to scrape the web and collect domains to a list with domains that has a specific ending like .io og .net
Iâ€™m new to scraping but I have created a spider with scrapy. At first I just need to create the list with a meta title and description like google. However I could imagine that my ip would be black listed if I just go through the entire web with a spider. I am building it on a linode server. 
Do you have any recommendations like
Timeout time before moving to the next domain or things I should notice, and maybe legal advice before I start to crawl. I will of cause obey the robot.txt",1,2024-01-19 03:33:47
Scraping oldest/first google maps review date for a given list of place IDs?,"Hello :) having issues trying to figure out how I can scrape the date of the oldest/first review for a list of 500+ place IDs I have stored in a CSV. The Places API can only pull the 5 most recent or most relevant reviews, and I really just can't manage to get selenium to work for me in R. any help greatly appreciated!!! thanks!",2,2024-01-18 21:42:33
"[Question] Can't get any useful info off of this website, any idea why?","My code is below. I originally had an issue where the website had cloudfare protection so I used Zenrows to exract but when I run the code it's not giving the names of players on the page. Reason behind this is I want to simulate the Gacha mechanic of the NBA2k game locally for myself to still enjoy all of the player variations without spending the time or money that is common with microtransactions. 2kdb has all of the cards for the last four years so I am trying to find a way to easily scrape for each card the player's name, over, def, height, weight, and photo ideally but the photo is optional. Any suggestions of tutorials or other resources would be great as I have limited experience in Python and none with web scraping (yet).
â€‹
import requests
from bs4 import BeautifulSoup
â€‹
url = ""https://2kdb.net/players/24?freeAgents=false&page=1&pageSize=50""
â€‹
# Send a GET request to the URL
response = response_Zenrow
â€‹
# Parse the HTML content of the response using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')
â€‹
# Find all span tags with the specified class
name_tags = soup.find_all('span', {'class': 'font-bold'})
â€‹
# Extract and print the names
for name_tag in name_tags:
player_name = name_tag.text.strip()
print(player_name)
â€‹
â€‹
â€‹
â€‹
Here is what actually outputs
General
Outside scoring
Inside scoring
Playmaking
Athleticism
Defense
Rebounding
Tendencies: Defense
Tendencies: Inside
Tendencies: Shooting
Tendencies: Freelance
Tendencies: Passing
Finishing
Shooting
Playmaking
Defensive/Rebounding
Extended Filters (Collections, Plays, Stats Sliders)
1
0
0",3,2024-01-18 14:18:53
Scraping data through a map,"Hey, Guys I was wondering if anyone could help me regarding this. I want to scrape the data about these charging locations. How do I approach this?
https://preview.redd.it/3020tflf86dc1.png?width=1303&format=png&auto=webp&s=4ceb28b4e90f43792e7962652750437519499381",1,2024-01-18 09:56:58
[question] scraping notion,"I am trying to scrape the webside notion and for this I am using selenium but apparently notion detects strange entries and now to enter the notion it asks me for temporary codes.
Has anyone worked with notion to extract information?",0,2024-01-18 15:24:00
how to scrape a pdf with multiple columns (like paper newspaper or magazine)?,"I have PDFs that I have to scrap that has 2 columns of text.  I'm able to scrape it via python but it will not distinguish the 2 column but rather copy the full line of text from both columns as a single line of data.  
This is should be a common use case just based on the sheer amount of articles, hard copy news papers, magazines that display data in multiple columns.  I'm sure someone has parsed it and able to maintain proper column / data structure.  
Any ideas?",0,2024-01-18 14:23:17
Amazon Reviews API,"Hi everyone
I am in the process of developing an amazon parser on Scrapy.
When creating an algorithm to parse API reviews, I tried to find information on the 'filterByAge' query attribute, but found nothing.
It's definitely filtering reviews by age (either from the time of publication or some other age...)
Does anyone know what this attribute really means?
What data does it accept, and in what form?",3,2024-01-18 08:24:18
Scrapping Amazon - Forced timeout?,"Hello guys,
I am developing a amazon scrapping script using playwright. I am able to scrapy some information, but every page I go ""page.goto"", the response get slower and slower until I get timeout on playwright.  
Does anyone knows if this behavior is some kind of Amazon bot protection?",0,2024-01-18 02:36:53
RSS Feed and Scrapes The Content,Before I dive into buidling an article scaper through Make.com RapidAPI and  I want to see if there is something like inoreader that follows news rss but will also scrape me the whole article so that I can have access to it without having to scrape the article myself?,1,2024-01-17 19:53:22
[Typescript] Question mark characters for content of site using Axios,"Hello there. I'm trying to scrape a certain page which has latin characters as content for some text. Usually I'd not care much about them but in this case I have to collect them.
The trouble begins when I'm getting the page data, in which all latin characters are turned into those question mark diamonds and I keep having them into the rest of my logic.
The code is quite simple, I use a GET request and then get it's content, then further I'm using it with JSDOM to search through it. Until then, the content is having the broken characters. Here's the simple code as an example:
return new Promise(async (resolve, reject) => {
        try {
            const url = 'https://cdep.ro/pls/parlam/structura2015.mp?idm=284&cam=2&leg=2020';
            axios.get(url, {headers: {'Content-Encoding': 'utf-8', 'accept-encoding': null}}).then(response => resolve(response.data));
        } catch (error) {
            throw new Error(`Failed request with url: ${url} having error ${error}`);
        }
    });

I tried the same thing with Puppeteer, and fortunately I don't have the same issue, but I get double the time to complete my scraping process. Any help is much appreciated.  
Update: Managed to solve this by getting the response as arraybuffer, then decoding it locally using iconv-lite from ISO-8859-2, the encoding the content was initially from.",0,2024-01-17 19:21:57
Python Selenium scraping help,"Hello guys I have a question. I want to scrape data from this website: https://www.aldcarmarket.com/en-it/sales/14014/. To be precise I want to scrape data from car cards. I managed to extract headers timers and specifications from each card but I have problem. Every card has a details button. When that button is clicked additional information appears. I want to scrape that information as well but I don't know how. I don't know how to target that element any ideas? I am very lost.
Edit:Added code
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import pandas as pd 
from bs4 import BeautifulSoup as bs

#Setup
driver = webdriver.Chrome()
userName = ""username""
password = ""pasword""
site_url = ""https://www.aldcarmarket.com/en-it/""
driver.get(site_url)
driver.maximize_window()

#Agree button plus waiting for login
buttonWait = WebDriverWait(driver, 
30).until(EC.element_to_be_clickable((By.ID, ""didomi-notice-agree-button"")))
buttonAccept = driver.find_element(By.ID,""didomi-notice-agree-button"").click()
loginWait = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 
""dropDownLogin"")))
buttonLogin = driver.find_element(By.ID, ""dropDownLogin"").click()


#Login
driver.find_element(By.ID,""userName"").send_keys(userName)
driver.find_element(By.ID,""password"").send_keys(password)
driver.find_element(By.ID,""login-submit-button"").click()


#Rating bypass
crossWait = WebDriverWait(driver, 
30).until(EC.element_to_be_clickable((By.CLASS_NAME, ""cross"")))
driver.find_element(By.CLASS_NAME, ""cross"").click()


#Navigate to sales page
salesWait = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, 
""btnShowSale_13956"")))
driver.find_element(By.ID, ""btnShowSale_13956"").click()



#Scroll down 140355
WebDriverWait(driver,30)




x = 0
while True:
   x += 1
   driver.execute_script(""scrollBy(0,1000)"")
   time.sleep(0.5)
   if x>7:
       break






cardsData = []
#cards = driver.find_elements(By.CLASS_NAME, ""card-body"")
titles = driver.find_elements(By.CLASS_NAME, ""vehicle-title"") 
timer = driver.find_elements(By.CLASS_NAME, ""sale-timer"")
specifications = driver.find_elements(By.CLASS_NAME, ""vehicle-specifications"")      

#""vehicle-specifications""
details = driver.find_elements(By.CLASS_NAME, ""details-title"")
specificationsResult=specifications[::2]
for i in range(25):
    cardData = {
        ""Title"": titles[i].text,
        ""Time"": timer[i].text,
        ""Specifications"": specificationsResult[i].text,
    }
    cardsData.append(cardData)
#print(cardsData)
df = pd.DataFrame(cardsData)
   html_table = df.to_html(index=False)
   print(df)
with open('table.html', 'w') as file:
         file.write(html_table)
#while True:
 #x += 1
#driver.execute_script(""scrollBy(0,1000)"")
#time.sleep(0.5)
#if x>70:
    #break",1,2024-01-17 18:57:48
How do I export a list of websites ending in a certain TLD?,"Hi, I'm hoping to export a list of websites, preferably by country to an excel sheet.
I've tested builtwith and semrush though they don't seem to be able to find it or be able to use wild card selectors to only list all websites ending in business.site etc.
Any ideas would be appreciated. 
Thank you!",2,2024-01-17 08:26:06
Octoparse scraping - duplicate help,"Hey guys, I'm working on a project to pull job postings using Octoparse. For sites like Y-combinator, there's only so many entries per day, but I'm seeing duplicate entries every time a new round runs and the data gets uploaded to a google sheet. Each run tells me, say, 50 runs with 45 duplicates, but it still appears to be uploading the duplicates. What setting do I have to tweak to make sure the duplicates aren't getting uploaded? This UI is less intuitive than I would like.",3,2024-01-17 05:00:36
Need help scraping a PDF embedded through CloudPDF,"You can visit their website to view an example of the exact look an embedded PDF has (i.e. the one I'm trying to rip).
https://cloudpdf.io/
The thing is I have near to zero knowledge about web developing so I'm really struggling to get any results. I hope it isn't really hard for you guys. Any help is welcome.",1,2024-01-17 04:14:49
Do you suggest any Proxy List Premium to bypass cloudflare?,"I need about 60-80K requests per day altough, and the cloudflare blocked the mullvad VPN network the last days",2,2024-01-16 23:48:11
Instagram Reels Scrapping,"Hii. I am currently implementing a python program for Scrapping instagram reels data.
I make the respective API calls for that and I am able to get the data perfectly. 
Now the issue is Instagram blocks me after sometime or I reach rate limit.
Is there any workaround for it? I heard proxy is one way of doing it, then how can I implement it? Is there any reliable way for that? 
If there is any other way it would be very helpful
Thanks âœ¨",6,2024-01-16 13:59:35
Any way to bypass cloudflare?,"Trying to scrap images from platesmania.com ,
Tried with undetected_chromedriver didn't work.",10,2024-01-16 06:23:05
Help with scraping LinkedIn profile,"Hi guys, I am trying to scrap job title, company, year for a LinkedIn web scraping project. However**, I am unable to scrap multiple job experience for a single company.** I could only capture the latest experience for a company. However, it seems that the code for different job title in the same company has the same class and div. I just couldnt figure it out why I cant scrap all the experience for a company.
My second question would be how can I write the code to automatically press the ""show all 14 experiences"" button if I want to expand the whole 'experience' category in LinkedIn?
The code is attached as a comment.
For instance in the screenshot I attached below, in LinkedIn I can only capture the title ""Head of Sponsorship Department,ALSA Malaysia Expo 2022"" but not ""Associate of Academic Department...""
https://preview.redd.it/qshmiabh9scc1.png?width=677&format=png&auto=webp&s=f26b2e3ffb120345aa1228d1a2269cb9cc3259d2
https://preview.redd.it/2lk2mk9g9scc1.png?width=737&format=png&auto=webp&s=27f2025dc49a77c537f546f4c22348cf24ff86b4
https://preview.redd.it/1jd7n66lzpcc1.png?width=1105&format=png&auto=webp&s=9a32b1ebf61eb104966842cb1c6f79e8d5ce114d",4,2024-01-16 03:21:56
Scraping data from video game stat page.,"I really like looking into Warzone Weapon Pick rates from websites like
https://wzstats.gg/
and
https://www.wzranked.com/mw3/meta
How can I get the weapons pick rate during a certain period and study that information, track the pick rate of weapons by making my own graphs, could also try to compare the data from both sites also and see how they different or similar it is.",1,2024-01-15 16:28:23
Selenium '.text' different than all other methods to get source on dynamic page,"Chrome 120; Selinum, Chromedriver 120; Python
Just want to automate my flight price searches on kayak. It was working for abut 3 months.  I changed up the search for a new flight and now having issues.   Changed to headful for now (was headless).  I manually wait 25 seconds for dynamic content to load (webdrivewait never seems to work right for me, but that tries to wait an additional 30).  I just want the source so I can parse with beautifulsoup, but I can't seem the CURRENT (dynamically loaded) page source.  The following returns price info:

driver.find_element(By.XPATH '//body').text

But I need structured data, not just the text. The following DO NOT work (all return 0 prices; sometimes I see some reformatted, obscure price info nested in script (non-html) code--page_source does this I believe)

driver.page_source (was using this method before)
driver.page_source after waiting for a deeply nested element, i.e. one of the result containers ('//[div[@data-resultid]')
driver.find_element(By.XPATH, '//body').get_attribute('outerHTML')
driver.find_element(By.XPATH, '//body').get_attribute('innerHTML')
driver.execute_script(""return document.body.innerHTML"")
there are no relevant iframes that I can see. I've switched to both iframe 1 and 2 and I see no relevant content, though I could be something wrong here.

The selenium API is pretty bad.  Is there really no equivalent to `.text` that will just give me all the HTML? That's all I want is just the source of the current page and beautifulsoup can take care of the rest.  I also really feel like the webdriver/webelement object contains this info.  At this very moment, I'm staring at 52 prices in the `.text` results, yet even when I search for the html attribute from the driver I don't see them. And as I mentioned, this is running headful, so when I inspect the source in the browser of course I do see what I'm looking for (though attributes and names change in some ways, anti-scraping measures?)  I can provide code if needed, though my bet it's my ignorance about something that is obvious to those in the know.
â€‹",4,2024-01-15 05:50:44
Are There Exchanges/Markets To Purchase Scraped Data?,"So I have a client who is looking for numerous databases, some of them were easy to just scrape the usual suspects, but he has a few others that are more difficult to find through typical B2B sources, especially since he's getting so granular. 
I'd seen in the past, through the dashboard of one of the more well-known scraping platforms a mini marketplace that had some scraped databases you could purchase, but for the life of me, I cannot recall which, and so far coming up empty handed. 
So this got me thinking, if I've seen that market, are there other markets I might be able to browse and see if someone else has already scraped the data he needs? 
Thank you in advance.",5,2024-01-14 15:53:07
Discord Channels,"Im looking to get messages from a couple discord channels. Maybe 10-12 times daily on each channel.  Since I canâ€™t add a bot to the server and have to use my personal account i wanted to ask if anyone had issues with discord doing this?
Since itâ€™s my personal, dont want to get banned or anything like that. Since im only going to do this max 24 times a day, would I even be bothered by discord? 
Thanks.",1,2024-01-14 19:59:18
how to make money out of web scraping,Hello I'm pretty good at web scraping and i want to make money out of that so please suggest to me some ways to make  good money out of web scraping besides working as freelancer,33,2024-01-13 18:19:55
Scraping gets blocked through fetch but not in browser,"Hi I am trying to scrape a grocerystore for their product data. When I visit the site through the browser it works and I can see the data, however when I try to use fetch or puppeteer it gets blocked. I copied the same headers and everything from the api request in the browser, but it still gets blocked.   
Any clue how they detect that it is sent through a fetch request or puppeteer?  How could I get around this?
This is the endpoint that I am trying to scrape: https://www.ah.nl/zoeken/api/products/product?webshopId=564313",2,2024-01-14 08:33:34
How can i scrape a website without being Blocked (ip address ),"I want a free method ðŸ˜…, if you have some resources just tell me",0,2024-01-14 15:18:30
"High quality Yelp web crawler made in go, open source","Happy new year to everyone. I'm full stack developer especialiazed on building web crawlers, bots, rpa tools.I've been on the industry for about 6 years and now I would like to contribute a high quality project to the community.
Yelp web crawler made in go:https://github.com/johnbalvin/goelp
â€‹
- Open source- Includes full seach support
- The bot will get  information including reviews, rating,  images, description,  title ..etc
â€‹
Previous bots:
- Amazon web crawler https://github.com/johnbalvin/gozon  , I'lll add search support on future realeases
- AirBnB web crawler  https://github.com/johnbalvin/gobnb , I'lll add search support on future realeases
Warning:- Try using proxies, if you don't,  your IP will be banned, my IP got banned and still cant access the page, so use proxies
It uses Golang as the main language and only HTTP requests like an animal, I hate using selenium, puppeteer, playwright .. etclet me know what you thinkthanks",8,2024-01-13 22:36:36
Can I upload IMDB data on Kaggle,"Greetings everyone,
I am new to web scraping and don't know much about data laws. I have just completed my first project, extracting the IMDb top 100 anime list. Can I upload that data on Kaggle and make it public? Also, can I publish that project on my GitHub?
â€‹
Thank you for your response In Advance.
â€‹
â€‹",1,2024-01-14 08:41:35
How to get out of shadowban.,"Hi everyone!
I'm trying to scrape a site. It's completely dynamic - the page source contains only a bunch of scripts and the text ""Turn on scripts, dummy"", so I'm using Selenium. Everything seemed to be okay until I got shadowbanned. There are no error messages, 404s or 403s, nothing like that. The page itself loads just fine, the site just doesn't provide any data for my requests.
It's not a surprise since the first versions of my script weren't utilizing any masquerading techniques. But now I've implemented everything I could think of - a random user agent, request headers matching that user agent, rotating proxies, and a bunch of fingerprint spoofing browser extensions. Nothing works. Just nothing. The site somehow knows that it's me and redirects me to the data input page. 
The same URL that fails on the server where I scrape the site works just fine on my personal machine. I have no idea what to do except drop my server and pick a new one.",9,2024-01-13 14:29:11
Amazon Time Interval,Iâ€™m scraping 100 listings on Amazon with a 30 second delay between pages . My bot can handle the captcha but I was wondering if I could lower the time interval between pages?,1,2024-01-13 23:26:30
LinkedIn Job Description Scraping,"Hi,
I want to use Python to automate feeding LinkedIn job description URLs and tell me whether a job is no longer accepting applications? This status only shows up when I log into LinkedIn so generic scraping doesn't work. 
Any idea how to do it? Thank you!
https://preview.redd.it/fmz6akyf85cc1.png?width=797&format=png&auto=webp&s=60d01881b3d2f4d21e04c07e856143fc4723eb89",5,2024-01-13 05:30:49
DOTABUFF Web Scrape,"I have been trying to transition to a career in Data Science for a while now. For this purpose, i have started creating my portfolio. As my first project, I decided to create a web scraping script that can scrape match data from DOTABUFF and I was able to successfully do so. 
Now my concern is, I went through the robots.txt at DOTABUFF, and they have mentioned this: 'Disallow: /esports/matches', in the document. So does this mean i am not allowed to scrape the above mentioned data, even though it is publicly available? I was planning to create a GitHub repository for this project as part of my portfolio. What do I do now?  
Any insights would be appreciated. Thanks.",1,2024-01-13 08:02:47
How could have the pointerpointer.com images been sourced?,"Looking at the types of images on pointerpointer.com (https://pointerpointer.com/images/347.jpg, https://pointerpointer.com/images/698.jpg) they all seem to be mundane early 2000s disposable camera type photos containing a person pointing at something. Any idea where / how to scrape these types of photos from a myspace/facebook archive or something?",1,2024-01-13 06:24:45
Scraping 404 page.,"As I encounter a problem, which I am not sure that this page exist or not.
Page:
https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/
General Tricks:
""Some servers are smart enough and tries to prevent the page from being scraped.""

User headers: User Agent, Cookies.
URL redirections.

Can we scrape this page?",1,2024-01-13 05:07:56
Visit LinkedIn profile and use with OpenAI?,"I want to code a python script for my own use, that will go through my lead list, go to each LinkedIn profile, get the data there and use OpenAI to generate a compliment for the first line of a cold email.
Any guidance on how to do this?
Maybe I need to use a web scraping API to make it easier (rather than using something like Selenium and proxies)â€¦
Or is there an easier way getting the AI to visit the profiles itself?
Thank you for your help!",1,2024-01-12 20:42:31
How to scrape data that is inside a div that changes its content?,"Hi,
site: https://komora-ucetnich.cz/cze/seznamy/clenove-komory/sort:surname/direction:asc/page:1
I'm trying to scrape this website for names and addresses to create an .xlsx or .csv file to use as address book as it's necessary for my job. More specifically, I'm trying to create a script that makes an excel file with those 2 parameters/colunms. 
I could access names by themselves, but am struggling with addresses.
I'm also new to this and I cannot quite figure out how to scrape addresses. Addresses are visible when you click on a name under ""Prijmeni"" column in the ""adresa"" row.
I would appreciate any tips on how to approach this.
Kind regards",3,2024-01-12 13:46:08
Lowes Item Number-Based Hyperlinks,"I'm not sure which channel this best fits on, but I'm trying to generate hyperlinks to Lowes products based on a unique identifier of the product, such as item number. Their website instead gives the client a long hyperlink that has a lot of noise in it, but it probably supports unique-identifier based links if given the right format.
I've called Lowes, but non-employees aren't allowed to contact their IT department and they have yet to respond after two months.
I've tried the following format, taken from https://images.lowes.com/animate/lowesvendorguide.pdf, but it redirects to their homepage.
http://www.lowes.com/lkn?action=productDetail&productId=\[item\_number\]
You all can probably figure this out, easy. Please advise, thanks!",1,2024-01-12 17:37:37
Request URL not functional at all,"Hi I am scraping a website that I've scraped for a couple of years now. I recently was scraping and I received no data when exporting to a CSV. I looked to see if the HTML had changed at all and noticed that the request URL was different. I updated that and am receiving a bad request status=400 error. Doesn't matter if I use post vs get. The HTML indicates post only works. My first thought is the site has upped its security for scraping. What could the issue be?
This is what pops up when I look up the request URL.
https://preview.redd.it/dnofqc4yo1cc1.png?width=661&format=png&auto=webp&s=daaa67c65e7f7294768484d1d6de8d30b2f96030",0,2024-01-12 17:35:21
Help - Scraping X/Twitter,"Fellow redditors,
I humbly ask for your support in the following issue.
I am currently working on a linguistics project examining political speech on Twitter/X. One major issue I have encountered is creating a corpus of tweets/posts. 
This is why I am looking for a scraper that would allow me to extract content (text with/without media such as pictures, with/without retweets) of a single user for a definable period of time (e.g. March 2022-August 2023). Obviously if there is a tool out there that would allow for further breakdown of the data (extract hashtags, time of posting, device, etc.) that would be even better, but the bare minimum of text and date would be sufficient.
I have next to no experience coding or programming, in other words the tool should be easy to use or some sort of ""manual"" should be avaiable to get myself acquainted with it. 
Feel free to suggest other workarounds if you happen to know of any other (other than paying X/Twitter) 
Thank you for you help!",2,2024-01-12 11:15:36
scrapy gets stuck 0 pages/ min,"Hey
I'm scraping big domain with lots of pagination and the crawler keeps getting stuck for hours with this log info
2024-01-12 02:11:16 [scrapy.extensions.logstats] INFO: Crawled 2016 pages (at 0 pages/min), scraped 4352 items (at 0 items/min)
Anyone faced that before?   
https://preview.redd.it/t1pybbfgbybc1.png?width=1043&format=png&auto=webp&s=8426e827f0f25c246fb838bcaef920c142b6c604
â€‹",2,2024-01-12 06:14:55
Extract database information from wordpress site,"Hi!   
There is a huge page with lenses and camera information https://lens-db.com/ I would like to extract from. I know I could browse it and scrap manually the HTML but something tells me there must be a smarter way to just do queries to its database and extract it in a cleaner format.  
I'm not so familiar with Wordpress or the php/jquery calls they do so looking for some advice on how I should do.",0,2024-01-11 16:02:45
[Help Request] Missing elements on a page,"Hi all,
I'm pretty new to webscraping generally. Also, I don't really have any background in webdev or anything like that (I'm an economist), so if I misname anything or don't refer to things correctly, I apologise. I've been trying to do some data collection for my PhD work and am running into an issue that I can't seem to get my head around. I've consulted Stackoverflow and chatGPT but can't seem to make any headway.
The crux of the issue is: I'm trying to scrape a website of services that people offer, that contains their user profiles and a host of information related to them (location, price per hour of service, user ratings etc.). For reference, I'm doing it in R Selenium as the page is dynamically generated and scraping involves cycling through different pages on the website, and selecting/clicking on bits of the page. My background is in R, because I mainly do statistics/econometrics work - though I can limp along in Python as well. 
The issue I'm running into is: 

The page can load a max. of 1,000 query results loading 20 at a time. Great. I write a script that takes the n number of returned queries and triggers the dynamic scroll the appropriate number of times to load all queries.
I write up a couple of lines that find the elements for userID. I extract them. I get a vector of N userIDs. 
I then write up a couple of lines that find the elements for the price per hour. I extract them. I get a vector of prices. 

However... not all users provide a price per hour. So now, I end up with a vector of 1,000 user names, but only a vector 995 prices, and I have no idea where they become misaligned. When I manually check to see the profiles that don't contain prices, they simply do not contain the HTML class that relates to prices (as if it were empty, I'd just write something to impute an NA).
My next approach was to try and forloop over the profiles themselves. So I run a findElement function for the profiles, assign to an object, and then for every profile in profiles I do a findElement for the prices and check length of HTML class 'price' and if >0 getElementText, if <= 0, then NA. The problem with this, is that for some reason I get ALL the text from the profile, not just the price. This sucks for 2 reasons, a) it is inefficient and runs very slowly (and I want to scrape the entire site, so this wouldn't be feasible) and b) it then requires a lot of data processing at the end to extract specific price information from the wall of text it has extracted. 
Does anyone know of any workarounds for this kind of thing? 
Much obliged!",0,2024-01-11 14:28:21
Scrape Submissions and Comments from specific subreddits.,"I am currently working on a project that involves extracting a large volume of submissions and their associated comments from a specific subreddit. I've attempted to achieve this using PRAW (Python Reddit API Wrapper), but I'm facing challenges in efficiently handling the rate limits and obtaining a vast amount of data.
My goal is to retrieve thousands of submissions and their respective comments for in-depth analysis. I would greatly appreciate any guidance, tips, or examples.",1,2024-01-11 09:39:55
Fully configurable scraper in go,"Hello everyone!
Link on project: https://github.com/PxyUp/fitter
Last half year I was working on scraper solution for personal usage (cheap flight ticket). For that I was built scraper which also can aggregate information from different sources and stick them and slowly improve it.
Current functionality:
1. HTML/json parsing
2. File downloading
3. Create aggregation of different sources in json format
4. Notification in telegram or console
5. Browser simulator
6. CLI for home using for bots/cron jobs and etc
I feel this project can also be helpful to other people so please take a shot!",3,2024-01-11 02:56:38
"Selenium - Can't extract text from item using ""driver.find_element(By.CLASS_NAME ""class name"")","I am trying to extract the text content of a using the CLASS_NAME method, and even though the is the only element with that class, when I assign the text of the span to a variable and print said variable, what gets printed is a return line.
The structure of the page is the following:
â€¢ There is the most outer that contain basically any information I need, which I was able top identify with the following snippet:
```
Only One with this classed
main_div = driver.find_element(By.CLASS_NAME,""//div[@class='am-appointments am-section']"")
```
Inside of previous there a variable number of other that compartmentalize the informations I am seeking on a date basis, so each day has it own day. , which I was able top identify with the following snippet:

child_divs = main_div.find_elements(By.XPATH,""./div[not(contains(@class, 'am-appointments-list-head') or contains(@class, 'am-pagination am-section'))]"")

with the former command I extracted a list of these child , each one of them has again a variable number of and each on of these contains the informations I need. So I ran a for loop like this:
```
for child_div in child_divs:
appointments=child_div.find_elements(By.XPATH,""//div[@class='am-appointments-list']//div[@class='el-collapse']//div[contains(@class, 'el-collapse-item am-appointment am-back-appointment') or contains(@class, 'el-collapse-item am-appointment am-front-appointment')]"")
for appointment in appointments:
    phone=appointment.find_element(By.CLASS_NAME, ""am-appointment-data-phone"")
    phone_text=phone.text
```
I created again a list of all the inside each child_div, and ran again the loop, to find and extract the phone number from a < span> that has the following features:

<span class=""am-appointment-data-phone"">+393314569013</span>

Even though the I am targeting in this final step is the only item with this class, the script doesn't print anything. So I guess somehow doesn't get assigned to the variable 'phone_text'. The < span> I am targeting is part of an animation, you click the button, the module expands and it's visible, or maybe there's some interference with Javascript ? I really don't understand what could be the issue. Thank you for anyone who can help!",1,2024-01-10 21:06:45
"Very small daily Twitter scrape - best way? OK with python, small amount of JS, or no-code.","Hi all
I'm looking to do a daily scrape of say, 10-20 accounts, and collect tweets from the past 24hrs from each one. I think this should not add up to many requests (?).
I feel I could almost do this manually, as the intention is to feed this into an LLM for some analysis each day (and eventually I will have a nice piece of research). But the LLM doesn't necessarily require the scrape to be amazingly formatted as JSON...  I think could just about load the page, do a text copy, paste into .txt, then send to the LLM.
However that's a little boorish and I'd rather do something more elegant!
Obviously API access is an issue these days.And not sure also if I need to be logged in to do this (probably on a new account so I don't brick mine?).
Many one-click or no-code solutions are simply WAY too expensive for my small test case here...
So I am looking for recommendations for a tool or tool stack/combo that could do this, or even run automated each day without me needing to specifically kick it off.
â€‹
I've looked into a few (can't name them here?).
Many other tools and guides are simply way of out date, from before Musk killed the API.
â€‹
Thanks for your advice and recommendations.
â€‹
## Update 1 ##
I think I was specifically looking for a twitter solution, but I might be better served diving deeper into what no-code/low-code scraping solutions exist.
â€‹
## Update 2 ##
I'm surprised there are no AI solutions either! Apparently now with LLM models with vision (am I allowed to say the obvious one?) you can easily scrape sites behind logins because it's analysing a screencap and extracting the structure and text. 
Also, how do the free chrome extensions go? Seems like this could also be small-scale enough for me to use. Not like I'm trying to sneaker-bot and need incredibly large number of calls and robust filtering.",4,2024-01-10 12:35:32
Any idea on this?,"const puppeteer = require('puppeteer');
const {Cluster} = require('puppeteer-cluster');

(async () => {
  const cluster = await Cluster.launch({
    concurrency: Cluster.CONCURRENCY_CONTEXT,
    maxConcurrency: 10,
    timeout: 120000,
    skipDuplicateUrls: true,
    puppeteerOptions: {
      headless: true,
      args: [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage'
      ],
    },
  });

        cluster.on(""taskerror"", (err, data) => {
        console.log(`Error crawling ${data}: ${err.message}`);
  });

    await cluster.task(async({page, data: url }) => {
        page.setDefaultNavigationTimeout(120000);
    await page.goto(url, { waitUntil: 'networkidle2'});


     const acceptBtn = await page.$('#onetrust-accept-btn-handler');
         if (acceptBtn) {
         await acceptBtn.click();
         console.log('Clicked on the accept button');
         } else {
         console.log('Accept button not found');
         }

    console.log(await page.url());

    await page.waitForSelector('.col-partnumber .highlight-link');
        page.setDefaultNavigationTimeout(120000);
    const urls = await page.$$eval('.col-partnumber .highlight-link', links => links.map(link => link.href));

    for (let i = 0; i < urls.length; i++) {
      const fileName = urls[i].split('partno=')[1].replace(/%23/g, '') + '.pdf';
      await page.goto(urls[i], { waitUntil: 'networkidle2'});
      await page.waitForSelector('#PIMBrowserLayout > div.torso > div > div.row > div > div.col-xs-8.page-left > div:nth-child(2)');
const pdf = await page.pdf({ path: fileName, format: 'Letter', background: true });
      console.log(`PDF generated for ${urls[i]}`);
    }

});

         const baseUrl = 'https://www.murata.com/ja-jp/search/productsearch?cate=cgsubCeramicCapacitors&stype=2&realtime=1';
         const numPages =100;

                 for (let i = 1; i <= numPages; i++) {
                         const url = (`${baseUrl}&pageno=${i}`);
                 cluster.queue(url);
                         }


    await cluster.idle();
    await cluster.close();

})();

anyone knows a work around timeouts, here is my current code, we are tasked to download all the pdf files of our product as practice but i am having issue handling timeout errors, i have tried to run it parallel at first but it took time so i switched to asynchronous, but now timeout errors are occurring more often...should i give up speed of run time for more accurate result?
â€‹",1,2024-01-10 15:34:17
In a search of an undetectable headless web browser,"Well, the goal is: scrape the CloudFlare-protected website with a set of proxies. Assuming I have 4gb RAM VM with 1 CPU and the target website has like 2k pages. I will need to run it every week.
The problem is, I still struggle to find the most developed and maintained undetectable headless browser.
Which option do would you recommend? Share your experience!",0,2024-01-10 20:04:24
Scraped data precision - any tools to improve the relevancy/accuracy of product price scraping?,"I am in the process of making a scraper using python/selenium. I scraped product data/prices from price comparison websites, but we found many incongruencies in some of the products.
The problem is - with some product codes (we have to use codes as the query for this website), there are many irrelevant products found, which ruins some data.
Example: https://www.kurpirkt.lv/cena.php?q=68116
We are looking for Beurer Humidifier, but there are many other irrelevant products found.
Are there any common practices for cleaning such data?
We already had an idea to calculate the average price and if the variance is huge, add the brand to the query.
Any other ideas?",1,2024-01-10 11:58:53
Help with understanding why my web scraping attempt is not returning the right page,"So I am trying to create a script (Python) to scrape this page:  Results in Windows - Microsoft Community 
Eventually I want to grab the question title, URL and views
However I am falling at the first hurdle as what is returned is not even that page
My Code:
from bs4 import BeautifulSoup
import requests
import csv
page_to_scrape = requests.get(""https://answers.microsoft.com/en-us/windows/forum?sort=LastReplyDate&dir=Desc&tab=Threads&status=answered&mod=&modAge=&advFil=MeTooVotes&postedAfter=2023-01-09&postedBefore=&threadType=questions&isFilterExpanded=true&page=1"", allow_redirects=False)
soup = BeautifulSoup(page_to_scrape.text, ""html.parser"")
titles = soup.findAll(""a"", attrs={""class"":""thread-title single-line-text""})
print(page_to_scrape.content)
â€‹
What is returned:
b'<html><head><title>Object moved</title></head><body>\r\n<h2>Object moved to <a href=""https://answers.microsoft.com/en-us/site/silentsignin?returnUrl=https%3A%2F%2Fanswers.microsoft.com%2Fen-us%2Fwindows%2Fforum%3Fsort%3DLastReplyDate%26dir%3DDesc%26tab%3DThreads%26status%3Danswered%26mod%3D%26modAge%3D%26advFil%3DMeTooVotes%26postedAfter%3D2023-01-09%26postedBefore%3D%26threadType%3Dquestions%26isFilterExpanded%3Dtrue%26page%3D1"">here</a>.</h2>\r\n</body></html>\r\n'
As though the page is being redirected",22,2024-01-09 13:58:15
Find where headers are being added to Ajax Request,"I am trying to recreate an HTTP request through Python Requests but it looks like there are headers being added to the Ajax Request on the page that is validated server-side. Resending the same request (that worked previously) causes an error to be returned so the headers are important.
Here is the website I am trying to login to.
I was able to find the code that sends the POST request:
var b = ""/entreg/json/AuthenticateAction"";
var a;
var c = {
    username: d.username,
    password: d.password,
    newPassword: d.newPassword,
    retypeNewPassword: d.retypeNewPassword
};
$.ajax({
    url: b,
    cache: false,
    type: ""post"",
    data: $(""#loginForm"").serialize()
})

Even when I run this code by itself, it sends the request and adds the headers (all prefixed with X-jFuguZWB-). It looks like there is some obfuscated code that runs something like $.ajaxSetup({headers: {""X-jFuguZWB-Z"": ""test-value""}}) to attach that header to all requests.
My question is, which part of the page is adding these headers and is there any way to recreate the headers when I make my own POST request via Python?
I did find some obfuscated javascript in the <!-- Google Tag Manager --> section of the page but I am unable to tell what it is doing.",1,2024-01-09 22:42:04
200 in Postman but 403 in Python,"Hey there,I have an issue where I can't get my API request from Python Requests to go through. The same request works fine from Postman and also from the browser. I have already tried copying the exact headers that my browser used. I have also tried switching my VPN on and off.
This is a service that requires authentication, which I have, and which gets accepted when I request from Postman/Firefox.
Any ideas what causes the API to block my request when it's coming from Python (even when the 'user-agent' header says it's coming from Firefox), and how to avoid it?  
EDIT: I was able to get a response with Python by using the 'curl-cffi' library as suggested by The__Strategist below. Thank you for the inputs.",7,2024-01-09 11:22:13
Trying to download images from a clothing site,"Hi, automod said my previous post needed more details, so trying again. I'm trying to download clothing images from this site - https://www.myntra.com/fusion-wear
The attributes of the images i'm trying to download are as follows:

HTML:
<picture draggable=""false"" class=""img-responsive"" style=""width: 100%; height: 100%; display: block;""><source srcset=""https://assets.myntassets.com/f_webp,dpr_1.5,q_60,w_210,c_limit,fl_progressive/assets/images/20695836/2022/11/10/ba1724c2-c606-481c-a0ca-63424b61a8661668078028270WomensRayonPrintedEmbroideredKurtaWithPantAndDupatta1.jpg"" type=""image/webp""><img draggable=""false"" src=""https://assets.myntassets.com/dpr_2,q_60,w_210,c_limit,fl_progressive/assets/images/20695836/2022/11/10/ba1724c2-c606-481c-a0ca-63424b61a8661668078028270WomensRayonPrintedEmbroideredKurtaWithPantAndDupatta1.jpg"" class=""img-responsive"" alt=""SINGNI Women Purple Ethnic Motifs Embroidered Mirror Work Kurta with Trousers &amp; Dupatta"" title=""SINGNI Women Purple Ethnic Motifs Embroidered Mirror Work Kurta with Trousers &amp; Dupatta"" style=""width: 100%; display: block;""></picture>
XPATH: //*[@id=""desktopSearchResults""]/div[2]/section/ul/li[1]/a/div[1]/div/div/div/picture
Attribute: #desktopSearchResults > div.search-searchProductsContainer.row-base > section > ul > li:nth-child(1) > a > div.product-imageSliderContainer > div > div > div > picture

I've tried using Selenium to scrape these but I'm only getting 11 images and then it stops. I have implemented a 60 sec pause to avoid getting blocked, but that doesn't seem to be helping as well.
Advice much appreciated! This is not a commercial project, just want to be able to scrape 100-odd images as part of a project.
Update: Also pasting the code here, in case someone can review as well!
https://codeshare.io/7849m1",2,2024-01-09 09:20:15
Legal advice: scraping IMDB data,"Hello,
I'm working on a movie-related project and need movie data like posters and ratings. I've been scraping IMDB for this purpose, but I'm considering going public and potentially making money from it. My question is, could I get into trouble for using data from IMDB and making money? I've come across mixed opinions on this issue. The most crucial information I found is from IMDB's website, stating, ""Robots and Screen Scraping: You may not use data mining, robots, screen scraping, or similar data gathering and extraction tools on this site, except with our express written consent as noted below."" It's pretty clear that scraping is not allowed. On the flip side, there are plenty of websites out there earning money that are displaying IMDB ratings (like justwatch.com).",1,2024-01-09 11:26:32
Scraping naukri.com,"I'm trying scrap naukri.com from f this page https://www.naukri.com/data-science-jobs?k=data+science , using bs4 and requests the parsed data shows only <path> tag with some numbers. Chat gpt said ""it might be because the site uses java script to generate or update the content, and your scraping tool might not be able to capture the dynamic changes made by JavaScript. "" I m putting the ss of the response here. Please help if you can....
https://preview.redd.it/w57gau5zycbc1.png?width=1920&format=png&auto=webp&s=7d80abc553c7705fe8b980e8f5d4d9d1b2828de9
https://preview.redd.it/8eeblu5zycbc1.png?width=1920&format=png&auto=webp&s=1d284c3b5a7ee08359a528e7f5c4a066fcd373e0
https://preview.redd.it/1p7g6u5zycbc1.png?width=1920&format=png&auto=webp&s=da507819273c43cc4413d8d11bc30d02b33e37ff
https://preview.redd.it/1v8edp90zcbc1.png?width=1920&format=png&auto=webp&s=8e252b3328793bf9ab0111ccd8d8a06c13be5ca0",1,2024-01-09 06:28:11
Stuck with Socks5 on aws lambda script,"Hi guys, I am stuck with a socks proxy on aws lambda, did anybody manage to run a socks4 or 5 proxy on aws lambda, I am using python and tried installing pysocks but it does not seem to work. any help would be appreciated",1,2024-01-09 06:07:41
Are there free hosting options to deploy a web scraper online?,"Are there reliable platforms that support web scraping tasks??
My options so far: PythonAnywhere, Heroku (I can't signup to Heroku for some reason so its out),  AWS .
The problem is that I don't know how to deploy them to these sites and what are the limitations.
Does anyone here have experience with this?",3,2024-01-08 23:03:25
How to scrape the details from the MS Community Forums?,"Hi all, I am wanting grab the post title and view count from the following Microsoft Community forum url:  Results in Windows - Microsoft Community 
Ideally as a csv for a project I am working on.
I would like do this once a month.
Whats the best way to start. I am not a programmer, but can usually pull someone else script and fudge it to my own means.",1,2024-01-08 14:42:14
Scraping from BI iframe,"Hey all,
I am trying to scrape some data from microsoft BI, from an iframe component  (Link). The data is publicly available but a source comfortable to manipulate and analyze is not possible to find.
Any help? I should probably note that I am not experienced at this at all. Tried to use octoparse for this but can't seem to get it to scroll correctly.",1,2024-01-08 12:08:14
List of newly created businesses?,Hey. Is there a website or a way I can scrape newly listed businesses? Does anyone know of anything like that or how to do it?,1,2024-01-08 04:03:46
How to bypass nuData / nuDetect Anti-bot,"NuData, a subsidiary of Mastercard, integrates with 3DS verification and features an anti-bot system called nuDetect, similar to Akamai's reliance on human behavioral analysis.
Understanding nuData's Functionality
nuData operates through a website's domain or server, using a fingerprinting script found in URLs containing /init/js/ or /sync/js/. For instance, Kohls.com uses the following URL for nuData integration: https://fc.kohls.com/2.2/w/w-552128/sync/js/, utilizing a customer ID for request identification.
The configuration of nuData is stored in a pageModeConfig property, and its fingerprinting script initializes a window property at window.ndsapi.
nuData's Initial Analysis Process
nuData examines various properties, such as:

pageX, pageY coordinates of page elements, including the main document.
Scroll positions: document.body.scrollLeft & document.body.scrollRight.
Current time: Date.now(). Likely for timezone calculation.
Numerous Math operations, likely assessing the JavaScript VM.
Common properties like window._phantom, window.callPhantom, window.__phantomas, etc.
Screen details: window.screen.width, window.screen.height, window.screen.colorDepth.
Flash plugin presence.
Navigator attributes, including language and device timezone.
WebGL parameters:


VERSION RENDERER SHADING_LANGUAGE_VERSION DEPTH_BITS MAX_VERTEX_ATTRIBS MAX_VERTEX_TEXTURE_IMAGE_UNITS MAX_VARYING_VECTORS MAX_VERTEX_UNIFORM_VECTORS MAX_COMBINED_TEXTURE_IMAGE_UNITS MAX_TEXTURE_SIZE MAX_CUBE_MAP_TEXTURE_SIZE NUM_COMPRESSED_TEXTURE_FORMATS MAX_RENDERBUFFER_SIZE MAX_VIEWPORT_DIMS ALIASED_LINE_WIDTH_RANGE ALIASED_POINT_SIZE_RANGE

Canvas fingerprinting and font metrics, using a specific script to draw and analyze text on a canvas using the following script:

â€‹
        var b = document.createElement(""canvas"");
        b.width = 200;
        b.height = 40;
        b.style.display = ""inline"";
        var c = b.getContext(""2d"");
        c.fillText(""aBc#$efG~ \ude73\ud63d"", 4, 10);
        c.fillStyle = ""rgba(67, 92, 0, 0.5)"";
        c.font = ""18pt Arial"";
        c.fillText(""aBc#$~efG \ude73\ud63d"", 8, 12);
        a = b.toDataURL()

Behavioral Analysis by nuData
nuData tracks all page events, including keyboard (keyCode) and mouse movements (pageX, pageY), logging the sequence and timing of these events. This data suggests analysis of user interaction speed. The collected data is encoded in a proprietary format, for example: ""vce"":""apvc,0,656p336o,2,1;fg,0,;zz,153,24r,2sn,;zzf,5r8,0..."".
Bypassing nuData
Bypassing nuData security requires a nuanced approach very much like bypass Akamai (https://reddit.com/r/webscraping/comments/186jdhk/how_to_bypass_akamai/), especially considering its reliance on GPU rendering information to determine the operating system and device type. 
Here are some strategies:

GPU Rendering: Emulate consumer-grade GPUs rather than professional hardware, as nuData's algorithms are tuned to recognize and differentiate between them.
Behavioral Analysis: Utilize tools like ghost-cursor (found at https://npmjs.com/package/ghost-cursor) to simulate human-like cursor movements and keystrokes. Timing is crucial here; movements or keystrokes that are too rapid can be flagged as suspicious.  

Learn more:
Anti-bots are a growing issue among web scrapers, especially given how complex bypass can be for beginners. If you are struggling with nuData, come say hi on the Web Scraping & Data Extraction discord server!
https://discord.com/invite/fHbbHTq4CQ",9,2024-01-07 16:03:33
Need help with the TG Desktop Restrictions.,"Hello, I need a way to surpass content d0wnl0ad restriction and forwarding on Telegram Desktop. I know on android files can be easily downloaded, and on TG Web there's some scripts for extensions for browser d0wnl0ading.
However, I prefer to d0wnl0ad with the Desktop version because it's the faster one, and more easier to use to d0wnl0ad. And nope, I don't want to d0wnl0ad nothing Â¡ll3gal or f0rbbid3n. Just a bit of soft-NSFW content,
Hope you have a solution for me, any app of script to work with the Desktop version.
Thanks.",2,2024-01-07 23:11:25
Find URL not in google search results,Im trying to locate several web pages that do not appear in Google search results. I know the main domain and I know keywords that appear on the page. Can this be done with scraping? What would be the best/easiest tool for this?,1,2024-01-08 01:41:56
How can my client run the script?,"Hi guys, Iam having a selenium script that autofills a form in a particular website. Iam making this for a client. How can I set it up in a way that he can run the script whenever he needs? 
Should the script be stored in cloud? Or can it be made into an executable file, so that it runs with just a click in his local machine?
Iam a noobie in web scraping. Hope you guys would help :)",8,2024-01-07 07:19:05
Airbnb web crawler made in go,"Happy new year to everyone. I'm full stack developer especialiazed on building web crawlers.
I've been on the industry for about 6 years and now I would like to contribute a small project to the community.
The project will get Airbnb's information including images, description, price, title ..etc
https://github.com/johnbalvin/gobnb
It uses only HTTP requests like an animal, I hate using selenium, puppeteer, playwright .. etc
let me know what you think
thanks",12,2024-01-06 22:23:18
How to find a job with web scraping skills?,The title says all.,7,2024-01-06 12:07:23
Pulling Redfin URLS not for sale,"Hi all,
I'm somewhat new to scraping but I've found a lot of great resources here and elsewhere for pulling home information off of Redfin in Python. So far I'm able to:

Given a list of urls, pull all of the information I'm looking for
Use a Redfin search url to generate a list of for sale or recently sold houses to use in step #1

However, I want to get a list of URLs for every single home in a certain zip code. For example, if I look up the house from Scream I can easily find it on Redfin. I could take that URL and add it to my list for step #1. However, I can't find an efficient way to get all of these urls for a zip code or neighborhood. Everything I've tried runs through Redfin's own search methods, which only ever produce urls for recently sold and currently for sale homes.
Any assistance would very much appreciated, this is the last step in my project. Thanks!",0,2024-01-06 18:49:22
Scraping google maps,"Has anyone had success in scraping google maps? I am stuck on attempting to scroll down the list of results. Only the first few results are loaded. (Imitating a higher screen resolution results in more results being loaded at start). Upon scrolling, more results are fetched and loaded. I have been stuck for a long time attempting to recreate scrolling down with the mouse wheel on the results panel. I have tried everything across multiple platforms, puppeter, selenium, html-requests, everything.
Link to the example of what I am talking about:
https://www.google.com/maps/search/scrap+yard+Kansas+City&hl=en",23,2024-01-05 22:35:36
How can I scrap Emails from Youtubers when I've reached the daily limit?,"I need to get the emails of a list of youtubers for my work but I reached the daily limit, how can I circumvent this? Thankis in advance.",1,2024-01-05 16:19:50
Proxies + VPN?,"Currently making a scraper and using proxies. However, I don't want to accidentally reveal my IP. Would simply turning a VPN on reduce the risk of accidental IP reveal? Or are proxies sufficient?",1,2024-01-05 15:32:51
Help! Scraping products from anaconda,"I am trying to automatically get product numbers on certain items at each store individually.  
Ordering 1 item from a click and collect store and then changing that number in cart view to ""9999999"" it gives me the error ""Only # left""  
I want to use automatically pull this number into a spreadsheet. I just want to know the simplest way I can pull this number because my previous web scraping projects have been overcomplicated and I am sure there is an easier way to do this.  
Error",2,2024-01-05 09:16:32
Scraping Hungama.com,"Hi, I am trying to scrape hungama.com for practising my skills. I tried using selenium, html-requests but nothing seems to work.
1. I send request to hungama.com
2. I send a request to hungama.com/music 
3. Pick up all a tags from div id languageBar from /music page html.
4. I tried using .click on the a tags using selenium it would not fetch the complete html that I can see on the browser. I tried using sleep, wait.until, implicitly_wait. 
Please guide me as to what I might be missing and point me in the right direction.",1,2024-01-05 07:15:34
Static tables: web scraping,"Hello everyone,
â€‹
I have this paginated table here(link), can anyone tell me tricks to scrape this? I want the data from years 2010-present and each year have 5000+page. Can anyone tell me a strategy to scrape this data?",2,2024-01-04 20:39:15
Help with Python Scraper not working,"Iâ€™m trying to create my own scraper, and following a tutorial online, but I can only get data from one product to appear rather than all the data on the page. In the tutorial, all the data for every product on the page appears for the guy. As you can see, on my terminal, only one shoe â€œWomenâ€™s Altraâ€ appears. The last photo shows what his results in. Please help!",0,2024-01-05 02:17:02
Help Scraping information from an iframe,"Hello, 
I am currently working to scrape information from a website where I need to login and then select a link where an iframe pops up. 
Iâ€™m currently using selenium to login which is working successfully, but I cannot find any elements inside of the iframe to begin scraping the information. 
Any suggestions on how to traverse an iframe? Any insight would be much appreciated. 
Thank you!",1,2024-01-04 15:32:50
Scrapping ecom site built on SAP Commerce,"Hello everyone, most ecom site are built using some framwork (shopyfiy, magento etc) and that make it quite easy to scrape maybe through API of Json or something..
I've been trying to scrape extra.com/en-sa, its a nightmare to scrape  efficiently.
Can you guyz please provide any idea :) 
â€‹
Thank you.",1,2024-01-04 12:39:41
How to scrape leads based on registration date?,"Hello guys, I have a SMMA agency, and usually we find our leads from google maps, yelp or yellowpages or whatever, but recently i had this idea of why not targeting newly registered businesses, i think it would be a great idea specially for niches that requires a solid budget to start the business like car rental firms, or hotels...  what do you think of this idea? and whats the best method to scrape leads of those businesses?
â€‹",1,2024-01-04 12:32:51
How To Replace Expired Cookies,"Scraping noob here.
I am using python to scrape websites with known bot detection by finding the exact endpoint I want data from and creating a request in python for that endpoint. Simple enough and it is working for a majority of my use cases. 
For a few of the websites certain cookies are required. Some of these cookies seem to expire over time for different reasons. I've recently been experimenting with using residential proxies and that does in fact cause the cookies to last longer.  
How can I either
A) make one set of cookies last forever (I feel like this is impossible)
B) replace the expired cookies with new cookies
My question then becomes how can I replace the cookies?
- Should I find the exact requests that create these cookies and reproduce those?
or
- Should I try using selenium to retrieve all the expiring cookies and then use the requests I currently have in place? 
I can put in the work to find the exact cookies that actually need to be replaced.   
PS. I know it's expiring cookies because if I go back to the site and get new cookies for that request , my code works again.",1,2024-01-04 07:15:08
I am building real-time online css selector tool.,"Inspired by https://try.jsoup.org/ and https://anglesharp.github.io/
I started to build real-time css selector so I can instantly see the result of css select.
This is the first version: https://css-selector-xi.vercel.app/
Please provide some feedback so I know what to build and if there's any errors.
If you know any other tool like this let me know.
Thanks",1,2024-01-04 07:06:12
Detect CMS for scraping,"First time scraping this kind of shops.
Anyone have similar experience, any other way instead of locating elements by class names ?
https://www.etniabarcelona.com/ba/en/optical/psator-o-gdpt
â€‹
h1: product-sidebar__title   = Title ... and so on",1,2024-01-04 07:06:01
Scraping Instagram stories?,"The title basically, a couple of years ago I was scraping Instagram stories - it was pretty straight forward back then. 
Now, I just get the feeling the the security system is much more robust, Iâ€™m using pva account (get blocked all the time) and so on. 
Any high level tips and tricks based on latest updates?",0,2024-01-03 21:18:39
How to run browser code on NodeJS?,"There is an API I am trying to reverse engineer, but it has tons of obfuscated code. There are IIFEs, closures, and more importantly, it uses document and window object judiciously. How do I take the script and run it in a NodeJS environment?",1,2024-01-03 20:35:10
Webscraping data sources for airfares,"Hi All,
I have no previous experience with webscraping but I'm trying to learn and have a specific use case in mind I'd like yourinput on. I want to find flight deals from a specific airport to any destination in the world (and ideally multi-city options for closeby cities as well) with departure date and duration date flexible (e.g. departure anytime in next 3 months and trip duration anything between 1 and 3 weeks). I'm having trouble pinning down though which data source I can use. Sites such as Skyscanner offer search options for destination ""anywhere"" and some predetermined durations like 1 or 2 weeks but not for any duration within e.g. 1 and 3 weeks. I'd appreciate your suggestions for airfare data sources where I could be able to find this data (perhaps it is available in Skyscanner/ Google Flights/ Kayak etc) and how to go about extracting these. Would be much appreciated as it would really help me in my first step towards learning webscraping.
Basically what I'd like to make for myself  is something like this: 
https://escape.flights/flights-from-new-york/  --> shows all flight deals departing from LAX
â€‹",1,2024-01-03 14:25:04
web scraping: ecommerce website content for an app. is it feasible?,"Hi! I want to use content from ecommerce websites to put on my app, and I actually have permission from the brands to web scrape. I don't want to get banned after launching the app and stuff. How do I ensure this, and how should I go about this in general?? Additionally, what are your opinions on AI tools for webscraping?
also, idk if this matters but im using flutter",2,2024-01-03 02:24:46
Web Scraping,"How to check is it allowed to scrape a website? even i checked by adding the word robots.txt but shows nothing.
and secondly, is it okay to scrap a immigration website for the immigration agency to provide all the visa details?",0,2024-01-03 05:09:39
Amazon web crawler made in go,"Happy new year to everyone.I've been working as web crawler developer for about 6 years and now I would like to contribute a small project to the community.
The project will get Amazon's product information  including images, description, price, title ..etc
https://github.com/johnbalvin/gozon
It uses only HTTP requests like an animal, I don't like using selenium, puppeteer .. etc
let me know what you think
thanks",31,2024-01-01 23:28:25
How can I extract only the Player stats table from FBRef?,"Hello! I'm kinda new to web scraping and the like, so if I sound like a beginner, it is cuz I am :)
Anyway, I want to extract the Player stats table alone from the below Fbref link with python (I'm using Jupyter notebook btw)
https://fbref.com/en/comps/9/misc/Premier-League-Stats#all_stats_misc
If you open the link there are two tables, one with Squad misc stats and another with Player misc stats below it. I want the latter. I tried using BeautifulSoup and only the Squad misc table was extracted which I don't need.
So if anyone can help me out with a code snippet in python with BeautifullSoup or a different method altogether, it'd be really helpful. Thanks!",1,2024-01-02 15:36:41
ERR_BAD_REQUEST with axios.get(),"Hello everyone,I need to do some data collection on the HLTV team rankings for the Counter-strike game. As of December 25, 2023, the team rankings are :  Counter-Strike Ranking | World Ranking | HLTV.org . I need to have access to the complete HTML of the page so I can lookup the rank of the team ""FaZe"", for example. I am totally new to webscraping, so I tried using a GET request via axios :
const TEAM_RANKING_URL = ""https://www.hltv.org/ranking/teams/2023/december/25""
const axios = require('axios')
axios.get(TEAM_RANKING_URL)Â 
.then(res => console.log(res))Â 
.catch(err => console.warn(err))
Unfortunately, I get the error message  AxiosError {message: 'Request failed with status code 403', name: 'AxiosError', code: 'ERR_BAD_REQUEST', config: {â€¦}, request: ClientRequest, â€¦} . I am running this code in node.js in VScode.
I think the issue is related to how the server accepts GET requests, i.e. they do not accept GET requests not coming from browsers. Can somebody help me connect to this website via node.js and have access to the html? Thank you
P.S. Ultimately, my goal is to be able to do stuff like document.getElementsByClassname() so I can parse through the rankings.
â€‹
â€‹",1,2024-01-02 15:16:20
Scraping for reddit posts and comments in a specific subreddit / subreddits with a specific keyword search ?,"Hi is there a way to to scrape for posts and comments , including meta data by keyword in specific subreddit s ? 
For example I want to scrap all the posts and comments for the keyword ""acoustic"" from audioengineering and python sunreddits ? I fhink praw library does not provide a way to do that right ?",3,2024-01-01 20:51:18
how to extract data with css in class names,"I am trying to extract data from this website:
â€‹
https://catalog.registrar.ucla.edu/major/2022/mathematicsofcomputationbs?siteYear=2022
â€‹
response.xpath('//div[@id=""Overview""]/div[@class=""css-3chzt3-Box--Box-Box-Card--CardHeader e12hqxty2""]/h3/text()').extract()
â€‹
I'm starting off by trying to get the ""Overview"" text using both CSS and xpath but nothing seems to be working and the class names and id seem to be long and random, which I assume is because its JavaScript generated. I was wondering how I would approach extracting data with those type of class names using scrapy-splash and scrapy. It would be very helpful if you can link some video or articles too.
â€‹
I checked the generated html on my dockers and realized that it is giving me the data in a json script. Is that supposed to happen?",2,2024-01-01 22:28:05
Looking for a scraper - maps data,"I'll start by saying I'm somewhat technologically advanced, but stuff like this is way over my head. I'd like to find a tool that's executable, or cloud-based because that's about what I'm able to understand and use. 
I know there's numerous options out there, but would love some direction on what the best fit for me is. 
I need to pull in Google Maps data from around 500~ restaurants. I have a list compiled of the ones I need data from. 
The data I need is their website address - and their Summery / Details in whatever format they have listed. (for instance, if it's an Italian restaurant and they have a brief 2-3 sentence description for their establishment, I need this) 
Also, I notice many restaurants don't have a summary or details listed on their Google business profile. Would there then be a way to pull this info from their site in some format?
I appreciate the help!",3,2024-01-01 18:09:08
Chegg scraper ideas?,[ Removed by Reddit in response to a copyright notice. ],1,2024-01-01 22:26:22
Making sure the scraped data looks the same when uploading..whats the best and easiest way to do this?,"https://us.store.bambulab.com/products/x1-carbon-combo i want to scrape this webpage, save it as excel file and then upload to wordpress, but i want the content to look the same i.e headings, formatting, pictures etc.. whats the best and easiest way to achieve this using python?",4,2024-01-01 11:54:28
Monthly Self-Promotion Thread - January 2024,"Hello and howdy, digital miners of /r/webscraping!
The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!

Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?
Maybe you've got a ground-breaking product in need of some intrepid testers?
Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?
Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?

Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!
Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",3,2024-01-01 11:02:53
"Scraping a price using AI, Vision, without tracking classnames and id's?","A year ago I built a book price webapp that checks a few websites, I was using puppeteer to find the correct div and extract the data.
Now with AI tools around, is there an easier way to do this? Is it possible, for example to get a screenshot of a webpage and use AI to get the correct price? Any other methods now that are more fail-safe then using classNames? Since there can change.",2,2024-01-01 09:20:55
https://pixelscan.net/ tips?,"Hello guys, recently I've become aware of https://pixelscan.net/, an interesting site to analyse my scraper if it's fingerprint is identifiable as bot.
As expected, its informing that my fingerprint is inconsistent, but does not offer any tip of ""how"" it detected. 
Any tips ?",5,2024-01-01 01:47:47
"Bulk scrape big list of URL's, the visible inner text (article content) to doc or txt files","I need something where can input CSV file of URL's which will then be scraped in bulk and then an output file to save the content to word document or text file if cant do word, ideally appending the data to the document as i want to scrape multiple URL's into the same file in many cases (like similar articles), purpose is bulk data collection for AI analysis for research.
Any ideas on apps to do this or get close, it feels simple enough to exist already?",2,2023-12-31 20:56:58
Would this project be feasible/useful?,"I want make a YT scraper that searches for a word, and then finds videos from the last week, comparing their views to channel avg views in last 10, and then attribute the positive or negitive difference to each word in the title, and finally end with a list of words sorted by how much they increase views compared to average.
Just wondering how feasible and or useful this could be?",1,2023-12-31 09:19:54
Scrapping Player stats from sofascore,"I' trying to extract players stats from sofascore however everytime i switch to inspect page the complete page change from this (pic 1.) to this (pic 2.) and the table dissapear.  
https://www.sofascore.com/tournament/football/colombia/primera-a-clausura/11536#id:52847,tab:top_players
https://preview.redd.it/9puoxne1fj9c1.png?width=1522&format=png&auto=webp&s=b83a1faeb689dfe4623664377213deaa00803a49
https://preview.redd.it/hc12knlefj9c1.png?width=1871&format=png&auto=webp&s=7a8ab2bbc6210e92a76ede9380effcb6890ce74f",3,2023-12-31 02:01:25
Need help.,"In output the same name is repeating, I want name and expertise of all the doctors on the website. Also, I need data of 500 doctors but it is giving only 10. Any solutions?",2,2023-12-30 16:09:11
How to scrape YouTube data like SocialBlade?,I want to build something similar to SocialBlade. They're tracking millions of YouTube channels. Does anybody know how I can scrape YouTube frequently at scale like that?,1,2023-12-30 19:02:37
Webscraping Shopee Marketplace,"Iâ€™m trying to scrap product data from shopee.com.br, but the pages donâ€™t load correctly, loads empty, with just the cookies CTA and background, without any data. It looks like it has advanced protection against scraping. Whatâ€™s the best approach in this scenario?",1,2023-12-30 17:13:17
Need help.,"from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
url = 'https://intake.steerhealth.io/doctor-search/aa1f8845b2eb62a957004eb491bb8ba70a'
driver = webdriver.Chrome()
driver.get(url)
Use WebDriverWait to wait for elements to be present
wait = WebDriverWait(driver, 20)
Wait for at least one element with the specified class name to be present
doctors = wait.until(EC.presenceof_all_elements_located((By.CLASS_NAME, 'NewProviderCard_Wrapper-sc-12vowct-0.iysCTA')))
doctor_list = []
for doctor in doctors:
    # Use findelement_by_xpath on the doctor element
    name_element = wait.until(EC.presence_of_element_located((By.XPATH, './/*[@id=""next""]/div/div/div/div[2]/div[3]/div[1]/div[1]/div[1]/div/div/b')))
    expertise_element = wait.until(EC.presence_of_element_located((By.XPATH, './/*[@id=""_next""]/div/div/div/div[2]/div[3]/div[1]/div[1]/div[2]/div[1]/div[1]/b')))
# Get the text content of the elements
name = name_element.text
expertise = expertise_element.text

doc_item = {
    'name': name,
    'expertise': expertise
}

doctor_list.append(doc_item)

df = pd.DataFrame(doctor_list)
print(df)
Help: I am a beginner in web scrapping. I need names of all doctors but it's repeating the first name only. Also, I need data of all 500 doctors on the website, but getting only 10. Any idea how to do this?",0,2023-12-30 15:51:17
Best way to booking webscrape without coding,"Hi, what is the best way to webscraping booking.com hotels using (booking filter) without coding?",2,2023-12-30 09:36:38
new here and trying to learn web scraping on a government website,"hi there,
I new here and trying to learn web scraping for the first time ,so i tried doing a project about finance and managing budget by using a open source government website that show all the cost for old projects.
link = https://tenders.etimad.sa/Tender/AllTendersForVisitor
â€‹
What do you guys suggest the best approach for getting all name of project and the cost ?
i tried using BeautifulSoup and i got a lot of error for some reason (mainly ssl.SSLCertVerificationError)
which libraries is best to learn for this kinda situation?
â€‹
thanks",1,2023-12-30 13:03:33
My scraper stops working from time to time. Would an E-Commerce scraper API be a solution?,"Hello, a guy from Fiverr build a webscraper for me. I am scraping Idealo and I think they are changing their website and bot protection often and that causes the problems. I am thinking to switch to an E-Commerce scraper API. It would be a bit more expensive, but I hope that would fix the issues and then the bot would run more stable. Can somebody here recommend a good E-Commerce scraper API service?",2,2023-12-30 08:02:43
Help Needed: Managing Multiple Users in a Selenium Flask App Without Data Conflict,"â€‹
Hi everyone,
I'm currently working on a Flask app that uses Selenium for web scraping. I've encountered a challenge where multiple users might be using the scraper at the same time. I'm seeking advice on how to ensure that there are no conflicts and users don't end up receiving data meant for others.
Here are some specific points I'm unsure about:

Isolation of User Sessions: How can I make sure that each user's scraping session is isolated and independent?
Data Integrity: What are the best practices to maintain data integrity so that each user gets only their data?
Scalability: As the number of users increases, what should I consider to keep the app scalable and efficient?
Error Handling: How can I effectively handle errors that might occur when multiple users are scraping simultaneously?
Resource Management: Are there any specific strategies for managing resources (like memory and processing power) when the app is under heavy load?

Any insights, code snippets, or resource links would be greatly appreciated. I'm relatively new to this, so detailed explanations or references to similar projects would be super helpful.
Here is my current flask app code
from flask import Flask, render_template, jsonify, request
import threading, string, random
from gmbscrape import Scrape

app = Flask(__name__)
data = []
def generate_secret_key(length=24):
    characters = string.ascii_letters + string.digits + string.punctuation
    return ''.join(random.choice(characters) for i in range(length))

app.secret_key = generate_secret_key()

@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST':
        location = request.form.get('location').split("","")

        industry = request.form.get('industry')
        if (industry and location):
            scraper_thread = threading.Thread(target=run_scraper, args=(location, industry))
            scraper_thread.start()
            return render_template('index.html', submitted=True, curr_status=f""Scraping {industry} in {', '.join(location)} | May Take A Couple Minutes For Data To Appear..."")

        else:
            return render_template('index.html', submitted=False)

    return render_template('index.html', submitted=False)

@app.route('/get-data')
def get_data():
    return jsonify(data)

def run_scraper(location, industry):
    scraper = Scrape(location, industry, data)
    scraper.start_scrape()


if __name__ == '__main__':
    app.run(debug=False)

Thank you in advance for your help!
â€‹",1,2023-12-30 06:29:19
Reselling energy prices through my own API,"I'm want to build a web scraper for energy prices in my country and resell these prices through my own API. The company responsible for calculating these prices has a monopoly (let's call them Ecorp) and are explicitly stating that ""automated extraction is not permitted"" on their webpage.
Some other facts about the situation with Ecorp:

They are publicly publishing the prices (without any login or authentication) once a day on their own website.
The prices are also available on many other websites, but as far as I know all prices are sourced from the Ecorp website, one way or another.
You can also access the energy prices through the Ecorp API that is quite costly to gain access to.

What I'm reading from different sources is that publicly available data that is not protected (with auth mechanisms) is free for all, but I'm very curious in this case since Ecorp are explicitly saying scraping is not permitted and they are reselling the data through their own API today.
Obviously they wouldn't be happy if I started selling this data too, but am I doing something illegal?
â€‹
Edit: I reside in the EU.",10,2023-12-28 22:14:13
Issues with LinkedIn li_at Cookie Expiration During Automation,"Hello fellow web scrapers,
I'm encountering a problem with LinkedIn automation using Playwright and custom scripts. Previously, adding the li_at cookie to the context allowed smooth operations from cloud services like AWS Lambda. Typically, the cookie lasted a few weeks unless I logged in again. However, I'm now facing a challenge where a freshly obtained cookie expires almost immediately after one or two brief uses in the cloud.
Attempts to use a residential proxy (specifically from Smartproxy) failed, as I couldn't access the login page. I suspect the issue might relate to logging in from Germany but running the scrape from a data center IP in a different location. Any advice or suggestions to overcome this cookie expiration problem would be greatly appreciated!",1,2023-12-28 19:57:08
Scraping text and attachments on a Website,"Hello, 
I am researching the management of the covid pandemic. 
I would like to scrape information from Hospital websites like this https://documentale.asl2abruzzo.it/L190/?idSezione=369&id=&sort=&activePage=&search=
However, other than what is written on the page, I would like to get also the attachments present on the site (PDF, EXCEL etc,). 
What approach do you suggest to get all the attachments on a website and 
also automatically read what is written in the documents? 
Do you suggest any approach/website to do that?
â€‹
â€‹",1,2023-12-28 14:25:35
Do price comparison sites infringe copyright law?,They usually scrape data and images from the site at least in the beginning. How do they still survive? These sites are obviously bad for the stores. Trolley.co.uk almost closed down because of this.,1,2023-12-28 06:27:13
Different content python (aiohttp) and safari,"Hi there.
I am currently scraping an online shop and they provide product data in json format, what is great.
Unfortunately they only provide this data if requested with a browser (safari/firefox). If I imitate the browser's requests (same headers) with aiohttp / Python, they return the full page but the json is not included.
Any hints on what I am missing out?
Using playwright, I receive the json data.
cheers!",1,2023-12-27 21:39:48
I need help to download a daily historical index price,"This index doesn't propose any downlad button but I don't know anything about python or webscraping. 
â€‹
https://finance.yahoo.com/quote/%5ESP500-5020/history?period1=1672099200&period2=1691020800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true
â€‹
If anyone can help me with that.",2,2023-12-27 17:06:30
Any advice on how to scrape a 150M tweets?,"Hello community,
I'm new to twitter scraping and I want to scrape a 150M tweets from certain geographic areas for my research. Any advice on what is the best and most efficient way to do it? how much it may cost me? and What would be the size of the collected text from these tweets if stored on a text file?
Thanks in advance",3,2023-12-27 12:33:39
How to reverse-engineer the NEWS API?,I want to use the NEWS API: https://newsapi.org/. Obviously I do not want to pay for it. I also want to get a lot of results: potentially 1000+ results per query. Do you guys know how I could replicate the behavior of this API?,2,2023-12-27 16:05:00
Weather Forecast Data from a Site With Dated Values,"Looking for an ordinary weather forecast site that shows extensive info about meteorological parameters with history of values for our data mining project, any suggestions?
A site like this maybe: WunderGround (A bot detection problem occurred.)
Or if it is possible, how can I pull data from removed, past content of a site effectively?",1,2023-12-27 12:03:11
Need Help Creating Uber Accounts,"Hello, I need to create 29 Uber accounts (regular riders accounts). The accounts I create usually get locked right after signup. I used TextVerified for the phone number verification, and MoreLogin as an anti-detect browser, and static residential proxies from IPRoyal. Does anyone have any idea about to to create the accounts and get them locked?",0,2023-12-27 15:33:32
Google Ads clicker detected as Invalid Clicks,"Hey,  
I'm using an ad_clicker which use undetected-chromedriver under the hood to click on some ads and some random residential proxy (Smart Proxy atm, no mobile proxy)  
I'm looking for a way to increase my valid click counts on Google Ads.  
Any tips appreciated ! :)",0,2023-12-27 10:40:53
Has anyone scrapped data from court websites ?,Real estate data etcâ€¦ looking for some pointers,2,2023-12-27 06:14:38
a way effectively scrape off data without limitation on google maps?,"if you use the google maps gui, or their apis, it seems that they do limit you results
if you search like for local business restaurants, in a selected area, then you will have around 10 or 20 results, which are too few vs the actual restaurants in the area, and so by consequence I do suppose that for some reasons gmaps is limiting your research queries",4,2023-12-26 13:49:41
How to pass antiBot test of aliexpress,.,2,2023-12-26 00:06:00
Do you have any resources of Heads and http protocol java selenium,.,1,2023-12-26 00:07:24
Goodreads webscraping and TOS,"So, I've been working on a side project just for fun where I wanted to measure my reading habits geographically and what not. This involved scraping Goodreads profiles, but the API has been deprecated for no good reason. Hence, I resorted to webscraping and am building a neat little scraper.
However, scraping probably breaks their robots.txt and what not, so I'm unsure how public I can make this scraper (I thought of it being on PyPi?). I'm very unsure hwo to proceed.",1,2023-12-25 22:49:22
Instagram scraping limits for media likers,"Hi I noticed recently instagram has more restrictions to scraping post likers data:  'username' 'full name' 'followed by user' etc. than scraping same data from user following/followers. Anybody know what the limit usually is? I can freely scrape 10-15k per account without hittinh 429 limit (too many requests) from user followers/following, but for likers is very random I get the limit after 2,5k 5,5k and 6,5k in few other sessions. I use 5 sec wait time between requests and load 50 accounts in single request",2,2023-12-25 15:43:19
How to scrape crypto address from a website?,"want to scrape the contact address for any cryptocurrency on the dexscreener.com website.
Example the webpage https://dexscreener.com/solana/ 69gr|w4pcsypznn3xpsozcjft8vs8wa5817vuvnzngth
Displays the information for ANALOS token, how do i get its contact address
7iT1GRYYhEop2nV1dyCwK2MGyLmPHq47WhPGSwiqc
Ug5) by scraping.",1,2023-12-25 15:44:42
How to scrape this text from a website,"I want to scrape the contact address for any cryptocurrency on the dexscreener.com website. Example the webpage https://dexscreener.com/solana/69grlw4pcsypznn3xpsozcjft8vs8wa5817vuvnzngth
Displays the information for ANALOS token, how do i get its contact address (7iT1GRYYhEop2nV1dyCwK2MGyLmPHq47WhPGSwiqcUg5) by scraping.
Kindly help.",0,2023-12-25 15:36:58
Sites to host?,"i wanna host my selenium-flask app i have found some sites but they dont have everything 

ability to host python code that works with selenium and flask
persistent disks
file system-like layout of being able to manually edit files

i found that alot of sites that are like discord bot hosting have all of these except only 1 worked with flask/web apps in general and now they removed their free tier and non worked with selenium
i did find render/flo which have free tiers but flo doesnt have persistent disks and no file system like structure to even check on the ephemeral files and render has persistent disks although not in the free tier + also no file system
the file system thing is just smth i have seen alot on discord/minecraft hosting sites but it would be rlly useful to have it with selenium",1,2023-12-25 09:58:44
Question regarding Nutch web crawl,"Hi all! Recently, I have been trying to use Apache Nutch to crawl websites. Itâ€™s been working great. However, I have ran into 2 problems recently.
1/ whenever I try to dump crawled data, it will result in a list of folders with names such as â€œ9sâ€, â€œ6bâ€, â€œ3aâ€. I canâ€™t even know which part of the site these data from. I want it to have folder structure like: â€œnewsâ€, â€œabout usâ€ instead of â€œ3aâ€etc.. can some one help me with this
2/ I dont know what I did, but recently when I try to dump a segment it keeps saying â€œcorrupt segmentâ€. I tried a lots of thing but it just doesnâ€™t work.
Please help me! Thank you so much!",1,2023-12-25 07:57:50
Best web scraping tool or service for goodreads rss feeds? About 3 billion rss feeds to scrape.,"This is what a typical rss feed looks like 
https://www.goodreads.com/review/list_rss/137464693?shelf=read&order=d&sort=rating&per_page=200&page=1
All though there's 3 billion pages to check, but lot of those will be empty. I suspect that there's maybe 500 million with actual data for each. 
As you can see, each page is only a few kb of data.
if it makes a difference, I had code to download from the feeds, but as of recently, been getting 503'd. I can't seem to download more than 10 pages in a row without getting 503'd, and I've been trying with different instances of the google colab servers. I figure it's better to use some sort of service.
I don't need anything to parse the data, I can do that myself, just need the raw download data.",1,2023-12-25 05:46:15
Need help! (New programmer),"I am trying to webscrape the Value Investors Club website for the ticker symbols within the ideas section (https://valueinvestorsclub.com/ideas). I am using BeauifulSoup and no matter what tags I search for I can't even get the body where all the post/ticker symbols are.  Could anyone point me in the right direction?   
Keep in mind this would be part of my first ""nice"" project so im pretty inexperinced so if im looking over something stupid then thats why lol.",2,2023-12-25 00:39:43
How to scrape using API,"What's your general approach for scraping using a backend API? I understand the idea of inspecting XHR requests, but more often than not, they will have some token in the payload or the header, which renders replay attack useless. In those cases, how do you reverse engineer the obfuscated JS to get the token?",3,2023-12-24 21:39:07
Scraping an API,"Hey Everyone! I am a noob when in comes to scraping so I am hpoing some of the pros out here can help out. I have a successful selenium code that is scraping some information from a website where I have to login and input a few things to get a graph which contains the info I need. Now, even though my code is working, itâ€™s not scalable because it takes too long to get the content when I need to pull thousands a day ( I can only do 200 a day right now). I am trying to pull it by making direct api calls but I am struggling to get past the authentication part, do any of you have any pointers on videos or documents I get read regarding getting past the authentication part of the api? I have my username and everything I just donâ€™t know what is not working. 
Another thing I have contemplated is running multiple python files at once, which since my computer is a tad bit old can only run 2 chrome browsers at once. But with a newer computer with better cpu and RAM, could I theoretically run say 100 selenium files, each opening a chrome browsers and scraping? 
Again I am relatively new so looking to understand all of these better",9,2023-12-23 20:23:21
Teach me how to scrap this website!,"Hey guys, i am looking for someone to teach me how to scrap this javascript driven website and get an  url for each housing item in this website. I don't know why i cannot find the url for every housing page, and the web is only redirected if i click the house item.  
Here is the sample link: https://mamikos.com/cari/yogyakarta-kota-yogyakarta-daerah-istimewa-yogyakarta-indonesia/all/bulanan/0-15000000?keyword=yogyakarta&suggestion_type=search&rent=2&sort=price,-&price=10000-1380000&singgahsini=0&gender=0,1&tag=10,11,14,15,17,648 
And i try to have this kind of url for evry item that listed:
https://mamikos.com/room/kost-kabupaten-sleman-kost-putra-eksklusif-kost-kuning-exlusive-depok-sleman-2?redirection_source=list%20kos%20result
Can anybody want to teach me? I'm willing to pay for your hour to teach me how to do it my self.",1,2023-12-24 03:47:42
Google Maps Data Scraping,"Hello,
â€‹
I am interested in extracting data for hotels, hostels, restaurants, and cafes from specific areas on Google My Maps. Please check out this link: (https://www.google.com/maps/d/u/0/edit?mid=1XjPboKkbC0XkIpXLC-RX0_m1VNfJqTg&usp=sharing) to view the designated areas for data extraction.
â€‹
Do you know of any helpful tools or tutorials that could assist me in this task?
â€‹
Thank you.",1,2023-12-23 07:18:03
"Scrapping Zillow with Rotating Proxy, Denied Every Request","I have a list of home addresses, I make a request to every zillow url using a rotating data center proxy from a paid service. My program would request addresses until i reach a captcha where it says I am denied, it will close the driver, connect to a new proxy and start again. This worked great for a week or so. But now everytime I make a request, it gets denied instantly. I am have lots of trouble getting past this. Any help is appreciated.
def runDriver(driver, url_links, lats = None, longs = None):
global total
for idx, url in enumerate(url_links):
print(url)
initDictionary()
try:driver.get(""https://www.zillow.com/homes/255%20Gilkeson%20Rd,%20Pittsburgh,%20PA%2015228/11398219_zpid/"")
time.sleep(2)
updated_url = driver.current_url
html_content1 = driver.page_source
soup1 = BeautifulSoup(html_content1, ""lxml"")
driver.quit()
For my driver set up it is:
â€‹
def serverDriver():
host = HOST
port = 22225
username = USERNAME
password = PASSWORD
session_id = random.random()
proxy_url = ('://{}-session-{}:{}@{}:{}'.format(username, session_id,password, host, port))
sel_options = {'proxy': {'http': 'http'+proxy_url,'https': 'https'+proxy_url},}
chrome_options = wb2.ChromeOptions()
ua = UserAgent()
user_agent = ua.randomprint(user_agent)
chrome_options.add_argument(f'--user-agent={user_agent}')
chrome_options.add_argument('--ignore-certificate-errors-spki-list')
chrome_options.add_argument('--disable-extensions')chrome_options.add_argument('--window-size=1920,1080')
chrome_options.add_argument('--disable-blink-features=AutomationControlled')
chrome_options.add_argument('--disable-extensions')
chrome_options.add_argument('--disable-popup-blocking')
return wb2.Chrome(seleniumwire_options=sel_options, options = chrome_options)
I have tried using undetected chromium with no luck with my local ip",4,2023-12-22 18:30:04
How to scrape entire website for products and information?,Hi. I am looking for help to scrape an entire online store. This would include hundreds of products and information. Is there a service available to save me the time? I don't need realtime updates for the data just a one off scrape to be stored into a csv file to be used in my online store. Any advice or suggestions would be great thank you. Adrian.,1,2023-12-22 23:27:28
Scraping YouTube Shorts,Is there a way to use the YouTube search API to access YouTube shorts in particular? I can't find any documentation.,2,2023-12-22 19:24:54
How to scrape all of the reviews of a user,"I want to ask for a userâ€™s Google name and scrape all the reviews theyâ€™ve made on Maps. How can I do this? Also, I want to scrape the reviews of important places in a city. How can I do these?",1,2023-12-22 22:47:05
org.openqa.selenium.remote.http.websocket$listener onerror,How to handel this exeption connection rest,1,2023-12-22 22:07:24
Getting data then scrape. Help needed !,"Hey ! 
I want to scrape Name of business owner from a business directory. It's the name of specific companies.
I have the registration number of the companies on a google sheet and I want to search company from this number. 
So in resume I need to get the registration number from the sheet then either :

Make a search on google ""number"" + ""businessdirectory.com"" then click on the first link then scrape the name on the page
Goes to the business directory website type in the search bar the number then click on the first result then scrape the name on the page
I have 0 knowledge on how do to this type of things. Can you guys help me ? :)

Thanks for your time.",0,2023-12-22 20:39:10
Scraping Instagram,"I want to incorporate an instagram scraper into an existing web application running on heroku that will search instagram with a natural language search term, maybe ""indie music"". the scraper will then find a list of posts related to indie music and send them to a client application. 
I tried using instaloader, but I get a lengthy error message on my deployment: ConnectionException(""Login: Checkpoint required. Point your browser to "" ... <long url>
this only happens on the deployment, locally this runs fine. Do you all have any tips for how I can make this work?
I guess a better way to say this is: How can I use instaloader-like python clients that require that I have a browser installed on my computer? Do I just need to make a container that will install chrome for me?",2,2023-12-22 15:48:55
Amazon Captcha,Any advice on getting around the Amazon Captcha when running a bot?,1,2023-12-22 19:12:41
How to automate a website with hcaptcha enterprise?,"I'm having trouble automating a website, which I'm not going to link to, so the post isn't removed.  Here is the link to the captcha provider they suggested for me to test,
https://2captcha.com/api-docs/hcaptcha 
but the documentation does not show how to extract the parameters for hcaptcha enterprise from the website, in the documentation of 2captcha have only this: An object containing additional parameters like: rqdata, sentry, apiEndpoint, endpoint, reportapi, assethost, imghost Usage example: ""enterprisePayload"":{""rqdata"":""test_string""} So the question is how do I find and extract this information from website?
Thank you for any help?",1,2023-12-22 13:50:42
Discord or Telegram community,"Hi all of you !
1st post in here for me :) I'm currently looking for webscraping community to improve myself in this. Do you have any link to some of those communities ? That could be Discord, Telegram, Forums, I don't care as long as the community is active.
Thanks !",1,2023-12-22 09:30:51
How to scrape documentation?,"I have to use an API published by an online company to get data. The Docs only have links to main pages and have NO word search capabilities.  They are also behind a username/password that you must enter to see them. (So Google cannot add these pages to its search engine.)
I really need a way to search these documents by WORD. I tried to download them using HTTrack (open source site copying software) but it would not work. 
I also would like to add my own docs to it as they don't show all of the parameters and return values for the APIs that they publish.
How would you get just the sub-set of pages dealing with their APIs (https://somecompany.com/api) in a format that you could use and search locally?
Would it be easier to just recreate their document pages (copy and paste) into some kind of documentation software?",0,2023-12-21 23:57:41
Selenium grid,"Hi guys, did anyone launched selenium grid server on the oracle cloud and was ablentonwork with it ?",0,2023-12-21 17:12:17
Approx how many puppeteer instances can I run with this server?,Any suggestions help I am trying to scale up my script to be able to run at least 100 threads.,1,2023-12-21 12:54:20
"How to fetch product review images from ecom website(amazon, myntra)","I've been attempting to extract images from product reviews on e-commerce websites such as Amazon and Myntra. However, I've only been successful in scraping the main product image and not the images within the review section. How can I proceed with extracting review images?",1,2023-12-21 07:28:55
"Best method to ""reverse scrape""* on LinkedIn? (I have job title and company, not individual person.)","Hi all, semi-competent and technical but I'm not sure how to ask the correct question to find my answer.  
I have a list of attendees to a tradeshow. I know their self-identified state of residence, I know their company name, and I know their job title as they self-identify it on LinkedIn. Can I use this information to scrape LinkedIn to finding matching names?  
From the reading I've done, the goal is often the opposite; find the company and scrape everyone from that company.   
Thanks y'all.",0,2023-12-21 02:44:14
API endpoint return error 400,"Hi, so im currently scraping every bit of information on the European Commission Expert Groups i can through API endpoints (using Rstudio).
Each expert group have their own endpoints that im looping through. I get the endpoints via the network tab on the expert groups website. It generally works great, however, im having trouble with one endpoint link that give me error code 400.This is the endpoint:  https://ec.europa.eu/transparency/expert-groups-register/core/api/front/meetings/search?page=0&size=10
I suspect its because there is no expert group ID in the endpoint like there is in the rest of the endpoints like this one, where the last 4 digits is the group ID:  https://ec.europa.eu/transparency/expert-groups-register/core/api/front/expertGroups/3280 
Example expert group:  Register of Commission expert groups and other similar entities (europa.eu) 
Under the response tab its says it should contain information, but i cant retrieve due to the error 400.
Does anyone know why i get the error 400, and how/if i can retrieve the information?   
I just started webscraping last week - sorry if im not being clear enough. I still dont have the language for it :)",1,2023-12-21 02:34:15
Scraping while changing urls?,"So recently I've been trying to make a spreadsheet of my ao3 readings.
I've searched online how to do it and I found this: https://www.reddit.com/r/AO3/comments/iv5yn2/comment/g5u130a/?utm_source=share&utm_medium=web2x&context=3 
But the only way is through your history and I currently don't have an account.
I might make an account in the future but right now I need to do my reading history with the urls i know instead.
The thing I was trying to do was to make it so that I'd give the extension I'm using (webscraper.io) the Url of the fanfiction I read and the extension would scrape the data I want (title, fandom, word count, etc) from every URL I give it instead of from a reading history like how they do in the post I linked above.
I don't know how to do this though, I tried to make a sitemap with a fanfiction url and made various selector with text and it worked for that fanfiction. But then there is no way to give it a different url and make it work for another fanfiction.
I explained myself like shit i'm sorry I'm just really not familiar with these things.
Basically I want something where I enter the URL and it gives me back the data I want (since all of the urls are organized the same way and from the same site), so like a sitemap but i can change the url each time.
Again this is a shit explanation sorry but I can go into more detail if someone is willing to help please",1,2023-12-21 01:32:29
Allocine scraping to get a csv,"Hi!
So recently I've been trying to get a csv file of my watchlist from allocine which is viewable from here :
https://www.allocine.fr/membre-Z20201030182341282651035/films/envie-de-voir/?page=2
I want to get the duration and genre of each of my movies.
I tried using this
https://github.com/ibmw/Allocine-project
but it doesn't work for 2 reasons:
- this code works for the general page of allocinÃ© which already give the duration of each film (you don't have to click on the film)
- for a reason I don't know  I get a 'list' object has no attribute 'to_csv' on line 134
â€‹
Can you please help me I really don't know anything about scraping...
Thanks !
â€‹",1,2023-12-20 19:32:39
How to scrape Amazon pages through a flutter mobile app,"First, let me tell you I'm newbie to the scraping world and still trying to learn the basic best practices.
I've setup a very basic scraping backend example with Playwright and have verified I'm able to extract some info from an Amazon product page. 
For some reasons I'd like to see if I can make the scraping algo works on the frontend, inside the mobile app which I am developing with flutter. I am not sure how to use headless browser or a library like Playwright in this case. I only see basic examples which use basic interfaces from http package which would not work since I need the whole page and not the initial html code. Also, seems like I am not detected like a human while doing that and can't get any meaningful html response.
â€‹
For example, I'd like to show the reviews and the rating of an Amazon product inside my app. One way to do that is to have a backend scraping server with proxies and a database, crawl the pages, fill-in the database and return the info to the user. The second way is to directly scrape inside the mobile app and show the info with no backend service, database and proxies needed. I'd like to understand if the second approach can be done at all. I'd like to evaluate both and decide which one to use based on cost and performance.",2,2023-12-20 15:13:43
How extract google my business category list?,"Hello,
How can I extract the entire category list from Google My Business? Currently, I have an external sheet that I found on the internet, but I want to extract the categories directly from Google My Business. Any ideas?",1,2023-12-20 14:06:46
Need suggestions for high powered server for running puppeteer script that harvests Akamai cookies.,Iâ€™ve managed to make a script that can pretty much harvest valid cookies for any Akamai protected site my issues arise when trying to run at scale. Im still pretty new too webscraping with puppeteer so I was wondering if anyone had any suggestions on server specifications for trying to run 100 simultaneous browsers. It seems to me puppeteer uses more CPU than RAM but Iâ€™m looking for a good balance of both for steady running. Any tips are appreciated.,1,2023-12-20 11:59:52
How many requests needed to be banned from a website,I want to websrape a website,0,2023-12-20 11:06:44
Help needed?,"New to python in last few weekd, however starting to get somewhere. I have the following script that is doing some webscraping.  Everything is working fine, however the instock result is repeating twice for each result.  Ie the first item has 2 in stock, then the next has 3, however the scraped result repeats the 2 from the first item.  Then the 4th will be the 3rd, etc etc.
The site code snippit :
            <span class=""Stock InStock HighStock"">
                <i class=""fa fa-check Icon""></i>

                        2

                In Stock
            </span>

And my code:
Code starts here:
for url in urls:
       driver.get(url)
       soup = BeautifulSoup(driver.page_source, 'html.parser')
       groupings = soup.find('h1')
   # Find items and prices (Adjust selectors as needed)
   items = soup.find_all('div', class_='Name')
   partnos = soup.find_all('span', class_='StockCode')

   for item in items:
       title = item.find('a', class_='productTitle')
       prices = soup.find_all('div', class_='Price')
       instock = soup.find_all('span', class_='Stock InStock HighStock')
       prices += soup.find_all(lambda tag: tag.name == 'span' and 'price' in tag.get('itemprop', ''))


   for partno, item, instock, price in zip(partnos, items, instock, prices):
       table.add_row([partno.text.strip(), item.text.strip(), instock.text.strip(), price.text.strip()])

driver.quit()",1,2023-12-20 11:05:24
Advice with amazons waf (invisible) and intelligent threat javascript.,"I used to scrape a website via http requests to an endpoint - I then parse the JSON file that is returned.
Recently the website has upped their security and I'm now getting 403s. (Yes, I use proxys).
â€‹
What Changed?

The webpage now requires javascript to load.
HTTP requests now fail and returns a message ""...enable javascript...""
Endpoint now has a header for a V3 token - 2captcha can assist here.
I also see a cookie header with an aws-waf-token parameter.
I also see the ""...\challenge.js"" link in the DOM - but no way to to get past this.

â€‹
I've been reading up on amazon intelligent threat url and this link pretty much sums up my roadblock.
I need a way to get a legit aws-waf-token to embed in my cookie header.
I can use selenium to do the html parsing however NOT ALL the data from the JSON file is displayed on the webpage - which brings me back to http requests.
Any advice is appreciated. Thanks.",3,2023-12-20 02:53:59
Selenium and cloudflare seem to have an issue,"If these are my configurations for firefox selenium:
options = webdriver.firefox.options.Options()
options.set_preference(""general.useragent.override"",  ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:120.0) Gecko/20100101  Firefox/120.0"")
options.set_preference(""dom.webdriver.enabled"", False)
options.set_preference(""webdriver.firefox.marionette"", False)
service = webdriver.firefox.service.Service(executable_path=""geckodriver"")
browser = webdriver.Firefox(options=options, service=service)
Why  is it that I am getting flagged by cloudflare turnstile and looping  through the same webpage again and again. The captcha is manual filled  out by me. I would appreciate any suggestions or feedback.",1,2023-12-19 23:08:13
5000 records data collection with webscraper,"does anyone know how to do this?? i have an assignment due in 3 days and i need to use webscraper/facepager/apify. the best i did was 500 records 
i just donâ€™t know how to collect so many 
any advice, helpful link to a yt tutorial would help a lot",1,2023-12-19 19:22:29
Need help scraping in Hacker Rank,"I need help scraping the leaderboard of a contest after the contest in HackerRank k(a programmingwebsitee)
I have tried but it is not working
well I think that what I ask is straightforward as it is not like I am half into a project and need help completing it
it is like hacker rank is not letting anyone scrape and wants any suggestions
sorry if I sound rood",1,2023-12-19 17:57:33
Mismatch between length of variable while scraping Airbnb,"I am from a data analyst background and I have been recently trying to learn to webscraping. I recently tried to scrape Airbnb but I have a mismatch in the length of expense and rate/cabins. I have tried adding an error check for empty prices list but that does not work.
```
url=""https://www.airbnb.com/s/Norway/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&flexible_trip_lengths%5B%5D=one_week&monthly_start_date=2024-01-01&monthly_length=3&price_filter_input_type=0&channel=EXPLORE&search_type=filter_change&price_filter_num_nights=5&date_picker_type=calendar&source=structured_search_input_header""
base_url=""https://www.airbnb.com""
cabins=[]
accomodation=[]
price=[]
expense=[]
rate=[]
count=1
while True:
    try:
        resp=requests.get(url)
        soup=bs(resp.text, ""html"")
        # print(f""Page {count}: {url}"")
        next_page_url=soup.find(""a"",{""aria-label"":""Next""})[""href""]
        url=base_url+next_page_url
        count+=1
    places=soup.find_all(""div"",class_=""t1jojoys atm_g3_1kw7nm4 atm_ks_15vqwwr atm_sq_1l2sidv atm_9s_cj1kg8 atm_6w_1e54zos atm_fy_1vgr820 atm_7l_18pqv07 atm_cs_qo5vgd atm_w4_1eetg7c atm_ks_zryt35__1rgatj2 dir dir-ltr"")
    for place in places:
        cabins.append(place.text.strip())

    rooms = soup.find_all(""span"",class_=""dir dir-ltr"")
    for room in rooms:
        accomodation.append(room.text.strip())

    prices=soup.find_all(""span"",class_=""_tyxjp1"")
    if not prices:
        expense.append(0)    
    else:
        for price in prices:
            expense.append(price.text.strip())

    ratings=soup.find_all(""span"",class_=""t1a9j9y7 atm_da_1ko3t4y atm_dm_kb7nvz atm_fg_h9n0ih r4a59j5 atm_h_1h6ojuz atm_9s_1txwivl atm_gy_z1y8gd_n6nuqf dir dir-ltr"")
    for rating in ratings:
        rate.append(rating.text.strip())

except:
    print(resp.raise_for_status())
    break

```",2,2023-12-19 13:30:05
Automated tool for find data collection entry points,"Hello, can anyone recommend a scrapping tool that identifies domains or websites (specific urls) that host data collection forms(like registrations, subscriptions etc.), where users can input their information?
If there's no automated tool, I would like some advice on resources or techniques that can help me identify such websites.",1,2023-12-19 06:46:47
"Concerns building on top,b2b","hey I was just curious if there's people who have built a scraper and let b2b clients to build on top of their solution via an api.
â€‹
What are the risk/concerns you had to assure the corporation ? What do they ask?How did it go?",1,2023-12-19 04:19:35
Help Scraping data from Airtable,"Hello Everyone!
I'm sorry if this seems like a dumb question but I'm very new to web scraping and I wanted to make a project by scraping data from my university research opportunities which are posted using Airtable and log all the different types of ""jobs"" by sorting them with their department, But I have been struggling with scraping this site ""https://airtable.com/embed/appd7poWhHJ1DmWVL/shrCEHNFUcVmekT7U/tbl7NZyoiJWR4g065""
I just want to pull all the data with a specific title but it seems to not work. Any hints or steps I should take would be very appreciated, I really hope to learn in the process!",1,2023-12-19 04:05:17
Empty Results from Table Scrape,"Hey all, thanks for any help in advance, feel like I am losing my mind. I am using Puppeteer to practice scraping this website: Foreclosure Site. Specifically, I am trying to map each of the listings here: 
  let currentWeek = await page.evaluate(() => {

        const listings = document.querySelectorAll("".AUCTION_ITEM"");

        return Array.from(listings).map((listing)=>{
            const startTime =         listing.querySelector(""div[class=AUCTION_STATS]"").innerText;
            const details =        listing.querySelector(""div[class=AUCTION_DETAILS]"").innerText;

            return { startTime, details };
        });
    });

However, this returns an empty array when run. I validated the selectors via the console and validated that the page had loaded before the scrape. During the troubleshooting, the listings array was returning results which leads me to think the issue is within the mapping. Any thoughts or help is really appreciated - been stuck on this for a couple of days.",1,2023-12-19 01:31:16
AI finder,"I'm looking for an AI where you upload a file and be able to ask it questions, where it gives you answers from the file",0,2023-12-18 19:41:27
Youtube Short Mass Upload Bot,"Looking to hire a developer to create a Youtube short mass upload bot. Must be able to automate account creation, posting shorts, commenting and pinning a comment. If no direct experience doing this, must be well-versed in proxies and automation on other jobs. IM to discuss.",1,2023-12-18 19:28:14
"Facebook groups (post, comments, reactions)","Hi, I need to scrape all posts, comments and reactions to the post in a certain group. I found nothing on github. Maybe I am too ""new"" to this. Do you have an idea how to achieve this? I am (relatively) comfortable with requests-, beautifulsoup and selectolax library. But maybe this is not sufficient to achieve this. Many thanks!",1,2023-12-18 19:00:16
Suggestions on scraping SEC fillings websites?,"What is the best way to scrape a text dataset out of SEC fillings? I'm trying to extract the text content of all shareholder proposals provided in a company's annual DEF 14A filling, the Board's recommendations for each shareholder proposal, and the text of its justification for those recommendations.
I have links to the all the filings I need as .htm and .txt links. I've tried using BeautifulSoup to parse the links, but given that the filing structure and terminology varies across companies (""shareholder"" vs ""shareowner"" and other unknown terminologies) and across years even within the same company, I've found this extremely challenging. I even set up a HIT on MTurk paying people to copy and paste but I keep getting 99% shitty bot responses, even with 1000 HITs/99% approval rates as qualifications. I also created a GPT using the to extract the proposal data - while I couldn't write the prompt I needed to get it to return the proposal text, it could return the proposal title at least in a few cases.
So, I thought I'd try turning to this community for other suggestions to collect this data. Any suggestions? This is for academic research btw. Thanks.",1,2023-12-18 14:55:44
How to get/request API key from Street Easy and Zillow ?,"Hi everyone, I will start a project on real estate. And I need to collect data from Street Easy and Zillow. But first step would be to get the API keys from these websites. Can anyone tell me how to get the keys? If the above doesnâ€™t work, what are the other ways to collect data from these websites? Thank you!",1,2023-12-18 00:20:45
Tips for scraping a site that only loads 1 screen full of data with js at a time?,"Hi.   I'm trying to scrape Dutchie for a personal project.  It's a Javascript heavy site.   It's pretty obfuscated, but I believe it only loads a screen of data at a time,  even though to the user it feels like a regular page with a scroll bar.   If I scroll down to the bottom and let all the images load,  it still only saves 3/4 of the data.
I feel like using a real browser in python is more likely to give good results,  so I'm trying selenium.   Am I on the right path?  Any tips?  I've looked at other code that scrapes dutchie in ruby and they have gone nuclear on the anti scraping technology since then.",0,2023-12-18 00:12:59
Web Scraping Betano,Is it possible to do web craping on Betano and predict the results of virtual football?,1,2023-12-17 22:14:23
How would you scrape thousands of sites for key terms?,I have a list of some three thousand sites and want to search them for terms like financing and payment plans - thoughts on the best way to do this at that scale?,2,2023-12-17 02:18:58
How do you find good website deals / coupons,"I am a reseller and know that many of the big resellers have a lot secret methods of sourcing products of course. I have come to the conclusion that many of them are scraping many of the main retail websites for deals as well as carrying out coupon scraping. 
I was wondering how would I be able to do this? I have researched website scraping but not able to understand it properly. I have recently just come across website/coupon scraping, so Iâ€™m not completely sure on how it works but I will continue to do research on it. 
I have watched videos on website scraping but from my understanding it exports the websites data on to a sheet. In my situation, I could just source these products from the website as it would be the same thing. Unless I am missing something here, if you could please help me understand id appreciate it, I donâ€™t mean to disregard webscraping in any way or offend anyone! 
I have also heard through the reselling community that a guy gets a lot of hidden/backend coupon and discount codes as well as colleague discount codes which initially had me on a hunt and I stumbled upon website scraping. Could anyone explain as to how someone may do this? 
Any help/advice on how I can get around to finding daily coupons / website deals would be greatly appreciated! Thanks :)",1,2023-12-17 05:33:29
Using gpt to analyze html content and output contact info,"Hello yall,
So i've been trying to scrape contact info from a list of about 1000 websites, which all slightly vary. For this reason, it's been difficult to use just regex patterns to find email, phone, address, ect. I thought I could connect a gpt api to my script, which can then analyze html data. The problem is, it seems to be running out of tokens to do this. Any advice?",1,2023-12-17 01:08:26
Vin Web Scraper,"I am new to using a web scraper. I have the chrome extention, but I currently work for a used car stone and we go to auctions where we buy our inventory. I am looking for help on how to link an excel sheet to a scraping queory. Exentaully I want it to run though the excel searching up each vin and then producing a sheet that pulls that car's information. It would make things so much more effective at the auction. In stead of searching each car's vin up when at the auction I could have it already prepared. Plus, I am trying to earn bonus cookies at work.",1,2023-12-16 15:57:59
"same code works first couple of times, then does not","this is the code that works fine:
â€‹
from selenium import webdriver
from selenium.webdriver.common.by import By
driver = webdriver.Chrome()
url = 'https://www.idealo.fr/scat/12953/sanitaires-robinetterie.html'
driver.get(url)
prices=driver.find_elements(By.CSS_SELECTOR,""div.offerList-item-priceMin"")

for price in prices:
    print(price.text)

â€‹
however the problem is on VScode, it does not show the results. On spyder it showed the results only first time, when i ran the code again, it does not, it shows and empty result. on jupyter notebook also teh same problem. gave result when i ran the first time, after running again it shows empty result. is this common, perhaps strategy used to fight web scrapers? or problem is from me?
thanks",2,2023-12-16 10:36:10
selenium how to access this site?,"I am trying to access this site, however I noticed not being able to do so.
I think they have a form in the front which must be clicked before the contents are loaded.
How would I do so?
this website http://sayurbox.com/ and this https://segari.id/ is basically implementing the same thing.
currently my code is.
So my current strategy is to use selenium to get the auth code before using python requests library. Because I don't see a way to use requests on its own to scrape the site.   
!pip install chromedriver-autoinstaller
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver') 
from selenium import webdriver
import chromedriver_autoinstaller
# setup chrome options
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless') # ensure GUI is off
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument(""--disable-gpu"")
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('ignore-certificate-errors')
# set path to chromedriver as per your configuration
chromedriver_autoinstaller.install() 
driver = webdriver.Chrome(options=chrome_options)
# Access a URL (replace with your URL)
url = ""https://sayurbox.com/"" # Make sure to include the protocol (https://)
driver.get(url)
# Get the page content or perform other actions as needed
page_content = driver.page_source
print(page_content)
# Close the webdriver
driver.quit()",1,2023-12-16 03:12:46
Scrap emails of youtube channel subscribers.,How to get youtube channel subscribers email id.,0,2023-12-15 09:08:23
Local website business scraping info?,"Hi, I do web and graphic design locally, most of my clients are from me searching local businesses and reaching out to them. I've done manual searching for years and just came across web scraping. 
Am I wrong to think I can use this to generate say lists of all local business with 4 star or more ratings who dont have a website? So I can contact them to offer services? Is there a guide somewhere that I could get started with? I'm pretty comfortable with coding and servers, I build pcs also as a local sidehustle so was trying to use the info this way as well. Just not sure if there's a cost effective place to get started with that has guides on how to use the tools.
My intial research found outscraper? Is that a recommended tool? Sorry if im mentioning something that is not allowed here just really have no clue.",3,2023-12-14 22:40:52
PrizePicks Web Scraping,"Haven't bet in a while, but when I did, I used PrizePicks. I remember when I used to bet they would often make blunders and make props that were way too high or low. Some of time they would refund them, but in some cases, they would pay them out. I want to code an automated detection system. I have two questions:  

Is this legal? Does PrizePicks allow web scraping?
What would be the best way to get realtime data from PrizePicks?",2,2023-12-15 00:07:42
Need Help with Python Script for Downloading Images from Tumblr Blog,"Hi everyone,
I'm working on a Python script to download all images from a specific Tumblr blog. The script successfully downloads some images, but it's not capturing all of them. The blog has about 705 posts, and my script is only downloading images from about 442 of these.
Here's a brief overview of what I've done:

Used the Tumblr API to fetch posts (handling both 'photo' and 'text' posts).
Extracted image URLs from the posts and downloaded them.
Added logic to handle pagination and rate limits of the Tumblr API.

However, I'm facing challenges with:

The script is not downloading all images, possibly missing images from certain post types or embedded in non-standard ways.
Ensuring that it captures all types of images (like those embedded within text posts or styled with CSS).

I'm looking for advice on how to ensure the script downloads all images, including those embedded in different formats or post types. Any insights, suggestions, or code snippets would be greatly appreciated!
Here's a snippet of my script for reference:
import requests
import re
â€‹
# ... [other parts of script] ...
â€‹
def download_images_from_post(post):
# Logic to handle different post types
if 'photos' in post:
for photo in post['photos']:
download_image(photo['original_size']['url'], create_filename(post.get('caption', '')))
elif post['type'] == 'text' and 'body' in post:
for image_url in re.findall(r'<img\[\^>]+src=""([^"">]+)""', post['body']):
download_image(image_url, create_filename(post.get('caption', '')))
â€‹
def download_image(image_url, filename):
# Function to download the image
response = requests.get(image_url, stream=True)
if response.status_code == 200:
with open(filename, 'wb') as out_file:
shutil.copyfileobj(response.raw, out_file)
â€‹
# ... [API request and pagination logic] ...
â€‹
Thank you in advance for your help!",1,2023-12-14 20:56:14
My scraper works locally but doesn't work when using a proxy,"i am setting the proxy in settings.py I keep getting 
<page url=""the url""> in page
I am using scrapy playwright",1,2023-12-14 14:39:04
Bypassing InCapsula (Imperva),"Hi,
Is there anyone that knows a viable way to bypass InCapsula (from Imperva). Used to scrape an endpoint, but it is now â€œprotectedâ€ by Imperva (https://content.toto.nl/content-service/api/v1/q/drilldown-tree?drilldownNodeIds=2&eventState=OPEN_EVENT).
Iâ€™ve tried different solutions in Python included Selenium, Undetected-Chromedriver, SeleniumBase and plain requests. However, Iâ€™m facing this protection in all of these solutions when running it on an AWS EC2 instance or through a proxy.
Thanks in advanced",1,2023-12-14 07:32:02
Webscraping Sofascore,"Hi everyone, 
I tried to scrape MLB results with each innings from SofaScore website. Unfortunately, I canâ€™t succeeded to obtain the result wishes. 
Could someone else help me to do it ? 
Thanks",2,2023-12-13 19:33:59
Scraping problem,"I am trying to scrape stock data from yahoo finance. I have pasted the code below that i am using.
The trouble that i am having is that it is only getting first 99 rows, where as i need the data for last five years. Now on the web page when you scroll down it loads up more data. I don't know how to translate that into code. Any help is appreciated. Thanks.  
# URL of the webpage
url = ""https://finance.yahoo.com/quote/GOOG/history?period1=1544659200&period2=1702425600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true""
headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}
# Send a GET request to the URL
response = requests.get(url, headers = headers)
# Check if the request was successful (status code 200)
if response.status_code == 200:
# Parse the HTML content of the page
soup = BeautifulSoup(response.content, 'html.parser')
# Find the table containing the stock data
table = soup.find('table', {'data-test': 'historical-prices'})
# Extract data from the table and create a list of dictionaries
data = []
for row in table.tbody.find_all('tr'):
cols = row.find_all(['td', 'th'])
cols = [col.text.strip() for col in cols]
data.append(cols)
# Create a DataFrame using pandas
columns = data[0]
df = pd.DataFrame(data[1:], columns=columns)
# Display the DataFrame
print(df)
else:
print(f""Failed to retrieve data. Status code: {response.status_code}"")
â€‹
â€‹",1,2023-12-13 22:07:04
Have a list of names and need emails from alumni database,Hey folks â€” I have a list of names of people who attended my university. I need to go through and grab their emails (always in the same place) by searching their name and clicking their profile in our alumni database. Is there a way I can do this thatâ€™ll work for the bulk of the data. Need to reduce how much manual work Iâ€™m doing here,1,2023-12-13 14:23:15
Very hard elements to select,"I'm using puppeteer to scrape a web page that have a pagination which element is basically like this:
- No classes
- No id's
- No href's (using onclick listener for navigation)
- The pagination don't change the url of the page it's the same whatever page I'm in (it's like a SPA which sends HTTP Post request to the server whenever I change the pagination to create new html subpage and replace it with the current content)
And there is more but those are the most frustrating parts currently. Should I give up on page like this?
Edit: Solved by this great friend u/Lonely_Role_4610
Basically we can relay on HTTP requests for this sort of sites",3,2023-12-12 19:15:00
"""webscraping"" of data from an online portal","Hi there,
I have a project I'm working on where I need to get current data in a CSV that is updated daily by logging into my account on a Government website, selecting my search criteria and clicking on download CSV at which time it downloads the CSV with the data I need.
What are my options for doing something like that in an automated fashion?  Is there some sort of wrapper for a web browser or a library in Python that I can get to do something like this automatically by capturing my commands somehow and then automatically submitting them to the website with the requests library?
Any thoughts on where to start would be most appreciated.",2,2023-12-12 16:03:28
[Hiring] Web Scraping/Product Manager Internship with Medical Supply Company,"We are a small medical supply company with big dreams.  Although we've been around for some time, we're still considered a startup as we try and rebuild the business after a catastrophic failure of our systems.
We are currently looking for an intern to come in and take the reins in terms of our product information scraping from vendors sites.  Ideal candidates would have experience in python, selenium, and beautiful soup, in addition to any of the various tools necessary to complete the mission.
We are flexible on work hours, so you can create your own schedule, and you will receive a certificate upon completion of this internship.
You do NOT have to be US-based to apply.
Please apply by sending a PM with your email, and/or link to your resume.
EDIT: this is a PAID internship, as well!",1,2023-12-12 17:48:43
Need help scrapping reviews from Tripadvisor,"Hello everyone,
For a school project i need to scrap reviews from tripadvisor ( things to do page example ) can you please help ? i'm just a beginner
Thanks in advance",1,2023-12-12 13:08:13
Best (and free) alternative to Tweetdeck,"Greetings friends. I've been out of the Twitterverse for some time and was upset to discover Tweetdeck is no longer free. Is there an alternative which works pretty much the same way, organising  like tweets into columns? Such a handy tool to have ... I really don't want to pay for this service, as I'm trying to cut down on spending .. Thanks in advance :)",1,2023-12-12 10:31:57
Automated screenshot to PDF with URL tool?,"Hi! Any recommendations on a tool that could take a list of URLs and screenshot them with a URL stamp? Hereâ€™s an example link with screenshot attached
https://www.amazon.com/newrong-Psychedelic-GT1372-20-Multicolor-59-1x51-2/dp/B08T8ZNY79/",2,2023-12-11 23:34:37
Open Source Instagram Scraping Tool?,"Anyone have any recommendation of instagram scrapers that aren't locked behind expensive subscriptions?
If I need to pay, it's fine, but I don't want to pay hundreds of dollars per quarter.",1,2023-12-11 11:28:58
Links for Facebook posts,"Does anybody know a way to get a list with some fb posts from a pi loc profile, using only http requests? Recently, I noticed that inside the html for a specific page (https://m.facebook.com/razer), you can find only the first first post. Thank you!",1,2023-12-11 08:52:19
OTP sent by SMS,"I'm developing a certain automation flow to some site which has OTP sent by SMS to the registered phone number (my number).
My current direction is using some VoIP provider that has a solid API (Python / Java / whatever) that will allow me programmatically retrieveÂ the SMS content that is sent to its number so my bot will use it.
I've tried twillioÂ but they block verification codes that are receivedÂ on theirÂ number.
Any recommendations for other VoIP providersÂ that will allow that ?Â 
Any other ideas how to get SMS content without running some crap on my phone ?",3,2023-12-10 21:12:41
How do I avoid being detected as a bot while scraping Facebook Marketplace?,"So when I'm logged into Facebook and searching through product offerings I'm wondering how long I could run my script for and how many times an hour I could refresh the screen before getting detected and flagged as a bot? 
I'm just asking for some rough opinions",1,2023-12-10 22:46:17
Anything like scrapy in other languages?,"I didn't find anything like scrapy in other languages. When I say scrapy I really mean web scraping framework scrapy, not beautifulsoup html parser which many languages have. Talking about full on web scraping crawling framework.
Closest I found was https://crawlee.dev/ for Javascript/Typescript although still seems not on the level of scrapy. I didn't try it.
Do you know any other framework for other languages which is we can compare to scrapy?",2,2023-12-10 15:51:23
Chromium webdriver version causing too many errors,"I cloned a repo from 4 years ago for a scraping project that uses python and packages selenium, beautifulsoup4, and Xlsxwriter. It seems that chromium made changes to the way the webdriver function works to something like this: 
service = webdriver.ChromeService(executable_path = '/PATH')
driver = webdriver.Chrome(service=service).  
How do I get this project to continue running? Is there a way to only import chromium of a specific version?
â€‹
I keep getting this error code:   
AttributeError:
'str' object has no attribute ""capabilities'
""/us/local/Lib/python3.9/site-packages/selenium/webdriver/common/driver_finder-py"", Line 40, in get_path
msg = f""Unable to obtain driver for {options.capabilities [browserName 'I} using Selenium",1,2023-12-10 15:33:54
Mutual Fund Reference Data (MorningStar),"This is very specific on the datasetâ€¦.I want to see if anyone has experience in scraping mutual fund descriptive data particularly Morningstar. I used to scrape monthly fund descriptions like region, cap, industry from their old html which was deprecated now. The new site is very different and I feel like my existing library (beautifulSoup) is no longer sufficient for the task. 
Has anyone here attempted and succeeded on their new site? Or is there any alternative site with similar data to scrape?",1,2023-12-10 14:17:15
Roast my web data parser system design,"wdyt - how can I simplify it? ideas what libs to use for each (php, python preferred).
main components and responsibility: 

website - user signup and viewing of a file
data manager - coordinates flow
scraper, parser, emailer... - single responsibility
analyser - adds value to data

â€‹
https://preview.redd.it/p86e9sbqvg5c1.png?width=1520&format=png&auto=webp&s=16d74c49fb784d32443c638e6100d30ce96597f5",1,2023-12-10 13:14:34
No-code Browser automation,"I'm looking for a browser automation tool to automate a task that i have, i want it to be a no-code tool, and to have the ability to make the bot open a new window, do some clicks in that window, and then go back to the previous window, any recommendations for a tool that I can use?",1,2023-12-10 08:24:03
getting files from website,"I'm trying to get files from this site. Can someone help?
https://gaia.cs.umass.edu/kurose/8E_pickup_area/lms_download.htm",1,2023-12-10 04:18:35
Will pay if helpful :) is it possible to Scrape Instagram Stories? help plz,"So basically I run an agency and want to be able to find clients by looking through public instagram stories to see if anyone indicates that they are looking for my service.
The challenge is that I don't have a specific user I want to search for, so looking through all the public accounts and I suppose that's not possible (I thought only the Official Instagram API would have that but they dont seem to have it).
Any ideas? Happy to pay if anyone can help me in any way!",1,2023-12-09 17:49:26
InCapsula bypass,"Hi everyone! 
Does anyone know any good libraries/solution for bypassing incapsula captcha while scraping with python?
Thanks in advance!",2,2023-12-09 11:22:52
Job Opening - Senior Web Scraping Engineer @ Rocket Money,"Rocket Money is hiring a Senior Web Scraping Engineer. This is a full-time role focused on full-stack development (in JavaScript) and web scraping. 
Details here: https://boards.greenhouse.io/truebill/jobs/5781440003
Reach out with any questions!",14,2023-12-08 23:14:53
Interactive Mapbox map,"Hi, I am trying to find all the entries on this map. I see there is an API requesting the data in the name of ""render-tooltip-server,"" but the payload is the issue. The payload has the following parameters: telemeterycommandId, tuplelds, and vizRegionRect. I don't know the logic behind the workings of these parameters or how to extract them.",1,2023-12-09 06:28:35
linkedin scraper architecture,"I need to scrape linkedin public profiles, the volume would be arround 100k profiles per month, does anyone know what would be the costs for this? would something like google cloud functions work or i would need residential proxies? also what type of services are the most cost-efficient? i have never done  anything related to web scraping, i know there is scraper as service, residential proxies sellers etc etc",5,2023-12-08 16:43:46
Scrape online/retail shops in the US that sells our competitor's items,"Hello, how do I scrape online/retail shops in the US that sells our competitor's items?",1,2023-12-08 18:20:35
How to scrape the table on Deloitte Fast 500 Companies using Google Sheets?,"I found the IMPORTHTML(url, query, index) formula online and was brilliant enough to paste the URL into it (below), but I'm clueless about the Query and Index. 
Can you help? Thank you!
https://www2.deloitte.com/us/en/pages/technology-media-and-telecommunications/articles/fast500-winners.html
â€‹",1,2023-12-08 17:48:27
Getting names and images from webpage?,"Hi, for a project Iâ€™m doing I need to retrieve the names, roles and photos of the people working for various non profit organizations.
Letâ€™s take this organizationâ€™s page for example: https://www.batcon.org/about-us/our-team/
If I check the html source page under the div_class â€œperson_detailâ€ I can see the personâ€™s name, their role and get the URL to their profile picture.
I have lots of organizations to go through and itâ€™s extremely time consuming to do it manually. Could anyone suggest a way to automate this, Iâ€™m thinking to scrape the URL but then I canâ€™t think of a way to extract the information I need.
Maybe with help of GPT-4 or a similar language model? Since the HTML structures are different from one website to another, I donâ€™t think I can set up deterministic rules to find the names, roles and url of pictures.
Any idea that could make my data retrieval workflow more efficient would be great. Or a suggestion of how to tackle this task. Thanks!",1,2023-12-08 16:07:48
Reddit web scraping,Is it possible to web scrape reddit contents and create a dataset?,0,2023-12-08 01:54:12
How to bypass Imperva?,"Hello, fellow programmers, I have a problem with bypassing security measures via imperva... â›â›”
The script's task, in short, is to create a session for a logged-in user, enter the appropriate page, complete the reCAPCHA (I do this part manually) and collect data that changes every few seconds for about 30 minutes.
I use the following libraries: undetected_chromedriver, selenium_stealth, as well as random_user_agent and selenium
â€‹
My script outline:
âœ… Goes to the main page
âœ… Goes to the login page
âœ… After logging in, it takes me to the main page
â›” I go to the data tab (at this point I am blocked)
-filling out reCAPCHA
-collecting data
https://preview.redd.it/abyf02wg7x4c1.png?width=1280&format=png&auto=webp&s=493785e9bae8cfb8612e77505c0638490cdea849
I would be very grateful for any help, I hope we can do it together. ðŸ‘ðŸ’ª",1,2023-12-07 19:05:46
Using FB Group data in CustomGPT,"I want to build a Custom ChatGPT incorporating all the threads and comments from a private Facebook Group that I am a member of. For the benefit of the Group so that future questions can be more easily answered. Can I easily/legally scrape this Facebook Group? Or do I need the Group Admins permission and/or signed agreement, to use the data from the group, in my CustomGPT?",2,2023-12-07 14:18:14
Web-scraper on internet?,"Is there a portal or anything where i can just plug in a search term, and have it ""scrape"" twitter or other sites?
I am just curious how well these things work - i do not write code or know anything about code insertion into a site.
thanks",1,2023-12-07 16:53:07
How Do I scrape twitter (preferably without authentication),hello i am newbie webscraper and I need to scrape twitter but i don't know what techologies I need to use or even steps to follow i would appreciate any advice thanks a lot,3,2023-12-07 10:34:46
Issue with Accessing Data,"Hello!
I am a beginner programmer and currently facing an issue with scraping the page https://firstaid4u.ca/east-blended-sfa/.
I need to access the following information:

Course dates,
Timings,
Location,
Course names.

Unfortunately, when JavaScript is disabled, these details are not visible. Therefore, I tried to locate them in the developer tools, especially in the ""Network"" tab.
Regrettably, I am having difficulty identifying where this data is coming from.
I would appreciate any guidance or hints that could help me access this information. Thank you!",1,2023-12-07 15:57:45
HELP WITH SCRAPING LAPTOP SPECS,"I'm relatively new to web scraping and currently working on a project to extract laptop specifications from [https://www.bhphotovideo.com]. Despite some success, I'm facing intermittent timeout errors during the scraping process, and I'm seeking guidance on how to overcome this challenge.
I have been somewhat successful with scraping this page but facing intermittent timeout errors like this: TimeoutError: Navigation timeout of 30000 ms exceeded.
Technology Used: Puppeteer (and a Proxy Service), Cheerio, Node.js
My approach involves using Puppeteer to navigate through the pages, Cheerio for extracting data from the HTML, and Node.js to run the entire script.
```javascript
(async () => {
    try {
        // Initialize puppeteer
        const page = await browser.newPage()
        // Scrape the landing page for each manufacturer page for links to a single device page
        await page.goto(url, { waitUntil: ""networkidle2"" });
        const selector = 'a[data-selenium=""miniProductPageProductNameLink""]';
        await page.waitForSelector(selector);
    const laptopLinks = await page.$$eval(selector, (links) => links.map((link) => link.href));
    // The section above works 100% of the time. Never failed once.

    // Looping through the links scraped above
    let specsArray = [];
    const specSelector = '.group_fDXOr6EpR9';

    for (let i = 0; i < laptopLinks.length; i++) {
        // Open the page
        try {
            const link = laptopLinks[i];

            // Go to the specs page
            await page.goto(`${link}\specs`, { waitUntil: ""networkidle2"", });

            await page.waitForSelector(specSelector);

            // Extract the HTML of the specs section
            const specsHTML = await page.$$eval(specSelector, (element) => {
                return element.map((el) => el.innerHTML).join("""");
            });

            // Grab the title of the 
            const title = await page.$eval('h1[data-selenium=""productTitle""]',
                (element) => element.textContent.trim());

            await page.goBack();

            // parseHTML is an helper function I use to extract the content of the HTML I need
            // and return an object.
            specsArray.push(await parseHTML(specsHTML, title));

            // Save to the database using a Helper function.
            await saveToDatabase(specsArray, ""Dell"");

        } catch (error) {
            console.error(`Error occurred during scraping laptop ${i + 1}: ${error}`);
            return;
        }
    }
    // close the browser
    await browser.close()
} catch (err) {
    console.error(err)
}

})();
```
The code fails consistently after a few runs when reaching the await page.waitForSelector(specSelector) line. It does not successfully scrape all 28 laptops on a page.
Is there something wrong with my approach? Or how can I best achieve this operation? Any insights or suggestions would be greatly appreciated. Thanks!",1,2023-12-07 12:06:57
How to detect and delete drop down menus from any random website,"I need to scrape product info from product pages, but leaving just all the text leaves too much irrelevant info that comes from drop down menu's for categories and stuff. I need some kind of rule I can use to delete drop down menus. As I noticed they're mostly <ul>, <li> and <nav>. Nav's I can get rid of safely I think, but deleting all text inside lists can damage valuable info... Any ideas?",1,2023-12-06 19:34:12
Help with Scraping Fetlife,"Hey all,
I built a script that automates login to Fetlife and scrapes user profiles based on criteria. I am using Playwright to do this. It worked well for years up until last month when FL added Cloudflare checks which results in getting this error in the brower
Failed to load resource: net::ERR_BLOCKED_BY_RESPONSE.NotSameOriginAfterDefaultedToSameOriginByCoep
Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'browsing-topics'.
Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
If you look at the browser there is a ""Verifying"" spinner that never completes and all subsequent requests fail. I am not too experienced in scraping so not sure if there is a way to get around this. I can provide some code snippets if that helps.
â€‹",0,2023-12-06 23:03:07
a parsehub freelancer wanted to take 150$ for this. is it actually justified?,"i want to give the program a whole bunch of different aliexpress product pages links. its supposed to return the ones that are available to purchase and the ones who arnt, in two seperate lists on an excel. how can it determine if its available or not?- by looking if there is or isnt a purchase button in the page.",0,2023-12-06 11:47:43
Alternatives to scraping hidden data without using the headless browser,"Basically what the title says. I am trying to scrape the data that is hidden as the content is dynamically loaded. I am using Scrapy, so I cannot use headless browser to mimic the behaviour.
How would one typically go about this?",3,2023-12-05 20:25:00
Good Websites to Scrape for a Class Project?,"As the title says, I am looking for a website that would be good to scrape for a project for one of my data science classes. This seems simple enough, but the issue is that I need a minimum of 2,000 observations of at least 20 different variables. Most of the websites I have found that have that much information are either incredibly difficult to scrape (especially since I am a beginner) or block me after a very short amount of time, even after adding a delay. Does anyone know of any websites that would both have this amount of data, but also would not be too complicated to web scrape?",1,2023-12-06 00:53:22
Looking for advice on webscraping,"I'm kinda new to web scraping and would like to learn more about what are the most recent methods or tools scrapers are currently using to avoid detection, scrap efficiently and correctly. 
Also is there a way to have a ""generic"" scraper that could word on multiple websites without being tailored to work specifically on each one  ?  I'm trying to make a scraper for published scientific articles from various websites for some uni project.
Any help would be great, thanks!",1,2023-12-06 00:22:23
Can anyone recommend an API which will scrape whatever video file (direct url) exists on a webpage?,"I'm looking to use an API which will allow me to input any link from youtube, vimeo, tiktok, facebook, linkedin, etc, and then return a direct video url to that (the .mp4, mov, etc)",2,2023-12-05 15:46:22
Need help,"I'm new to web scraping, i need the data of pet stores in us can you guys guide me to do it",1,2023-12-05 10:17:05
need Help with is error,"'<head><title>Not Acceptable!</title></head><body><h1>Not Acceptable!</h1><p>An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security.</p></body></html>'
i am facing this  error  everytime am time am trying to scrap this website  'zabusaries' .Is there a way bypass this",1,2023-12-05 10:13:07
How to scrape this type of sites.,"Does anyone have a suggestion on what method to use for this site ?  
https://skyward.iscorp.com/scripts/wsisa.dll/WService=wsfinnorwinpa/rapplmnu03.2 
I am having a hard time even accessing the list. Sometimes I get a WebSpeed error while trying to access the data.   
Thank you in advance.",1,2023-12-05 09:08:19
Sitemap generation without triggering adsense,"Hello All
I created a node script using puppeteer that i want to use to generate a Sitemap for the website. This particular website has a lot of links generated as pSEO so there is over 5K links. The script basically crawls each page and extracts the links from the pages. The issue i have is the website has google adsense running and google is triggering the account as too many suspicious requests. This is my website im doing it against so im wondering if there is a flag or something i can pass that will stop adsense from being triggered.",1,2023-12-04 22:14:47
Marketing,"How would I go at making a scraper or anything that gets fed url, e.g. facebook.com and then based on this url I would find some relevant information about facebook like what is company about, processes, work ethics, maybe even contact info, some images to follow on top of the textual data, company name, logo and similar. The only data I would have is url, it can be facebook.com, it can be cnn.com, or any other url for any company.",1,2023-12-04 21:39:11
Web labels me as a bot after scraping 5 cars (Scraping coches.net),"So i'm trying to scrape this web: https://www.coches.net/segunda-mano/ that sells used cars, my goal was to get the link of each car in the front page and then scrape the data of all the cars by going into each link. My code works perfectly fine and does everything right, but after scraping the data of 5 cars, the site knows that i'm a bot. After this i'm not able to scrape more data after at least half an hour. How can i improve my code so they don't detect me as a bot anymore? 
This is my code: 
import requests
from bs4 import BeautifulSoup
import time
from fake_useragent import UserAgent
import random

# Initialize the UserAgent object
ua = UserAgent()

headers = {
    'User-Agent': ua.random,
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Referer': 'https://www.google.com/',
    'DNT': '1',
    'Upgrade-Insecure-Requests': '1',
}

# Use a session for persistent connections and automatic cookie handling
session = requests.Session()

for i in range(10):
    url = 'https://www.coches.net/segunda-mano/'
    r = session.get(url, headers=headers)

    time.sleep(random.uniform(3 + i, 7 + i))

    soup = BeautifulSoup(r.content, 'html.parser')

    # Extract car links
    links = soup.find_all('a', class_=""mt-CardAd-media"")

    # Extract href attribute for each link in the result set
    links_href = [link.get('href') for link in links]

    # Print each car link
    for link_href in links_href:
        print(""LINK: "", link_href)
        new_link = ""https://www.coches.net/segunda-mano"" + link_href
        r = session.get(new_link, headers=headers)

        time.sleep(random.uniform(2 + i, 4 + i))

        soup = BeautifulSoup(r.content, 'html.parser')
        # Extract car titles
        car_titles = soup.find_all(""h5"",
                                   class_=""mt-TitleBasic-title mt-TitleBasic-title--xs mt-TitleBasic-title--black"")
        for title in car_titles:
            print(""NOM: "", title.text)

        # Extract car prices
        precios = soup.find_all(""h5"",
                                class_=""mt-TitleBasic-title mt-TitleBasic-title--xs mt-TitleBasic-title--currentColor"")
        for precio in precios:
            print(""PREU: "", precio.text)

        # Extract characteristics
        ul_element = soup.find(""ul"", class_=""mt-PanelAdDetails-data"")
        if ul_element:
            for li_element in ul_element.find_all(""li"", class_=""mt-PanelAdDetails-dataItem""):
                print(""Caracteristica: "", li_element.text)
        else:
            print(""No <ul> element with class 'mt-PanelAdDetails-data' found."")
            time.sleep(random.uniform(1800, 1805))

â€‹",1,2023-12-04 17:12:22
"List of URLs I want to scrape, any help?","I have a list of URLs in a Google Sheets column that I want to scrape. URL only contains a JSON like that : https://annuaire.experts-comptables.org/expert-comptable/300000407001/telephone.  
What is the best way to scrape the Phone number on these JSON files? I have more than 500 pages to scrape :D",1,2023-12-04 15:24:46
Need Help with Inconsistent Data While Scraping Hotel Prices,"Hi everyone,
I'm currently working on a web scraping project where I'm trying to extract price data from a well-known hotel's website. However, I'm encountering an issue where the data I scrape is inconsistent. Sometimes I get the correct prices, but other times, the data seems off or incomplete.
I'm relatively new to web scraping and would appreciate any insights or advice on how to tackle this problem. Below is the code I'm using:
import requests
from bs4 import BeautifulSoup
â€‹
headers = {""User-Agent"":.......}
url = 'Example.com'
resp = requests.get(url, headers=headers)
soup = BeautifulSoup(resp.content, 'html.parser')
price = soup.find_all('span', class_='prco-valign-middle-helper')
â€‹
Sometimes I get the discounted correct price I visualize when visiting the website and other times I get the non discounted price when present.",1,2023-12-04 11:53:35
freelancers of r/webscraping what are the frameworks and libraries you use to do your job,"Just curious, thanks for your time ðŸ˜„",9,2023-12-03 16:34:34
websites where scraping is legal,"Hello guys,
Do you have any suggestions about websites that let you scrape reviews?
It would help me websites of e-commerce, or where it talks about products, BUT big e-commerce sites put a lot of obstacles and you can not scrape all the data to use in sentiment analysis...",2,2023-12-03 15:27:05
How to retrieve Reddit comments in the right order in PRAW?,"I'm looking to collect comments on submissions in a handful of subreddits through PRAW. However, I'm only able to collect first all the 1st level comments, then all the 2nd level comments, followed by all the 3 level comments etc. I'd prefer it if I could collect the first level comment, the replies to it and replies to replies so I could follow the discussion. I've been looking at the PRAW tutorial and documentation but I've been unable to make progress. Is it possible to have the comments in the right order with PRAW?
Alternatively, I think I could get the comments/discussion in the proper order if I used Scrapy instead of PRAW. So, collecting submission IDs, building the ulrs with IDs and using Scrapy to fetch the comments, but would that be frowned upon by Reddit?",1,2023-12-03 16:23:20
Ebay scraping help,"How do i avoid scraping results of spare parts( costing a few dollars 5-10) when I want to scrape prices for the main products ( costing 2000$ and above)
Any help is much appreciated.",1,2023-12-03 15:34:39
Search query text,"Hi. Wondering if itâ€™s possible to scrape the queries that searchers on google etc. for example, Iâ€™m looking for mentions of â€œfood donationsâ€â€¦ can I find out the language people are using to search for it?",1,2023-12-03 15:11:02
How to find restaurants and cafes numbers,Hello everyone Iâ€™ve been looking for the past week for a viable easy to use google map scraper since im starting a local seo agency and Iâ€™m looking for a way to scrape google maps for reviews and phone numberâ€¦ can you guys help me on how to do that please.. thanks,1,2023-12-03 13:24:06
Is there a method to scrape Twitter that works?,Title says it all: I wanna scrape Twitter data without being blocked. This is for my graduate study research and I want to get historical data all the way back before COVID-19. Is this feasible? Is there any library that allows this?,2,2023-12-02 17:35:42
Legal resources or guidance?,"Some former colleagues have started an open source intelligence (OSINT) business that leverages social media data (Twitter, facebook, Instagram, TikTok, YouTube) to surface insights related to criminal activity, primarily for public sector organizations. Additionally they scrape various public websites for information.
Notably, they are doing this for clients in several countries and have aspirations for global scale.
They are currently avoiding all PII and any restricted content, and are not (yet) commercializing or sharing the raw data. So they believe these keeps them on legally sound footing. However they want to be proactive rather than reactive in ensuring evening is done in compliance with any rules and best practices.
What specific online resources would you recommend for navigating the varying laws and regulations? Are there rules of thumb? Are there particular traits of legal counsel youâ€™d recommend?
They are willing to put cash toward this, but want to ensure basic foundations are in place first.",1,2023-12-02 18:43:08
does using ajax bypass website/cloudflare automated user detection,"hello, so I was working on a website where I have to click some things or make ajax calls to follow onto the next page, and after a while I thought why not automate it using javascript on tampermonkey and python to control real user input, which quickly got me stuck on a cloudflare bot page. I didnt have any problems doing what I was doing using only ajax calls, but would like the input of others before I get ip restricted or something. thanks :)",0,2023-12-02 17:30:03
"Bypass Cloudflare's ""under attack"" mode w/ puppeteer ?","Hi,
The puppeteer-extra-plugin-stealth module usually bypasses Cloudflare, but not when the ""under attack"" mode is enabled.
What to do in this case ?
Thanks
UPDATE : solved by https://github.com/berstend/puppeteer-extra/issues/817#issuecomment-1669544250 (add targetFilter: target => !!target.url() to puppeteer.launch params).",3,2023-12-01 19:15:23
Uni project - Web Scraping of Mountain Shelters,"Hello everyone! 
I need ur help: can you help me extrapolating from this page (https://www.escursionismo.it/rifugi-bivacchi/) a csv file or a database in whatever format you prefer that contains all the mountain shelters for those three regions: Piemonte, Lombardia and Friuli Venezia Giulia. You can select the region by clicking on the advanced search option. 
It would be optimal if the code conducing the scraping is in Python. 
Thank you in advance to anybody willing to contribute constructively! ðŸ¥°
https://preview.redd.it/1k0jtmljjp3c1.png?width=2880&format=png&auto=webp&s=daaf53b80fdf6db43e6208296ba80b801156aba5",2,2023-12-01 16:11:56
Monthly Self-Promotion Thread - December 2023,"Hello and howdy, digital miners of /r/webscraping!
The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!

Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?
Maybe you've got a ground-breaking product in need of some intrepid testers?
Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?
Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?

Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!
Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",4,2023-12-01 11:02:27
Puppeteer: Development without getting blocked?,"I want to scrape this site: https://www.immobilienscout24.at/regional/tirol/wohnung-mieten
the thing is my IP isn't blocked. When I visit the site everything loads really fast. Duo to having to reopen the puppeteer-browser after every few code changes they somehow detect that this browser is automated and make loading speeds really slow, if it even loads.
How to prevent this?  
I saw a few recommended proxy servers, but I'm currently not willing to pay for a proxy, at least while in development. I could also use a vpn like proton vpn or UrbanVPN but even there after reopening it to many times i need to switch my vpn to another country so my ip is different.
â€‹
Is the site somehow detecting that this browser is controlled by software, because as I mentioned it, if my ip would have been blocked I couldn't even access it in my daily browser.
â€‹
Gratefull for any advise!",1,2023-12-01 16:20:20
I need some help with page scraping and script release on cloud,"Hi, basically I have a scrapper which goes through real estate listings. I came into few issues. Anyone can suggest any solutions for cloud solutions, how do you release your script, where you release it, basically I need that script to run periodically. Thanks for suggestions",1,2023-12-01 13:38:50
Scraping Social Media Links from Genius Artist Page,"Scraping lyrics and bios from Genius was easy using Cheerio, an NPM package. But when I was trying to get the Social Media (Facebook, Twitter, Instagram - see screenshot below), I fail to get it. Exhausted everything from Cheerio. Came here for help. Was wondering if there are better tools in the first place? Or am I just doing things wrong with Cheerio. Any help would be much appreciated.
https://preview.redd.it/pxgwu2puqo3c1.png?width=2026&format=png&auto=webp&s=89b5b4d2e4d59f9125265be9d44b0ce40566f241",1,2023-12-01 13:31:28
Cloudscraper with asyncio,"Hello, as the title says i have been using cloudscraper to access a website I need to scrape, however as the size of the data I need grows I would like to use cloudscraper either with asyncio or multithreading. Is this possible? what other alternatives are there for scraping a website that needs a cloudflare bypass?
â€‹
I'm using python.",1,2023-11-30 14:14:54
Instagram,"Hello there
I want to boost my Instagram following. Iâ€™m guessing people that follow my competitors would like my content too.
How would I get a list of my competitors followers and then follow them in the hope they check out my Instagram, like what they see and follow me back?
Thanks and have a blessed day",0,2023-11-30 13:12:19
Webscraping monitor tools,"Is there any solution to monitor a webscraping script deployed on multiples machine , something like number of pages scrapped per machine.
ps: I can store the data in machine local storage or on a shared database",1,2023-11-30 10:07:54
Rightmove autocomplete search data,"Dare I ask does anyone have access to Rightmove autocomplete search data? 
https://preview.redd.it/ypt0p9hklg3c1.png?width=652&format=png&auto=webp&s=e970f9fd77c8415cbfb68ed8b8a36dacad0feb32
https://preview.redd.it/gnydvujilg3c1.png?width=750&format=png&auto=webp&s=4e58f6b353b29a97241135dd0d7096d6998eead3",1,2023-11-30 10:06:36
How to Bypass Akamai,"Akamai is renowned for its advanced anti-bot and anti-scraping solutions, widely adopted by major websites like Nike.com. The effectiveness of Akamai lies in its sophisticated approach that combines traditional fingerprinting techniques with behavioral analysis. This analysis involves tracking the JavaScript events that are triggered during a user's interaction with the website.
Recognizing Akamai
To identify Akamai's presence on a website, look for these characteristics:

Endpoint Structure: Akamai typically operates without external servers, sending data to an endpoint on the website's domain. These endpoints are characterized by long, random character paths that change with every new page load. Each endpoint typically ends with a seven-character string, comprising a mix of random lowercase and uppercase letters. Example from Nike.com:

â€‹
https://www.nike.com/-V9sK5EuHw5zf/s5azPS0VO0/SGj4/3z9ifbmQLb/M0lvcEBEAQ/a0tSI/UgeeikB


Sensor Data:  Akamai sends a key JSON payload named sensor_data to these endpoints. This data is encoded or encrypted using a proprietary algorithm, making it distinct and recognizable. Example payload from Nike.com:

â€‹
{""sensor_data"":""2;4337974;4473908;15,0,0,0,2,0;E9_ 4*o9=r]a0cpl4wxD!z/b4xhdXjOQD4`,=D{bOJjj4:Ob&oj?(6D UK8,-@2;{*U1@Y2jjl+*Uu8=cbeB#HC$]^HeXmd%sZFm5K3rwzDK{=iw;HV}d;J+hn]wr43Y6F}[Q,TF&~?v*V3:q~+jbv7&jh])6JT^wcQF](wf|vPkOrG_(3QG>],jMSj/!Na*w>rh3]WAcv8YMnC) IJJBZKF8E9iPXh2)gmsEmv`@QSY?d:%0KgTC&R9Bj3ZqGX7>=9f2rjbf=X>.SExZq45>i1FGWkIf-Cl8]BL2ffk2g1l_8%aG^=5: Q[ct?Y*m(f%MFU}hlxNq[68ejD&W~27%jru9N5[>bJ[5;B~f|~i!sU^54s_0g}$w[_fDt<*R-v3by%gUu#d1w`Q52K2Y=`z[KevKiU*#X8X9HWG2hnUd`{q`jR[%q$M@$Si;l^.{iR$Sx&(~-W;z}l%q)lpP6Q}pd(E/RF+}dO8n,@M.n5|MhZgk}euOJqC5=^...""}


Cookies: Akamai set the following cookies on the client browser:


_abck
ak_bmsc
bm_mi
bm_sz
bm_sv


Akamai's Device Fingerprinting
Akamai extensively uses techniques like Canvas Fingerprinting and WebGL fingerprinting to identify user devices. This is achieved using JavaScript, as illustrated in the provided scripts. These scripts create a unique identifier for each device based on how it renders certain elements.
var n = document.createElement(""canvas"");
if (n.width = 280,
  n.height = 60,
  n.style.display = ""none"",
  ""function"" == typeof n.getContext) {
    var o = n.getContext(""2d"");
    o.fillStyle = ""rgb(102, 204, 0)"",
      o.fillRect(100, 5, 80, 50),
      o.fillStyle = ""#f60"",
      o.font = ""16pt Arial"",
      o.fillText(t, 10, 40),
      o.strokeStyle = ""rgb(120, 186, 176)"",
      o.arc(80, 10, 20, 0, Math.PI, !1),
      o.stroke();
    var m = n.toDataURL();
    e = 0;
    for (var r = 0; r < m.length; r++) {
     e = (e << 5) - e + m.charCodeAt(r),
     e &= e
   }
   e = e.toString();

   var i = document.createElement(""canvas"");
   i.width = 16,
     i.height = 16;
   var c = i.getContext(""2d"");
   c.font = ""6pt Arial"",
     a.rVal = Math.floor(1e3 * Math.random()).toString(),
     c.fillText(a.rVal, 1, 12);
   for (var b = i.toDataURL(), d = 0, s = 0; s < b.length; s++) {
     d = (d << 5) - d + b.charCodeAt(s),
       d &= d
   }
   a.rCFP = d.toString()
}

...

try {
   var t = document.createElement(""canvas""),
     a = t.getContext(""webgl"");
       bmak.wv = ""n"",
       bmak.wr = ""n"",
       bmak.weh = ""n"",
       bmak.wl = 0,
     a && (bmak.wv = ""b"",
       bmak.wr = ""b"",
       bmak.weh = ""b"",
     a.getSupportedExtensions() && (bmak.weh = bmak.ats(bmak.mn_s(JSON.stringify(a.getSupportedExtensions().sort()))),       bmak.wl = a.getSupportedExtensions().length,       a.getSupportedExtensions().indexOf(""WEBGL_debug_renderer_info"") >= 0 && (bmak.wv = a.getParameter(a.getExtension(""WEBGL_debug_renderer_info"").UNMASKED_VENDOR_WEBGL),       bmak.wr = a.getParameter(a.getExtension(""WEBGL_debug_renderer_info"").UNMASKED_RENDERER_WEBGL)))) } catch (t) {
    bmak.wv = ""e"",
    bmak.wr = ""e"",
    bmak.weh = ""e"",
    bmak.wl = 0
 }

Akamai Behavioral Analysis
In its highest security settings, Akamai can capture multiple sensor_data payloads per minute. This includes recording various user interactions like mouse movements, clicks, touch events, and keyboard inputs. The system also tracks the positions of these events on the page.  Here is the event it is listening to:

touchmove
touchstart
touchend
touchcancel
mousemove
click
mousedown
mouseup
pointerdown
pointerup
keydown
keyup
keypress
touchmove
touchstart
touchend
touchcancel
onmousemove
onclick
onmousedown
onmouseup
onpointerdown
onpointerup
onkeydown
onkeyup
onkeypress

Bypassing Akamai
Bypassing Akamai's security requires a nuanced approach, especially considering its reliance on GPU rendering information to determine the operating system and device type.   Here are some strategies:

GPU Rendering: Emulate consumer-grade GPUs rather than professional hardware, as Akamai's algorithms are tuned to recognize and differentiate between them.
Behavioral Analysis: Utilize tools like ghost-cursor (found at https://npmjs.com/package/ghost-cursor) to simulate human-like cursor movements and keystrokes. Timing is crucial here; movements or keystrokes that are too rapid can be flagged as suspicious.

Learn more:
We're talking about multiple ways to bypass Akamai on the Web Scraping & Data Extraction discord server!
Come say hi! https://discord.com/invite/fHbbHTq4CQ",27,2023-11-29 06:54:28
How to get email address from this site?,"hello can any one tell me how can I get email address and web site from this site ""https://www.gsaelibrary.gsa.gov/ElibMain/contractorInfo.do?contractNumber=47QTCB22D0554&contractorName=1+ADAPTIVE+ARROW+ALLIANCE+LLC&executeQuery=YES""
I want to extract by using beautiful soup!
â€‹",0,2023-11-29 12:24:18
Can you scrape a website that is generated after the page loads by JS or something else?,"I am trying to get a Cookie and Bearer token generated inside this page
https://secure.reservit.com/fo/booking/58/53991/dates
â€‹
https://preview.redd.it/rwt8ho45253c1.png?width=1455&format=png&auto=webp&s=b4e9a96129adef1f15881810862883fd01f34b45
But all these data are generated after the page loads. Is it something possible by using a headless browser?",3,2023-11-28 19:19:10
Looking for Web scraping solution for jobs and grants listing â€¼ï¸,"I have job-board listing where I share various types of jobs/scholarships/grants, etc. that help people travel the world  
Instead of manually clicking and reading and typing in every detail, I want to improve efficiency of making new posts. 
I'm looking for a way to create or find a webscraping tool or service that can...
1. Scrape links that I copy/paste into it, and pull information for specific key fields (title, deadline, eligibility requirements, etc)
2. Insert the information directly into an Excel spreadsheet/Google sheet.
3. Preferably thenâ€”somehow using Zapier or some other tool or wayâ€”directly into our WordPress website backend to create a fully filled out draft in the WP Job Manager plugin  
(*To be clear, I don't need the tool to find new opportunities - just scrape info from the links that I find)
What's the best way to do this? I'm not techy at all
ChatGPT?
A bot?
A Google chrome extention?
And if I do need some developer to custom make it, what should I specifically ask them to do? 
â€‹
Thanks so much I'm really overwhelmed by this",4,2023-11-28 17:06:04
Scraping facebook,"Hello people, I am a newbie to scraping and wanted to scrap facebook data based on keywords for a project of mine. Is it possible to do so and if yes then can you list out any source from where I can get a overview on how it is done.
Thanks !!",3,2023-11-28 15:44:39
Learning how to monitor navigator and other Browser API objects,"Hi, very new to all this. Started learning Javascript and wrote a quick puppeteer script some time back to scrape. Interested to know what frameworks or software you fine folks are using to detect how Navigator and Screen API objects (as an example) are being called by heavily obfuscated telemetry scripts. I've started down the path of reversing telemetry scripts and patching certain calls on ""page load"", but you only can patch what you know is getting called.
â€‹
TL;DR how are you all detecting what API objects a telemetry script is touching?",1,2023-11-28 19:27:44
How to avoid this error?,Hello when I try to scrape this page I get a message 403 to enable the cookies and javascript how to avoid this on Postman?   https://www.lancome.com.br/,1,2023-11-28 17:40:17
Scraping Data Error,"I have a web scraping project and I was blocked IP from the web once so I bought a rotated proxy. However, when I run the code it said that: 
â€œProxyError:HTTPSConnectionPool(host='bonbanh.com', port=443): Max retries exceeded with url: /oto-cu-da-qua-su-dung (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 407 Proxy Authentication Required')))â€
Please tell me what I need to do resolve this problems, Iâ€™m very desperated rightnow ðŸ˜­. Thanks for reading.",1,2023-11-28 16:45:57
Web-scraping helpâ€¦am I missing something?,"Looking to do a web-scraping project for a college class, specifically on US newspaper article data. Most of the APIs are pretty expensive and outside my budget. Is there a way to do web-scraping on an academic database like Lexus Nexus? Would make me life a whole lot easier.
Thanks everyone!",1,2023-11-28 11:56:10
Completely new to webscraping is this possible?,"Hey guys , I'm completely new to the webscraping world and I was mainly driven here on my journey to extract data from a subreddit for marketing research purposes. I don't really quite understand how webscraping works entirely other than you have to write code. I've watched a bunch of YouTube videos as well and I've seen people extract data like posts and the comments of the post from a subreddit. However my question is, would it be possible to search for a specific keyword in the entirety of the subreddit or maybe even around like 5000 posts. would that be doable or is it a very difficult task? I also wonder if it would be possible to automate and create a bot that scans and constantly updates you if someone comments a specific word within the subreddit. Am I expecting too much? I'm asking before I invest my time into potentially learning how to code or finding someone to hire for a task like this. Would be grateful for any insight.",3,2023-11-27 23:08:10
Shopify app devs,"Hey I want to put together an email list of shopify app developers, for this could email campaign but I'm struggling to find a good database of shopify app developers. Any suggestions?",2,2023-11-27 18:52:14
What tools should I use if I want to iterate through a website that requires login credentials?,"I have many job postings identified by ID that I want to review. This can be done by changing the ID in the link, such as https://www.linkedin.com/jobs/view/3720156786. I have a list of these IDs, but since LinkedIn requires a login to access the pages, I am a bit lost and unsure about which libraries to use, preferably in Python. I am completely new to this, so I'm not sure where to start or what tools to use.
Ideally, I would like a solution where I can log in once and then iterate through the pages, collecting the necessary information from the HTML code.  Additionally, I'm curious about whether the collected data will be encrypted.
Thank you.",0,2023-11-27 11:53:37
Scraping Google Maps Data,"I am looking to scrape Google Maps data but itâ€™s pretty simple. No phone numbers or emails needed. I just need business name and business category (a couple specific ones) location and if possible search based on about description and website preview. But the big one is the business category. I already have specific individual contact info from another source   
What is my best option for the means and methods of doing this? My company had a part time IT guy who Iâ€™ll get involved but I wanted to first probe around and see what this community thought about where to start for the specific data im looking for.",3,2023-11-27 02:38:21
Need some proxies or suggestions,"Hey guys,
I do quite a bit of webscraping and have all my other sites down no issues. Who all does everyone use for proxies? Iâ€™ve tried one proxy who I use for a few other sites but the cookie I had was causing it to get blocked. Local run on node JS got through with zero issues with the same cookie, and if I go with no cookie through the proxy it has zero issues as well. Good ole cloudFlare has this site locked down tighter than most Iâ€™ve seen with bot checks often. All cookies are encrypted as well so what each one does is hard to say. Iâ€™ve tried turning on and off each one but the moment I touch them a bot check is instant and will forever check any IP with that cookie. Like wise, I can run using puppeteer stealth all day on the website with no issues. As soon as I use that same cookie with my current proxies itâ€™s instantly bot checked followed by any ip with that cookie also having the same issue. 
Thanks for any advice!  
TLDR: cloudFlare encrypted cookies are making my life difficult and I need a proxy to get past the bot checks.",1,2023-11-26 17:03:52
Scraping McDonald's Europe,"I'd like to compose a list of all McDonald's restaurants in certain European countries.
Many countries (eg. the UK, Netherlands, Switzerland) use a common API which delivers the results as a JSON array:
https://www.mcdonalds.com/googleappsv2/geolocation?latitude=51.54&longitude=-0.15&radius=200&maxResults=150&country=gb&language=en-gb
https://www.mcdonalds.com/googleappsv2/geolocation?latitude=52.55&longitude=5.60&radius=200&maxResults=150&country=nl&language=en-nl
https://www.mcdonalds.com/googleappsv2/geolocation?latitude=46.20&longitude=6.14&radius=200&maxResults=150&country=ch&language=en-ch
The parameters should be quite self-explanatory.
But the API seems to have a upper limit regarding the ""maxResults"" parameters depending on the ""country"" parameter. While the GB call returns 150 results, the NL call only returns a subset of 38.
It seems to be a Google API. IS there any known way to bypass this behaviour?
At the moment I bypass this limitation by setting the maxresults to a low number, changing the latitude and longitude parameters by a slight margin and calling the API again and again eliminating double results afterwards. But I'm worried that I might miss results in dense areas like London or Amsterdam.
Any ideas or suggestions?",5,2023-11-25 17:05:32
Any tools for generating structured data from messy html?,"I'm looking for advice on tools to use in web scraping processes where its not possible to extract data from html consistently. I've run into this in two different situations,  either because I am scraping many different websites, so its not possible to write a distinct scraping process for each website, or because the pages being scraped are not consistently built (ex. free form text in forums, inconsistent html, etc...).
Ideally any AI / ML options for throwing html into an AI model and receiving cleaned data back out.
Has anybody else run into this situation before? What tools did you use?",2,2023-11-25 15:23:06
Scraping through automating gui tasks?,"I tried to retrieve a webpage with python requests, but it failed. My google research suggests that the site in question uses protective measures to prevent bots from downloading data and that it also blocks selenium. 
Now I manually surf to the pages and tabs on the pages in question, save them manually and then process the saved data. It is only a few pages so manually doable, but not ideal. Is there any way to automate this process? I came across PyAutoGui that looks like a solution, but at first sight it also looks like such script would be very fragile and prone to break. 
Is this impression of fragility correct? Are there, preferably free, alternative tools available that are better suited for the task? Any ideas on robust ways on how to get the job done?",3,2023-11-25 03:31:20
School assignment,"Hi everyone,
I am a master's student and I have a web scraping assignment. I need to collect a dataset of at least 5000 records using scraping - we have mainly use Chrome extension and Octoparse. Afterwards, I have to create at least five data visualisations based on the data using Tableau Public or KH Coder. 
Can someone please help me out with this assignment? Any website is perfect, I just need some indications on how to complete the fields on the Chrome extension for a specific web page.
Thank you!",1,2023-11-25 08:56:04
Web Scraping Legality,"TLDR: Educational website paywalls content unsuccessfully, can I re-host publicly available data? 
â€‹
So here's the situation, there's a fairly large educational website who monetizes it's content behind a paywall. Aka some free stuff and then the rest is locked behind a paywall. They sell monthly/yearly/lifetime subscriptions.
They teach a certain web technology, the irony is their own implementation of the web technology is flawed (which ngl pisses me off). As such they have a bug in their code that allows anyone with a free account to access all their content in a relatively easy way (if you know how). The bug is reproducible and easy to exploit. I genuinely feel like I'm maybe the only person that has found this? I've searched for scrapers to this site, on GitHub, but all of them rely on having a ""premium"" account. Also their content isn't really available on pirated websites.
I reached out to them telling them that their paywalled content can be accessed via a bug (I didn't explain the steps to bypass), and after some back and forth they said they would have me talk to a developer, nothing came of it (its been months). I feel like my ethical obligation is over. 
Now how legal/illegal would it be to create a website that serves their content? I already created a pretty robust python cli that can download all their courses, and the course metadata.  I will not be downloading their videos and hosting them my self, my webapp would link to their own hosted content, so as far as I'm concerned I'm not distributing ""stolen"" content, but instead distributing links (that can be accessed publicly). 
AKA
<video src=""their-website.com/video.mp4""></video>

Would they have any grounds for legal recourse? 
â€‹",8,2023-11-24 11:25:59
Can websites track me through undetected selenium even with proxies?,"I'm trying to scrape this website: https://www.bhphotovideo.com/c/buy/rebates-promotions/ci/22144/N/4019732813 using undetected selenium. But after a few pages, I usually get hit with bot detection, the problem is that the bot detection seems to track through proxy, even if I change my ip, it still figures out very quickly, usually after 1-2 pages. 
Can website track me through selenium with proxies using things like OS, browser version, download speed etc.? If yes, anyone know how to avoid that? If no, anyone knows why do I keep getting blocked?",3,2023-11-24 15:21:46
How to download online documentation?,"If I wanted to download all of the online documentation for a product or service (assuming the documentation is available only online) how would I do that?
Let's use a couple of big examples like https://developer.wordpress.org/reference/ or https://docs.python.org/3.12/....",3,2023-11-24 06:29:51
Any threadsafe selenium/playwright analog there?,"Hi.
Searching for some threadsafe analog of playwright for python.
Like, I start 20 tabs of one instance of browser, then give each tab to its own thread. And thread independently works with each tab.",2,2023-11-24 11:41:32
Webscraping for accommodations,"I know nothing about webscraping - be gentle with me.
I want to collect market date for temporary accommodations to assist in eventually booking with the best suited accommodation. How difficult/legal/ethical would it be to webscrape Airbnb, for example, with parameters like this:
Iâ€™m looking for 2 bed, 1 bath, pet friendly accommodation with a 5 mile range of this zip code. Return the number of properties that fall into a $150-250/night bucket, $250-350/night bucket and $350-450/night bucket, with URLs.
Or more specific cases of maybe wheel chair accessible, or has a hot tub/pool etc.",1,2023-11-24 05:39:30
Scraping to Dynamic web,"Hello community, I'm new to this web scraping, I want to get some data for a personal project, ie I need to automate processes, the detail is that I found a page that shows little html and I can not access the content I need, do you know any way to access all that html?",1,2023-11-24 04:07:26
Webscraping Leafly,"Based in the UK Im building an app that would use similar data to websites like leafly and Weedmaps.
I want to collect strain information, product information, pictures and brand information.
The data is publicly available and not behind a login wall but they restrict it's collection in their terms of service.  Would this data count as intellectual property?  I'm not collecting user generated data just generic information about cannabis strains.
Is this something that many apps do that generate revenue and is just a grey area that's not heavily enforced or is it really something they could come after you for.",1,2023-11-23 13:52:50
Remove cookie tags from html,"I need to extract text from a website, im when using 
soup.get_text()

, some websites have cookies, resulting in the retrieved text being filled with cookie policy information. Therefore, I need to remove the cookie tags from the HTML. How can I achieve this when the website is dynamic, meaning I only obtain the entire page with Playwright and not through requests?",1,2023-11-23 10:52:49
Booking appointment,"Hi people, is anyone here who can book appointments whose dates are not available using web scrapping or something else.",1,2023-11-23 07:01:58
Stock Analysis Scraper - This tool scrapes 0.8 million data points at 15-minute intervals and directly inserts them into Google Sheets.,"Hello everyone, I have created a script that allows you to scrape data from stock analysis websites and apply filters in your Google Sheet. I believe this can be helpful for you in learning Google Apps Script. I am sharing the URL to my GitHub repository:
https://github.com/sahilsuman933/Stock-Analysis-Scraper",5,2023-11-22 13:59:30
BeautifulSoup,BeautifulSoup has powerful tools for parsing data for utilization but often requires a good understanding of the document structure and the underlying parsing concepts. Would you say BS could be used sufficiently in an automation project that will be collecting data from 10 select websites?,1,2023-11-22 20:33:11
New to webscraping - Meaning of robot.txt?,"Hi everyone, 
I have a webscraping project where I wanted to webscrape Glassdoor Interview Questions.
Now, I read about the robot.txt concept and consequently looked it up from Glassdoor. To me it looks like they specifically do not allow web scraping on their Interview questions. 
I was wondering the following: Are there alternative ways to download this data which is not considered webscraping?   
I found a few github repositories which actually do webscrape glassdoor, however, I was wondering if Glassdoor had a different policy back then. 
â€‹
I would appreciate if you could give me some pointers on what the appropiate behaviour would be in this case. I do not want to disrespect the websites' preference on webscraping, I am simply wondering if there are alternative ways that are generally okay to use in a case like this.",0,2023-11-22 19:42:08
Lightweight Web Scraping Framework Suitable for API,"I'm looking for a lightweight automation framework capable of handling simple tasks, such as loading pages and gathering information. The pages required JavaScript enabled.
What is the most suitable framework for this purpose? Thanks for your time.",1,2023-11-22 14:36:51
Scrape Online Stores that Host with Shopify,I want to scrape ecom stores that built on Shopify. Any good ways of doing it? Tools?,1,2023-11-22 08:15:12
How can I get more than 1k posts from a subreddit?,"So I often rely on reddit to get trainingdata for LLMs, but due to api changes I can't. Can I somehow interact with reddit through the web and that way bypass this?",4,2023-11-21 22:50:43
"Python error with praw ""this comment does not appear to be in the comment tree""","I wrote a python script that scraped a post I made  last night here: https://www.reddit.com/r/aoe2/comments/1804c5e/are_all_aoe2_players_old_post_you_age_and_career/
I was able to extract all comments. Today (just now) I tried rerunning and see every comment states: ""this comment does not appear to be in the comment tree"". Any help would be great.
I'm using praw, and it's literally one line of code:
submission = reddit.submission(id=post_id)",2,2023-11-21 23:52:47
Is it possible,To scrape a mockup file from a site like printify. Basically i am just wanting to get the psd file of the mockup for products so i can use it in photoshop and not their site. Easier for bulk creation. I have been messing around scraping images and fonts but have no clue if this is even possible. One would assume the file has to be some where if you upload your image and it updates to mockup. Might be a stupid thought but worth a try .,1,2023-11-21 23:38:31
Firebase Functions to Google Cloud Run,"Hi. I'm a junior dev. I build an app that uses Firebase Functions, but I want to move it over to Google Cloud Run with Google Cloud Tasks. 
The user base is exceeding what can be processed by a single Puppeteer instance. I need to be able to load each of the tasks into a queue and then have workers scale to match the size of the queue. I know what I need to do, but have no idea where to start. Perhaps someone can offer some advice on where to start?",1,2023-11-21 22:29:43
It morphed into a testing framework while I wasn't looking,"While I've built a number of commercial and open source scrapers over the years, culminating with Automaton, I never really wanted to commercialize it and never really thought more about it except when scraping something for some other project.
A few years back I built a browser test runner to allow you to run specific tests within an otherwise idiomatic mocha suite in a browser. It was for a specific project, but I always kept that capability in the back of my mind...
Then earlier this year when native browser esmodules became viable, I realized I could repurpose automaton as a generalized test runner using the scraper engines to power it.
I call it moka and it's working really well for me:
Some example test suites(all these run in node + all browsers, headless):
- @environment-safe/canvas - a fully uniform canvas that supports node + the browser
- extended-emitter - an emitter with subscription criteria
- array-events - simple array utilities
- image-booth - compares pixel outputs from image operations to canonical output
- @environment-safe/authentication - client + server auth, with a simple example of fixtures
If you have any need for testing native esmodules, check it out!
I'd love to hear any input: good or bad.",2,2023-11-21 17:34:12
HELP with first scraper,"Hello guys,
I'm making a scraper for this website: base.gov.pt
The problem is that after some time the resources are not being loaded (js, css, etc). I already added a delay between requests and im using a free proxy rotation service.
What can i do? I need to fetch some entries and after some time im getting i assume blocked by them",1,2023-11-21 16:52:21
Looking for PTO (Paid Time Off) data,"hey guys,
looking for PTO data, companies PTO policies, anything related to PTO.
PAID/FREE hit me up",0,2023-11-21 13:44:26
selenium - 'Proxy' object has no attribute 'add_to_capabilities',"im trying to connect to a site with selenium trough proxy. i found a solution which seems to work with my proxy but im getting an error. What im doing wrong?
from selenium import webdriver
from selenium.webdriver.common.proxy import Proxy, ProxyType
# Proxy-Settings
prox = Proxy()
prox.proxy_type = ProxyType.MANUAL
prox.http_proxy = ""http://my.proxy.de:8080""
prox.socks_proxy = ""http://my.proxy.de:8080""
prox.ssl_proxy = ""http://my.proxy.dede:8080""
capabilities = webdriver.DesiredCapabilities.CHROME
prox.add_to_capabilities(capabilities)",1,2023-11-21 10:09:14
Best solution for scraping lecture archive for academic research?,"So there is a web archive online that hosts several thousand plain text lectures and books. The text of each lecture is served up on a separate page but that text is surrounded by banners and side-bar text that I don't want to capture.
Interestingly enough,  there is a 'print/pdf' button off to the right on each lecture page and when you click there it shows just the lecture text to print. This functionality makes me think that there must be a way to scrape the entire archive and neatly save each lecture into it's own PDF automatically, rather than me having to do this by hand 3000+ times.
The catch is, I'm not a coder and I don't know what tool I could use to pull this off. Since it is a very simple website (3000+ text files served up via HTML, one file at a time), perhaps configuring a command-prompt based tool like scrapy wouldn't be the hardest thing? No clue what I'm doing here :)
The website I'm looking to archive is rsarchive.org. I'm looking to download all the lectures and books hosted there so I can build a GPT and then query the archive that way, using it as a super-charged re-search tool.
Thanks!
â€‹",2,2023-11-21 02:06:43
Prettifying BS4 output after removing some tags,"So I have been trying to clean a web page to extract certain things on a HTML page and generate a clean text to train a ML model.
I decompose certain tags that I do not want in my current plain text such as XBRL tags and I remove all stylings and horizontal rules. I have a reference document with which I want to get as close as possible.
But what ever cleaning methods I employ using Beautiful Soup I get almost bad results with the orientation and the placement of texts completely getting destroyed.
This is the current cleaning code that I am using. Please let me know if there are better ways to get the text of a cleaned HTML.
analysisPath =  ""../extracts/und-analysis/""
os.makedirs(analysisPath,exist_ok = True)


for filename in tqdm.tqdm(glob.glob(extractPath)):

    content = None
    with open(filename) as file:
        content = file.read()

    soup = BeautifulSoup(content, 'html.parser')

    testFileName =os.path.basename(filename)
    baseName = testFileName.split('.')[0]

    # print(baseName)


    with open(analysisPath+'/{}-original'.format(baseName), 'w') as f:
        f.write(str(soup.prettify()))


#     for tag in soup():
#         for attribute in [""class"", ""id"", ""name"", ""style""]:
#             del tag[attribute]

    for tag in soup():
        if (tag.name) == ""hr"":
            tag.decompose()

        if fuzz.ratio(tag.text.lower() , ""table of contents"") >= 92:
            tag.clear()

        if tag.name in xbrl_tags:
            tag.clear()

        # if (tag.name) == ""table"":
        #     tag.decompose()

    soup.prettify()
    soup.smooth()


    with open(analysisPath+'/{}-html'.format(baseName), 'w') as f:
        #print(soup.prettify())
        f.write(str(soup))

    with open(analysisPath + ""/{}-text"".format(baseName.replace('html', 'txt')), 'w',encoding='utf-8') as file:
        file.write(str(soup.get_text('\n')))

Here is the Reference file that I am trying to get close to https://gist.github.com/shivanshvermaa/95cf93649dd464753030cd2cab6a36d4
Here is the original html file https://gist.github.com/shivanshvermaa/88cbadf6f53ab0fbe5ebf8bbdb1914b4
Here is the text file that I get after cleaning up the code https://gist.github.com/shivanshvermaa/50301a1892ebf319fec8fdbed18fd7c6",1,2023-11-21 03:04:12
Recent trouble getting responses with python requests/postman,"I'm working on a project that scrapes many websites and it was running smoothly until recently.
I use Python requests and I run requests through a proxy. Starting last month, 3 out of the 10 (seemingly unrelated) websites I scrape are not sending back responses. I usually debug this with Postman and I'm not getting any response there either. Through a browser it still works, but then if I ""copy the request as cURL"" into Postman it doesn't work.
I tried multiple things and I found that ""copy as Powershell"" and running the script in terminal gets the response I want. I've combed through the parameters and cookies of both the python and powershell scripts and I could not find a difference.
What could be making my requests fail?
Here is an example of page I try to scrape:  https://www.iwc.com/ch/en/watches.html",2,2023-11-20 14:45:06
Categorize scraped products by using their title and/or description,"What Iâ€™m building
Iâ€™m making a project that uses data from different webshops to show the best prices of PokÃ©mon products. Iâ€™m working with Nuxt 3 as my framework, and a mongo database with all the data for my products. Now what Iâ€™m trying to add is some kind of way to categorize these products by 3 properties: â€˜Categoryâ€™, â€˜Serieâ€™, and â€˜Expansionâ€™ without having these properties available through the webshops.
Problem summary
Different webshops all use a slightly different name/title and description for the same products. Also most of the time they donâ€™t have their products nicely categorized.
Examples
Here are 2 examples of data I have, and what Iâ€™m trying to get from them.
// Input:
{
    title: ""Pokemon Sword & Shield Darkness Ablaze Sleeved Booster Pack"",
    description: """", //Will not alway be there
}

//Output:
{
    category: ""Booster Packs"",
    serie: ""Sword & Shield"", 
    expansion: ""Darkness Ablaze"",
}

// Input:
{
    title: ""Pokemon Charizard EX Premium"",
    description: ""Charizard ex Collection Box vernietigt de voorsprong van de tegenstander met de sluwe vuurkracht van een Darkness-type PokÃ©mon! ... etc etc."",
}

//Output:
{
    category: ""Collection Box"", //Got from description
    serie: ""Other"", //No data
    expansion: ""Other"", //No data
}

Things I tried:

Just a simple string matching approach. So I made some arrays with categories, series, and expansions. Then just to some stuff with includes(). Got it right sometimes, wrong some times. Might be the way to go, but looking for something better for now.
I was now playing around with some NLP libraries. Like with Natural I tried to use this BayesianCalssifier()(See here) and then train it with the categories, series, and expansions. Same as above, sometimes works, sometimes no.

This is the structure of my data I was using:
const categories = [
  ""Booster Pack"",
  ""Booster Box"",
  ""Theme Deck"",
  ""Elite Trainer Box"",
  ""Special Box"",
  ""Collection Box"",
  ""Tin"",
  ""Blister Pack"",
  ""Other"",
];

const pokemonTCG = [
  {
    serie: ""EX Series"",
    expansions: [
      ""Ruby & Sapphire"",
      ""Sandstorm"",
      ""Dragon"",
      ""Team Magma vs Team Aqua"",
      ""Hidden Legends"",
      // etc..
    ],
  },
  //etc..

Some problems I ran in to with above things:

The first example I gave has this title: â€œPokemon Sword & Shield Darkness Ablaze Sleeved Booster Packâ€. The problem here is that â€œSword & Shieldâ€ is the series, but â€œSword & Shieldâ€ is also an expansion from that series. So with the things I tried I never got â€œDarkness Ablazeâ€ back as the expansion.

Some other ideas I had:

Chat-GPT can do this really well it seems, but prefer first to try to get this to work for free.
Train my own model. Not sure if I want to get into that.
Manually add them.

What Iâ€™m looking for
Mainly some ideas on things to try to make this work. Is some NLP library the way to go here and Iâ€™m just not looking for the right functions? Off course if someone has been where Iâ€™m currently and figured it out, I would love to know how. Or maybe should I not try to do this at all?",1,2023-11-20 16:21:16
Getting Blocked while Scraping - Site denied session cookies,"Greetings!
I wish to extract airfare prices from a specific airline called 'AZUL.'
However, when I access this website using a browser through Selenium, Playwright, or Puppeteer, I am blocked from accessing the airfares. The website treats the session cookie from the browser as a robot and denies access, resulting in a 404 error.
In this regard, how can I avoid being detected as a robot by the website and access its prices normally?
Here is the URL of the website in question: https://www.voeazul.com.br/br/pt/home 
Thank you very much for your help!",1,2023-11-20 12:59:06
Project,"Hello all,
I need a script, that scrapes the Goat website for a specific model and in specific sizes and when a new product is added to the website in that criteria, it sends me a message with size, price, etc via discord webhook
If anyone can help me code this or can code this for me (paid) then let me know!",0,2023-11-19 22:24:26
Alternative to this site or tips on scraping?,"I'm trying to scrape this website, but because of how it is designed, I need to do a workaround by loading it in a virtual browser before scraping it, this method however is very slow and does not have a 100% success rate per page being scraped... Is there an easier way to scrape this page or is there a different website with the same information?
https://www.complex.management/legal/documents/STSMA-S:1
This is the code I am talking about for reference (to load the page html)  
def get_site_html(url: str, wait_time: int = 60):
    print(""Getting"", url)
    # Start the Chrome driver
    driver = webdriver.Chrome(options=chrome_options)

    # Load the website
    # https://www.complex.management/legal?code=my_code&state=my_state
    driver.get(url)

    time.sleep(wait_time)

    # Wait for the website to finish loading
    driver.implicitly_wait(wait_time)  # Wait for x seconds

    # Get the page source
    page_source = driver.page_source

    # Close the browser
    driver.quit()

    return page_source

â€‹",2,2023-11-19 15:30:59
PDFs from free charting website,"Hi, I'm looking for a way to download PDFs from the website chartfox.org (free to use and download site for flight sim charts). 
I would like to store the files locally for easy access when flying without having to download and save each chart in the correct folder.. is this possible?
Thanks",1,2023-11-19 12:39:12
Getting contact information for a traditional mailing effort,"Good day, I want to offer a video service to my local 50 mile radius of Churches. The District Conference has a website where you can 'search for a Church' and a result will come back for your given zipcode. I only have 2 zipcodes to enter so the return will be two separate queries.
The results come back in 3 or 4 lines with either an email address or POC as the 4th line of the address. Usually the display will be 10 results per page and the pages can be up to 3 for the results.
I've coded as a SysAdmin in the past with Python for various taskings, but not too much in the Web arena. I'm aware of JSON but very seldomly had to parse it for results, our DevOps folks did that.
The website does not have an API that I am aware of, all the information is public facing.
My ultimate goal is to get the results and create a Database of the Church addresses, phones, POCs and email addresses.
I have access to Windows 11, MacOs and Linux PopOS and am comfortable with each. 
Grace and Peace. 
Edit: added OS options.
â€‹",1,2023-11-19 10:13:16
LinkedIn Automation,How to avoid being blocked while automating LinkedIn using selenium python? I've tried lot of techniques but nothing worked :(,2,2023-11-17 20:02:44
Teach me How to scrap data from I particular website,"I am trying to find ways to scrap data from a website but Fails every time.
Can anyone help
website : https://damangames.in/#/login",0,2023-11-17 17:31:44
Scraping facebook events list,"Hi, for my project, I would like to scrap events within my city. Is there any solution to do so? I created a few bots to do so, but I'm always struggling with being not logged in or fb cutting down results.",1,2023-11-17 08:00:13
Are there Python libraries for working with HTML ruby tags?,"There are so many ways to use <ruby> tags to annotate text and provide reading data.
Is there a library that helps me extract that information? For example as a pair of text and reading or something like that.
Even just plain old flattening without the reading hints would be good enough.",3,2023-11-17 00:29:15
Browser/proxy providers and pricing,"As I near a soft-launch of my browser as a service I have taken note that all of my competitors price their usage based on various credit systems, i.e. for $x/mo you receive 250K ""credits"" on the service. What a credit gets you depends on what you need for an individual request. For example, if just need a plain HTTP request it might be one credit, but if you need a JS render with a proxy you'll use 10. Others charge one credit per 30s of time used with additional credits consumed if you require a proxy.
For those that use these services, how does that work out for you?  I can see the usefulness of such a system in that you have one pool of resources that can be used in a variety of ways. Whether you just need unproxied non-JS renders or full proxy with JS you use the same system. On the flip side I wonder if this is viewed in any negative light amongst consumers in that it's not entirely clear how far your credits will get you and it's possible to incur overages.
I had in mind an alternative that I wanted to run by you all:
Unlimited usage, pay by concurrency 
In this system I offer unlimited usage based on concurrency. If you just need a small amount of concurrency to complete all of the scraping you need, you pay for one worker. If you have a lot to get done in the month you pay for the additional concurrency needed to complete the required tasks. It's still a scaling system in that you pay only for what you need, but it's more transparent about what you're getting and there's no possibility of overages or billing surprises. 
For proxies I was thinking this would be an add-on for a worker; a non-proxy worker costs $x/mo but a proxy enabled one adds a proxy surcharge. 
The advantage of this system is that you can use the service without concern of incurring extra charges. 
One disadvantage is you're more tied into running your requests continuously in order to take advantage of the unlimited usage, i.e. you cannot just complete all your requests at once, you have to spread them out over the month. In some way this might be a good strategy to prevent being blocked as the traffic being more steady is less likely to trip detection, but it of course depends on use case.
----
My last thread was very insightful so I look forward to hearing any feedback on this concept. Please feel free to shoot it down entirely if it's not helpful. I'm not tied to any particular system and am trying to find the best fit.",1,2023-11-17 04:12:29
Scraping Amazon Seller Central,"I need to grab some data from my own Amazon Seller Central account. The problem is that it's report data that is not available via the API. Again, this is my own account.
Anyone know how to do this easily? Ideally from Python. Thank you in advance.",1,2023-11-17 02:51:00
How to scrape plain-text over a modal,"We'd like to scrape an online directory https://annuaire.experts-comptables.org/recherche?localite=&adresse=&insee=&type_localite=&comptable=&langue=&type_cabinet=SEC&departmentCode=). The data is easily collected via the Web Scraper extension, but the prospects' phone numbers are only accessible via a modal that has to be opened first.
â€‹
I can't scrap the plain text once the modal has been opened. Has anyone ever had this problem, and if so, how did they solve it?",2,2023-11-16 13:14:16
Extract annual turnover from sec reports in xbrl format,"How to extract the annual revenue/turnover from ""ifrs-full"" (https://data.sec.gov/api/xbrl/companyfacts/CIK0001835268.json) and a ""us-gaap"" (https://data.sec.gov/api/xbrl/companyfacts/CIK0001835236.json) json object? There is so much information that I dont understand were do i get the annual turnover. Can anyone help?",1,2023-11-16 12:34:00
Want to scrape Discord server list,"Thereâ€™s approximately 30000 servers here that Iâ€™d like to scrape into one document/database and be able to filter based on size and active users. Is this possible? How would I go about doing this? 
Hereâ€™s the link: https://discord.com/servers/undefined",0,2023-11-16 07:59:28
"Product hidden data (Tags, ID, SKU) Scraper for e commerce","Hi, I'm new to this community and appreciate your patience. 
I'm in the process of setting up an e-commerce store specializing in hardware products. My goal is to compile a comprehensive product sheet with details like SKU numbers, IDs, tags, etc., which I can then easily import into my WooCommerce site.
I'm curious if there's a feasible method to develop a scraper that could automatically populate this sheet with the necessary data from a given product link. 
I'm aware of the possibility of writing my own code or using a service like Barden to scrape visible information like prices or product names. 
However, what I'm particularly interested in is extracting the ""hidden"" data from competing e-commerce sites. I'm looking to replicate the same ID and SKU numbers so that if someone searches these on Google, there's a higher chance my store will appear in the search results. Additionally, using the same tags as larger, more SEO-savvy companies seems like a strategic move. ðŸ˜„",3,2023-11-15 23:58:16
Standardizing data for web scraping,"Hi I have been working on a pc parts list scraper for a while now. I just want to ask some tips regarding scraping the specifications of pc parts. I am scraping the pc parts list from different e-commerce store and some items are just the same with each other there are just some data that is different like the price. Some data that are similar are structured differently like in one store the name of the product is ""AMD Ryzenâ„¢ 5 5600X Desktop Processors (Up to 4.6GHz)"" and in the other store ""AMD RYZEN 5 5600X MPK (AM4) WITHOUT COOLER (TRAY TYPE)"" but this is just the same item. I am asking because I don't want to do a custom code for scraping the specifications for each e-commerce store. Thank you.",2,2023-11-16 01:31:49
How can I collect these infected URLs?,"Many government websites from my country (ending with gov.br) were invaded and are promoting gambling websites illegaly. I can find many of them on google by typing keywords related to online gambling: https://imgur.com/a/IiRVkxo
Upon clicking, some of the links open directly under the legimate government domain. Others are more sophiscated and redirect to external domains using javascript - this only happens if the referer of the request is a search engine, otherwise the URL will return nothing. This is a common survival tactic for malwares: https://imgur.com/a/BDY6kvs 
What I know about the infected URLs:

Their domains end with ""gov.br"".
Their content preview on google search contain keywords related to online gambling.

I would like to collect these URLs and create a list with them, for research purposes. What's the best way to automate this process?",3,2023-11-15 23:12:39
how crawling large data that spend less time,"Hello, it's me again. In my previous post, I didn't have a clear question, and now...
So, I am crawling users from Twitter X, with information about name, location, joining time, followers, following, and also posts. I am using Selenium along with BeautifulSoup (without API). However, I've only managed to crawl a very small amount of data (only 1MB) while running for hours, taking a lot of time. Can someone guide me on how to crawl a larger amount of data (I am required to crawl at least 1GB of information)?
Thanks for reading and have a good day!",0,2023-11-16 05:00:41
How to Webscrape a website that poisons their data?,Are there any ways to scrape a website that makes their data hard to get or impossible to read?,5,2023-11-15 17:41:54
LinkedIn post scraping,"I run a small design business, and with the insane amounts of applicants on contract roles nowadays I am struggling to get leads through LinkedIn job postings.
Iâ€™m having more success now searching posts of mentions of contract design or freelance design. Howeverâ€¦. This takes ages. 
Anyone successfully figured out how to scrape LinkedIn posts?",1,2023-11-16 01:38:54
A quick question how to get start ups funding information by stages?,"I can think of two ways
Hard scrap crunchbase data,  which I am not sure is legal
Or use a scrapping agent to collect and parse news?
Curious to hear peopleâ€™s take",0,2023-11-16 00:21:05
How to make Fake accounts on Meta's threads for web scraping?,"I want to web scrap threads. But I needs accounts to get inside threads web app. 
Do someone know how can I make fake accounts and scrap it?
Or any other alternative?",0,2023-11-15 18:10:49
What's the most challenging aspect of web scraping today?,"I have a site that requires me to scrape data from 5,000 websites daily. Each one requires a long script to change the date 10-14 times and then wait for DOM updates. It was complex to set up and there weren't (at the time) any affordable options for hosting the crawler side of it in the cloud. I ended up doing a self-hosted setup with puppeteer.
I've been considering breaking into the SaaS space using what I've learned. In order to do so I'd like to know what are the hardest problems so I can chase those down. I have my own thoughts about what they might be, but I'm curious if others have a thoughts I should be looking into.
Top of mind for me:

storing output and building an API to retrieve scraping results for later use
alerting and responding to DOM changes before data becomes out of date
getting blocked (seems somewhat solved by proxying, although expensive)

â€‹
â€‹
â€‹
â€‹",14,2023-11-14 20:29:56
AI Assist chatbot for E-store Website,"FYP - An AI-assisted chatbot to enhance the customer service provided in an estore.  This AI assisted chatbot should be able to have conversations with the customers accurately, human hand off when complex queries are asked by the customers and this AI assisted chatbot should be able to read and Analyse the front end data of the Web application of the estore and provide recommendations and other details for the customers from the read and Analysed data on the Web application in real time.
â€‹
Guys, I need help to make this project. If you have ideas, please leave a comment",0,2023-11-15 06:37:45
what web scraping project of yours would benefit from massive parallelism?,"Hey All, 
Does anyone have a web scraping project that would benefit from a massive amount of parallelism? 
I'm trying to build a dev tool for python developers to easily parallelize their code. I want to get some project ideas so I can build useful tutorials especially for web scraping. I have a couple of ideas (scraping github for emails & government contract sites) but want to open it up to the community before I decide where I should focus my time. Please drop any project/tutorial ideas and if you think someone's idea is good please upvote them (so I know you think it would be valuable).",1,2023-11-15 04:20:23
Help with saving images,"I have a script that can download the images I want, however, I can't figure out how to save the images with a file name that corresponds to the data in its div.
I'm using this site's script: https://github.com/oxylabs/scrape-images-from-website
But it is generating a filename like this:  filename = hashlib.sha1(image_content).hexdigest()[:10] + "".png""
Here is the code:
import io
import pathlib
import hashlib
import pandas as pd
import requests
from bs4 import BeautifulSoup
from PIL import Image
from selenium import webdriver  
def get_content_from_url(url):
   driver = webdriver.Chrome()  # add ""executable_path="" if driver not in running directory
 driver.get(url)
   driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
   page_content = driver.page_source
   driver.quit()  # We do not need the browser instance for further steps.
 return page_content  
def parse_image_urls(content, classes, location, source):
   soup = BeautifulSoup(content)
   results = []
 for a in soup.findAll(attrs={""class"": classes}):
name = a.find(location)
 if name not in results:
results.append(name.get(source))
 return results  
def save_urls_to_csv(image_urls):
   df = pd.DataFrame({""links"": image_urls})
   df.to_csv(""links.csv"", index=False, encoding=""utf-8"")  
def get_and_save_image_to_file(image_url, output_dir):
   response = requests.get(image_url, headers={""User-agent"": ""Mozilla/5.0""})
   image_content = response.content
   image_file = io.BytesIO(image_content)
   image = Image.open(image_file).convert(""RGB"")
   filename = hashlib.sha1(image_content).hexdigest()[:10] + "".png""
 file_path = output_dir / filename
   image.save(file_path, ""PNG"", quality=80)  
def main():
   url = ""https://your.url/here?yes=brilliant""
 content = get_content_from_url(url)
   image_urls = parse_image_urls(
 content=content, classes=""blog-card__link"", location=""img"", source=""src"",
 )
   save_urls_to_csv(image_urls)  
for image_url in image_urls:
get_and_save_image_to_file(
image_url, output_dir=pathlib.Path(""nix/path/to/test""),
 )  
if __name__ == ""__main__"":  #only executes if imported as main file
 main()
â€‹",2,2023-11-14 22:19:18
(apple silicon) Why can I use a non-arm64 chromedriver with an arm64 chrome binary?,"and what are the repercussions of continuing to do so?
for context I am doing some functional and performance testing",2,2023-11-14 20:53:55
Is there a working solution to scrape Wayback Machine?,"I'm working on re-establishing a website for a new client who lost a server and didn't have backups. After a bunch of poking around hard drives, the only thing we have in terms of content is access to indexes from Wayback Machine, which is enough to recover most of what we need. At minimum, it would be nice to get just the text from these pages into a document tool of some sort (excel, doc, etc.) so I could import back into their new platform.
I've tried the following method via Python and have had no luck getting results, with errors on the get request. My knowledge of python is limited to what I was able to learn in a few hours yesterday ðŸ˜‚
https://github.com/sangaline/wayback-machine-scraper 
Does anyone have a solution that's even slightly automated? We're talking 250+ pages/articles, or I would just grab it manually. Even a browser plugin that requires some manual input is fine by me, but I don't do web scraping and don't know where to start.
Thanks in advance for any help!",3,2023-11-14 17:18:22
Scraping Stake Casino,"So I am trying to make a betting bot on stake casino. I know there is an API but there is no documentation and I do not know if its functions will help. So I am using a web scraper.
The problem is that when I open the website it knows its a bot so it doesnÂ´t let you log in so you can start betting. Does anyone know a way around it? I am new to web scraping any help would be appreciated.
Thank you
â€‹",1,2023-11-14 16:44:07
Whatâ€™s the most amount of data youâ€™ve scraped?,And what was it?,7,2023-11-14 01:35:18
Best way to scrap users emails posts followers from tik tok,Thanks ðŸ‘,0,2023-11-14 09:24:39
Scrape ratings per hour,Hello! Is there any way to get a restaurantâ€™s ratings by each hour of the day? Like to see what gives high ratings morning or night. I want to scrape some data to analyze the restaurant but I canâ€™t figure out how to get that. Is there any way to get it?,2,2023-11-13 22:40:15
Ways to scrape Amazon results,"Can anyone guide me or something on how to scrape search results from Amazon, I only need the top 20 results and their price.",1,2023-11-13 15:50:58
Automating scraping data from internet using keywords.,"Hello Everyone!
Is there anyway to scrape data from Internet using different Keywords?
Let's say I want data of all the Restaurants in  United States but it should be filtered out in states, city, rating, timings etc.
How can I do this because manually doing this will take a lot of efforts and time.",2,2023-11-12 23:12:25
Could not scrap Reddit beyond a certain limit after which older posts won't get listed.,"The title pretty much sums it up. I used Selenium for the task and when I came back to see how much work it had done, after about 1000 posts (don't know the exact number as I only kept track of posts that satisfied certain criteria), new posts simply wouldn't load, as if the bottom of the subreddit had been reached, even though the subreddit is much older than the last post there.
Has anyone else encountered this, and if so, is it possible to scrap beyond that limit?
UPDATE:
I have also tried scrapping old.reddit afterwards, but reached the limit even earlier at 347 matching images (26 pages).
https://old.reddit.com/r/AnimeGirls/?count=625&after=t3_13b32d0
You can notice that here there is no [next] button on the bottom, only [prev]",4,2023-11-12 12:41:48
how to crawl 5Gb data from reddit,"Hello there!
I have a question about crawling text data from subreddit, but how to up to 5Gb. 
Help me, any idea? I am a beginner for crawling data from 
Thank a lot.",0,2023-11-12 17:51:28
How to Automate the File Download from This Site,"This is the official page to view/download data for National Air Quality Index - India. 
Here, we choose the date and hour, and a clickable link directly downloads a spreadsheet with data for that time-stamp.
Need ideas on how to automate that file download process in Python, so that we can download daywise and hourly data by specifying the date/hour range in the code.
Thanks in advance!",2,2023-11-12 10:08:54
Scraping LinkedIn legally as a smaller company,"Hi everyone, hope you're all having an amazing day. Recently I started working on my own SaaS company which helps salespeople prepare for their calls etc, I won't bother you with all the details.  
Basically, my question is, is there any way I can allow my users to legally scrape the profiles of other LinkedIn users? Via maybe LinkedIn API, or allowing them to log in to their profile, etc. I'm fine with the 50 profiles a day limit as my users would only do maybe like max of 20 profiles a day.  
Or is there maybe any other subreddit that could answer my question, where they maybe know more about all the laws, and legality when it comes to scraping data.  
Thanks a lot of any information or help!",5,2023-11-11 22:44:20
GPT-4 vision utilities to enable web browsing,"Wanted to share our work on Tarsier here, an open source utility library that enables LLMs like GPT-4 and GPT-4 Vision to browse the web. The library helps answer the following questions:

How do you map LLM responses back into web elements?
How can you mark up a page for an LLM to better understand its action space?
How do you feed a ""screenshot"" to a text-only LLM?

We do this by tagging ""interactable"" elements on the page with an ID, enabling the LLM to connect actions to an ID which we can then translate back into web elements. We also use OCR to translate a page screenshot to a spatially encoded text string such that even a text only LLM can understand how to navigate the page.
View a demo and read more on GitHub: https://github.com/reworkd/tarsier",5,2023-11-11 21:24:56
Free GUI web scraping tool,"Hello,
Is there any free (and hopefully open-source) web scraping GUI?
Best regards.",3,2023-11-11 21:32:47
Use Selenium in Google Colab Notebooks,"Just made this cool wrapper around Selenium (ChromeDriver) that allows you to easily, with just a few lines of code, run Selenium in a Google Colab Notebook!
https://github.com/jpjacobpadilla/Google-Colab-Selenium",13,2023-11-11 06:38:48
MQTT Web Scraping: Payload Challenge,"Hello,   
I'm currently working on scraping data from the MQTT protocol. While I've successfully established a connection, I'm facing challenges when it comes to sending the payload to receive the data. Upon inspecting the sent requests in Firefox network tab, I noticed some unusual characters in the payload. Could someone guide me on how to obtain the correct payload so I can send the request accurately?",1,2023-11-11 13:02:23
Need Advise from the community for an AI/WS Project,"Hello everyone!
So this is my first post on the community, so I'm sorry in advance if this kind of post is not either allowed or encouraged.
Anyway, I'm building a project (kinda of a pet project for fun) that I might make available for free if I'm able to implement enough interesting features. (as it involves third-party APIs, I'll probably make it so any user would just need to add his own API keys in it and of course I'll open source it so everything is transparent).
About the Project:
The project (MVP version) is basically as follow (in term of flow):
1) Log in (OAuth2).
2) Add the needed API Keys in the profile.
3)a) Subscribe to a list of websites (Mainly ""News"" or ""Engineering Blogs"") either from the existing ones or add a new one.
3)b) Subscribe to a Topic (something like ""stocks"", ""databases"", etc..).
4) At the end of each Day/Week (Depending on user preferences), be alerted (Through Email in MVP and then in other channels in future versions) on the new articles that have been added to the subscribed sources (the websites).
5) Get a compilation (on the alert email for example) of summaries (using OpenAI's APIs for now) from all the subscribed sources new articles (with links to the original articles at the bottom of each summary). That could also be something like a summary per website or per topic of interest (still thinking about it).
6) New features that I didn't thought about yet.. (Stuff like make a Read-ONLY mode of the scraped content, categorise through AI/LLMs the articles topic among existing ones etc..).
About Web-scraping:
So I've already implemented the ""Summarise one article given its URL"" (I'm scrapping the article's source using a very basic approach with JSOUP and getting all the ""post"", ""p"", ""h1, etc.. HTML elements that could contain anything relevant to be used for the summary.
However, I'm actually struggling to define the approach that I will take to kinda ""observe"" the different websites (I guess scrap each sources daily/hourly(?) to discover any new item) and then for each one scrap/parse the articles to pull the content and then summarise through LLMs APIs.
I'm also not sure if I'd need a headless browser approach here and if I should:
- build everything manually (relying on open-source libraries at the most).
- use OS frameworks like amerkurev/scrapper's one on github (or anything similar as I'm not aware of the SOA in WS).
- use APIs (and give the user to put his own API key of course) like scrapingant.
â€‹
Anyway, I'm trying to define an approach that would scale (as much as possible as I'm not aiming for a big tech product kinda scale but robust enough to not fail once number of users starts to grow) with enough complexity to be able to support most of the website that users would want to follow/observe (I guess News/Finance/Engineering blogs/etc..).
Any idea, suggestion, opinion on the above is very much welcomed and needed !
â€‹
BTW, I'm building the project using Java/SpringBoot but as I'm using a micro-service approach so It would be able to code the ""observer"" / ""scraper"" components in other languages if you guys think of something more suitable/efficient with something else than Java.
â€‹
Thank you again!",1,2023-11-11 12:26:44
How to start in web scraping journey?,"Hey there, folks!
I'm excited to dive into web scraping, and I could really use your help. I'm pretty comfortable with computer programming - I can handle languages like C#, JavaScript, Python, Java, HTML, CSS, and SQL, and I'm comfy with Linux too (not necessary).
Actually, I've played around with web scraping in a personal project before, but back then, I did know what I was doing; the thing is, I didn't realize that what I was doing was actually web scraping. I managed to grab URLs and tool installation commands from the Kali Linux website using Python and Beautiful Soup (BS4), though.
The cool thing is, as I've been programming, I've realized I really enjoy working with data. So, I want to get serious about delving deeper into the world of data, especially through web scraping. Your advice and tips would be super helpful on this exciting journey!",2,2023-11-11 08:14:29
User agents? Any resources?,"Hey all. Scraping an image board, and my script is working well until I use: 
wget.download()
I get back a ""FORBIDDEN"" response.
I'm reading that changing my user agent would probably help, but having trouble getting that working. Anybody have a good resource that can get me up to speed? It seems like there's a lot out there, particularly for Java.
I'm using Python, Selenium, Beautiful Soup and Requests.
Thanks!
EDIT: I'm very new to Scraping. If you have any YouTube teachers or resources you'd recommend outside of this, I'm all ears!",3,2023-11-10 23:42:23
Scrape Linkedin connections and display in a web app?,"Hey y'all!
Have an app in bubble.io and need to show user their LinkedIn connections list. Trying do it like https://leaddelta.com/ or https://ulinc.co/
Usecase: User Logs in with LinkedIn and sees all his 1st connections in a simple web app.  
Seems Linkedin doesn't give access to its' Connections API, so Perhaps there are any extensions/scrapers or easy solutions that would allow to do it by connecting to my app?",1,2023-11-11 02:49:41
Scraping 144 pages of Bing results.,"How can I scrape the URLs of 144 pages worth of Bing search results? If your wondering what Iâ€™m doing with them, Iâ€™m archiving the indexed documents and webpages on Discordâ€™s CDN. Discord is forcing a change on all users to have all CDN links expire by the end of this year, and this will break MANY random links online.",2,2023-11-10 20:06:13
Open source evaluations for AI Agents in web tasks,"Recently created Banana-lyzer, an open source AI Agent evaluation framework and dataset for web tasks with Playwright (And has a banana theme because why not) and would love to get feedback/support. There are a few issues with existing evals repos:

Websites change overtime, are affected by latency, and may have anti bot protections. We need a system that can reliably save and deploy historic/static snapshots of websites.
Standard web practices are loose and there is an abundance of different underlying ways to represent a single individual website. For an agent to best generalize, we require building a diverse dataset of websites across industries and use-cases.
We have specific evaluation criteria and agent use cases focusing on structured and direct information retrieval across websites.
There exists valuable web task datasets and evaluations that we'd like to unify in a single repo (Mind2Web, WebArena, etc).

Read more here: https://github.com/reworkd/bananalyzer",2,2023-11-10 18:15:35
Selenium script to scrape Microsoft Teams chat history?,"Does anyone here have a script that will scrape all of your personal thread history? My company is putting a policy in place in a week that will purge all chat history older than 90 days.
I would use an API, but everything is locked down.",1,2023-11-10 18:46:05
Is it possible to scrap HTML Data/code and saving it..,"I want to scrap the data from this page 
https://shop.mitutoyo.eu/web/mitutoyo/en/mitutoyo/01.02.01.041/Digital%20Microm.%2C%20Non%20Rotating%20Spindle/$catalogue/mitutoyoData/PR/406-250-30/index.xhtml
Starting from description to the end of mass : 330 g. I want the data to look the same when it is uploaded to my website.. 
Also when i scrap it should save everything in one excel cell.. 
I have tried with my code below but I am not able to get the ""Description and Features"".... 
â€‹
â€‹
import scrapy  
class DigitalmicrometerSpider(scrapy.Spider):
name = ""digitalmicrometer""
 allowed_domains = [""shop.mitutoyo.eu""]
start_urls = [""https://shop.mitutoyo.eu/web/mitutoyo/en/mitutoyo/01.02.01.041/Digimatic%20Micrometers%20with%20Non-Rotating%20Spindle/index.xhtml""\]  
def parse(self, response):
dmicrometer = response.css('td.general')  
for micrometer in dmicrometer:
relative_url = micrometer.css('a.listLink').attrib['href']
 #meter_url = 'https://shop.mitutoyo.eu/web/mitutoyo/en/mitutoyo/01.02.01.041/Digimatic%20Micrometers%20with%20Non-Rotating%20Spindle/index.xhtml' + relative_url
 meter_url = response.urljoin(relative_url)
 yield scrapy.Request(meter_url, callback=self.parse_micrometer)  
#yield {
# 'part_number': micrometer.css('div.articlenumber a::text').get(),
#  'url': micrometer.css('a.listLink').attrib['href'],
# }
#next_page
 next_page = response.css('li.pageSelector_item.pageSelector_next ::attr(href)').get()  
if next_page is not None:
next_page_url = response.urljoin(next_page)
 yield response.follow(next_page_url, callback=self.parse)  
def parse_micrometer(self, response):  
description_header_html = response.css('span.descriptionHeader').get()  #delete this
 description_html = response.css('span.description').get()    #delete this
 product_detail_page_html = response.css('#productDetailPage').get()      #delete this
 concatenated_html = f""{description_header_html} {description_html} {product_detail_page_html}""
 #element_html = response.css('#productDetailPage\\:accform\\:parametersContent').get()
 table_rows = response.css(""table.product_properties tr"")  
yield{  
'name' : response.css('div.name h2::text').get(),
 'shortdescription' : response.css('span.short-description::text').get(),
 'Itemnumber' : response.css('span.value::text').get(),
 'description' : ' '.join(response.css('span.description::text, span.description li::text').getall()),
 'image' : response.css('.product-image img::attr(src)').get(),
 'concatenated_html': concatenated_html, #delete this
#'element_html': element_html,
 }",1,2023-11-10 15:11:58
Amazon and Google reviews,Is it legal to scrape Amazon and Google Reviews if I remove personally identifiable information? I am seeing mixed information online. I thought it should be okay since its publicly available information? I need to be extremely sure because this will go on my portfolio.,1,2023-11-10 12:41:39
Trying to scrape daily closing price with specific date range,"Hello! I'm using jupyter and python 3 (ipykernel) to scrape daily closing prices from this site:
https://ca.investing.com/equities/3m-co-historical-data
I need to save the daily closing price for all of 2022, but the site automatically displays 2023 data and uses inputs to change the time frame. I can't figure out how to pull from the new dates. This is my code so far:
companyurla = 'https://ca.investing.com'+'/equities/3m-co'+""-historical-data""
headera = {'User-Agent': 'Mozilla/5.0'}
requesta = Request(companyurla, headers=headera)
companypagesa = urlopen(requesta)
companysoupa = BeautifulSoup(companypagesa, ""html.parser"")
tablea = companysoupa.find(""table"", {""class"":""w-full text-xs leading-4 overflow-x-auto freeze-column-w-1""})
tablebodya = tablea.find('tbody')
companya = pd.DataFrame([row.find_all('td')[0].text.strip() for row in tablebodya.find_all(""tr"")[:365]], columns=['Date'])
companya
Libraries:
from bs4 import BeautifulSoup
import urllib, os, urllib.request, requests
from urllib.request import Request, urlopen
Please help me!!",3,2023-11-09 16:21:37
Headless returns different results,"Background : Iâ€™ve been pulling json data via a 3rd party API for years using php/curl and itâ€™s stopped working. When I turn on verbose I can see itâ€™s failing due to HTTP/2 stream not closed properly - internal
Error.
The odd thing is if I just call the link with chrome the JSON appears.. so I thought Iâ€™d go headless to get the data.. but when I use the exact same chrome browser but with â€”headless (and optionally no gpu) it returns nothing, just like when I call with Curl.
So the question is..  whatâ€™s different? Does the non headless version show something thatâ€™s not picked up/ returned by directing the headless output to a file? Am I missing a setting? The API is used via jquery on the .red party website and seems to work fine there",2,2023-11-09 18:32:39
Detect if there are new products available on a website,"Hi everybody!
I am developing a Node.js app that utilizes web scraping to retrieve products published today from multiple websites. The idea is to run this script once or twice a day using an Azure function and receive notifications about these new products. The problem is that I am scraping the entire list of products from each website because they don't have a filter that shows only the ""published today"" ones. As a result, I am getting around 50 products per website when I should only be getting 4 or 5.
I am considering saving all the products in a database. If I try to save a product and can do so because it doesn't already exist in the database, it will be saved with today's date. After the entire saving process, I can retrieve the ones with the ""createdAt"" field equal to today's date and notify the user.
Is this a valid approach? I would prefer to avoid using databases if possible because I want to minimize time-consuming operations, but if there is no other way, it is okay. Do you have any ideas? Thank you!",1,2023-11-09 22:17:02
scrapy-impersonate: Scrapy download handler that can impersonate browser' TLS signatures or JA3 fingerprints.,"Hello everyone!
I wanted to share with you a tool I've been working on lately and found very useful: scrapy-impersonate. If you're conducting web scraping with Scrapy, this tool allows you to emulate TLS signatures or JA3 fingerprints of specific browsers. This will help you be less detectable, which can be useful when attempting to scrape some sites protected by Cloudflare.
â€‹
What is scrapy-impersonate?
Is a Scrapy download handler. This project integrates curl_cffi to perform HTTP requests, so it can impersonate browsers' TLS signatures or JA3 fingerprints.
â€‹
How to use it?
Is relatively straightforward if you're already familiar with Scrapy. Simply integrate it as your download handler and configure the browser signature you wish to impersonate. You'll be all set to make HTTP requests, impersonating browser signatures.
import scrapy


class ImpersonateSpider(scrapy.Spider):
    name = ""impersonate_spider""
    custom_settings = {
        ""DOWNLOAD_HANDLERS"": {
            ""http"": ""scrapy_impersonate.ImpersonateDownloadHandler"",
            ""https"": ""scrapy_impersonate.ImpersonateDownloadHandler"",
        },
        ""TWISTED_REACTOR"": ""twisted.internet.asyncioreactor.AsyncioSelectorReactor"",
    }

    def start_requests(self):
        for browser in [""chrome110"", ""edge99"", ""safari15_5""]:
            yield scrapy.Request(
                ""https://tls.browserleaks.com/json"",
                dont_filter=True,
                meta={""impersonate"": browser},
            )

    def parse(self, response):
        # ja3_hash: 773906b0efdefa24a7f2b8eb6985bf37
        # ja3_hash: cd08e31494f9531f560d64c695473da9
        # ja3_hash: 2fe1311860bc318fc7f9196556a2a6b9
        return {""ja3_hash"": response.json()[""ja3_hash""]}

Thanks!",3,2023-11-09 16:01:47
getting email address from list of websites,"I will have 1000s of website in a CSV file, I want a tool which can login to each website and then get the email addresses it has.
Which tool is best for this use case?",1,2023-11-09 17:22:13
How to scrape more than 10k results from elastic search,"I got a curl command from a website that allows me to use a get request to scrape a website data. I noticed it has elastic search in its backend, which allows me to perform pagination up to 10k results. How can I go beyond it? I found online that I could try sorting the results and use a search after parameter. But I can't make them work in a get request. Any hints? Thanks!",1,2023-11-08 21:05:54
Scrape the DBA name,"How could we scrape out something specific from a website?
For example
1. I have the legal names(that has the terms LLC/INC in it) and also the company URLs. I want to scrape the DBA(Doing Buisness As) name from the website. (Note: DBA name could be as simple as the legal name without the legal terms(LLC/INC), or as difficult as something completely different from the legal name) I tried using selenium but as the websites do not have a uniform way of alignment of things, it is difficult to write a generalised code for this purpose.

I have the website URLs , I want to scrape out all the details of any employee mentioned on the website. It could be present on the homepage itself, or it could even be present in some sub-link like contacts/about-us page.

Could anyone help me in this. Please share the code snippet/ documentation link if possible.",2,2023-11-08 16:35:11
Resolving recaptchas when form submit button is disabled,"So i have a problem. I have singed up for 2captcha service recently and i have tested it on googles recaptcha demo page and it works perfectly. But when I go to site i actually want to scrape i need to log in and i get the form with username, password and recaptcha. The submit button for the form is disabled by default and it is enabled after solving recaptcha (manually). I use selenium to populate those fields and to find all the data i need to send to 2captcha. After that i get the token in response which I put in that hidden textarea ""g-recaptcha-response"" but the button remains disabled. What do i need to do? I have tried just enabling and clicking the button but i get access denied. What could be happening when i manually solve the captcha that makes the button clickable? Should i somehow send a request to google so they would verify my captcha? Or maybe there are some javascript events i should trigger? Did anyone else have this kind of problem and how did you solve it?",2,2023-11-08 09:14:38
Is there actually a way to scrape Instagram without login ?,"Recently, a client of mine wants a script that can scrape all the followers of a specific account and to save them in a csv file, just a list of followers nothing more, I've looked a bit at resources online and it seems that there is no way to do this task without having an IG account login, I wouldn't want to risk me or my client getting our accounts or IP's banned (seeing as the targeted account has 3 million+ followers) so I'm asking here, is there actually a way to scrape only Instagram followers of a public account using Python ? Hope you have a nice day and thanks for reading !",7,2023-11-07 11:54:07
How to Use Headers When Reading JSON URL?,"Hi all! Thanks in advance for any guidance. I'm trying to read a JSON url in Python. When I open the url in chrome, I can see the data, but when I read it in python, I get {'error': 'Unauthorized'}. When I open the url in Edge, I get the same error. I determined it was because I have to be logged in to the website through my gmail account, and then I can see the JSON data. When I inspect the site's Network, I see a request called Session that contains the following (I changed some of the personal details): 
{
""user"": {
    ""email"": ""ekraft@gmail.com"",
    ""sub"": ""58edee6c-e210-4f2c-99a2-568efde67a6a"",
    ""auth0_id"": ""google-oauth2|106222607358455934444"",
    ""username"": ""ekraft""
    }
    ""expires"": ""2023-12-07T20:21:51.686Z""
}

Below is my current script. I googled more on it and it seems I have to work with these headers, but I'm not quite sure how to implement them. Any guidance is greatly appreciated! 
import requests
url = ""https://www.otmnft.com/api/reignmakerspro/football/liveListings""
response = requests.get(url) 
print(response.json())

â€‹",1,2023-11-07 21:03:47
Final thesis: Uni alumni analysis with Linkedin,"Hello people,
I am doing my final thesis on topic of future of my uni's students and their digital trace. We decided to use Linkedin for the data, currently there are around 4500+ alumni with Linkedin accounts, what is the best way for me to extract as much data as possible (ex. companies, positions, countries...)",1,2023-11-07 18:06:51
how not to iterate over the same posts,"I'm doing a school project, where i want to collect posts in certain timeframe, and while i believe the timeframe works, i assume my code keeps iterating over the same 1000 posts how one would make it look over the next batch?  
Sorry for noob questions  
here's the code  
# Define the subreddit and timeframe
my_subreddit = ""DotA2""
start_year = 2023
start_month = 5
end_year = 2023
end_month = 9
â€‹
# Define the keywords to filter by
keywords = ['The International', 'TI', 'Dota 2 Championship','TI12']
â€‹
# Define the maximum number of runs
max_runs = 10
â€‹
# Initialize an empty list to store collected posts
posts_data = []
â€‹
# Run the data collection process multiple times
for run in range(max_runs):
print(f""Run {run + 1} of {max_runs}"")
â€‹
â€‹
# Retrieve submissions from the subreddit
submissions = reddit.subreddit(my_subreddit).search(' '.join(keywords), limit=1000)
â€‹
# Filter posts based on the timeframe
filtered_posts = []
for submission in submissions:
submission_date = submission.created_utc
â€‹
submission_year = datetime.datetime.fromtimestamp(submission_date).year
submission_month = datetime.datetime.fromtimestamp(submission_date).month
â€‹
if start_year <= submission_year <= end_year and start_month <= submission_month <= end_month:
filtered_posts.append(submission)
â€‹
# Extract data from filtered posts
for post in filtered_posts:
post_data = {
'title': post.title,
'upvotes': post.score,
'timestamp': post.created_utc,
'body': post.selftext
}
â€‹
posts_data.append(post_data)
â€‹
# You can add a sleep here if needed to avoid overloading the API
# time.sleep(60)
â€‹
# Create a DataFrame from the collected data
reddit_data = pd.DataFrame(posts_data)
â€‹
# Save the DataFrame to a CSV file
reddit_data.to_csv(""reddit_test_submission_db_3.csv"", index=False)",1,2023-11-07 16:05:25
Scraping eBay as logged in user,"Iâ€™ve recently noticed that eBay made some product data protected, which means you need to be logged in to see it.
How would you scrape this data at all and without getting blocked?",0,2023-11-07 14:13:37
Is there a code/script to scrap press-releases of a website and automatically publish them as news article in your website?,"Hello everyone, one of my jobs is to monitor and publish the press releases of several ministries within my state. I get them through email but also can copy/paste them from the ministries press section. Itâ€™s all allowed and approved, but time consuming and mindless. 
In the website they are then published these press releases always have the same category and tag and always the logo of the state.
Is there a way to build something which basically just screens the press sections, clicks on the newest press release, copies the content, creates a new article, fills it with the tag and category and pastes the content and hits publish? 
Thanks In advance and sorry if this question is too easy/known. I just want to think of a way to automate an repetive task if possible",1,2023-11-07 12:00:16
Scrapy-splash help,"Hello all, 
I am currently in the process of converting a small scraper that i have built using selenium into scrapy using scrapy splash. During the process i have run into a frustrating roadblock where when I run the code response.css('selector'), the selector does not seem to be present in the DOM rendered by splash. However, when I run response.body, I can clearly see the data that i am trying to scrape in text format. For reference I am scraping a heavy JS website. This is an example of what i am trying to scrape, 
https://lens.google.com/search?ep=gsbubu&hl=en&re=df&p=AbrfA8rdDSYaOSNoUq4oT00PKy7qcMvhUUvyBVST1-9tK9AQdVmTPaBXVHEUIHrSx5LfaRsGqmQyeMp-KrAawpalq6bKHaoXl-_bIE9Y2-cdihOPkZSmVVRj7tUCNat7JABXjoG3kiXCnXzhUxSNqyNk6mjfDgTnlc7VL7n3GoNwEWVjob97fcy97vq24dRdsPkjwKWseq8ykJEI0_04AoNIjWnAFTV4AYS-NgyHdgh9E-j83VdWj4Scnd4c44ANwgpE_wFIOYewNGyE-hD1NjbcoccAUsvvNUSljdUclcG3KS7eBWkzmktZ_0dYOqtA7k_dZUeckI3zZ3Ceh3uW4nHOLhymcBzY0R2V-doQUjg%3D#lns=W251bGwsbnVsbCxudWxsLG51bGwsbnVsbCxudWxsLG51bGwsIkVrY0tKREUzWXpreE16RmxMV1UyTjJNdE5ETmxNeTA1WXpObExXTTNNemM1WkRrMk5XWXdNeElmUVhkQ2QySTBWbWRpTlRCbGEwaDRiR3BST0hJemVGODBRblJDTW5Wb1p3PT0iXQ== 
When i run the command items = response.css('div.G19kAf.ENn9pd') it returns an empty list. The equivalent code works perfectly in selenium.",1,2023-11-07 03:32:25
Loading the page in playwright and reading responses vs or making API calls to backend API directly,"When I load a webpage it sends a call to a backend API and downloads the data. I can get that data from Playwright. Or I can use Python requests to directly make calls to the API.
What are the pros and cons of each? Is one more likely to get detected? Is one more fragile/likely to break in the long run than the other?
Aware it might vary by site, but looking for broad principles.",2,2023-11-06 17:08:37
Scraping eBay - Can I get more than just exactly what's on the results page?,"I am pretty new to Python and web scraping. I know there's tools out there where I can enter a URL (the exact URL for what I searched for), then I can choose which data on the actual page I want exported to Excel. This limits me to the Title, Price, type of sale (Auction or Buy Now) and date. However, there's dozens of fields/metadata behind every listing. Is there a way to get an export of all the listings returned in my search with columns populated for the specific fields you'd only find on the specific listing's page?",1,2023-11-06 19:02:23
chrome driver version issue for long term use,i have a script that scrapes data from various websites and i am running the script on weekly basis but after chrome update i started getting issues related to driver is there any permanent solution for this.,3,2023-11-06 04:56:58
Automatically pulling a website's contents,"Hi everyone,
â€‹
Hopefully this is the right place for this type of post.
For a project I'm working on, I have a list of websites whose contents I need to pull (text, links, title). Specifically, have all the text from the websites but none of the HTML or Javascript. In addition, it wouldn't just be every website but also the contents of all the site's subpages. Is there a tool or a library that allows me to do this in an easy manner?
â€‹
Many thanks
â€‹
â€‹",2,2023-11-05 23:52:43
Help with store location,"Hi all, 
Sorry Iâ€™m super new to scraping and only just managed to get my head around using a surface level element scraper from a chrome add in. 
Iâ€™m looking to pull all of the store names and locations for Subway Australia https://www.subway.com/en-AU/FindAStore
But given the data only pulls from when I search, rather than manually checking every post code is there a faster way to tabulate this info? 
Thanks!!!",1,2023-11-06 03:04:55
Using Web Scraper Chrome extension to scrape Facebook post images,"Hi! First time poster here!I've been trying for some time to go over my many years of Facebook's Saved Posts, iterate over them and from each Post (many in private Groups) extract the description plus images. I've had zero success with GraphQL or scrapping from the backend, but I recently had some great success with Web Scraper Chrome Extension.The problem is that I don't seem to get the ""Sitemap"" right, specially when I try to ""click"" on the first image of a post, and then iterate over the remaining images to get their SRC.
In case anyone is interested, this is the sitemap I'm using. I can paginate over the different images, but I'm not getting any src.

{
   ""_id"":""facebook-saves"",
   ""startUrl"":[
      ""https://www.facebook.com/saved""
   ],
   ""selectors"":[
      {
         ""id"":""posts-wrapper"",
         ""parentSelectors"":[
            ""_root""
         ],
         ""type"":""SelectorElement"",
         ""selector"":"" div.x9f619.x1n2onr6.x1ja2u2z > div > div > div > div.x78zum5.xdt5ytf.x1t2pt76.x1n2onr6.x1ja2u2z.x10cihs4 > div.x9f619.x2lah0s.x1nhvcw1.x1qjc9v5.xozqiw3.x1q0g3np.x78zum5.x1iyjqo2.x1t2pt76.x1n2onr6.x1ja2u2z.x1h6rjhl > div.x9f619.x1n2onr6.x1ja2u2z.xdt5ytf.x193iq5w.xeuugli.x1r8uery.x1iyjqo2.xs83m0k.x78zum5.x1t2pt76 > div > div > div > div > div:nth-child(2) > div:nth-child(3)"",
         ""multiple"":false
      },
      {
         ""id"":""post-link"",
         ""parentSelectors"":[
            ""posts-wrapper""
         ],
         ""type"":""SelectorLink"",
         ""selector"":"".x1iyjqo2 a[target]"",
         ""multiple"":false,
         ""linkType"":""linkFromHref""
      },
      {
         ""id"":""description"",
         ""parentSelectors"":[
            ""post-link""
         ],
         ""type"":""SelectorText"",
         ""selector"":"".x1iorvi4.x1pi30zi span.x193iq5w"",
         ""multiple"":false,
         ""regex"":""""
      },
      {
         ""id"":""first-image"",
         ""parentSelectors"":[
            ""post-link""
         ],
         ""type"":""SelectorLink"",
         ""selector"":""div > div.x1n2onr6 > div > div > div > div:first-child > a:has(img:not([data-imgperflogname])), div:nth-child(2) > div.x1n2onr6 > a:has(img:not([data-imgperflogname]))"",
         ""multiple"":false,
         ""linkType"":""linkFromHref""
      },
      {
         ""id"":""paginator"",
         ""parentSelectors"":[
            ""first-image"",
            ""paginator""
         ],
         ""paginationType"":""clickMore"",
         ""selector"":""div.x6s0dn4.x1ey2m1c.x78zum5.xds687c.x1qughib.x10l6tqk.x17qophe.x13vifvy > div:nth-child(3) > div > div > div > i"",
         ""type"":""SelectorPagination""
      },
      {
         ""id"":""carousel-image"",
         ""parentSelectors"":[
            ""paginator""
         ],
         ""type"":""SelectorImage"",
         ""selector"":""img.x1bwycvy"",
         ""multiple"":false
      }
   ]
}",1,2023-11-05 19:01:01
Web Crawling JS heavy websites,"Hi, so Iâ€™m having a problem with the following task. Given an arbitrary site â€˜urlâ€™ I want to be able to find all its sub-links given a specified depth, and then get the HTML content of each of those sub-links including the given site.
Iâ€™ve been trying to use Scrapyâ€™s Crawlspider to find all sublinks, which has been pretty successful. However, Iâ€™m facing a problem parsing whenever the site is JS heavy. I want to use Playwrightâ€™s Scrapy-Playwright extension to address the issue, however Iâ€™m having trouble integrating it with CrawlSpider.
Has anyone done anything similar, or got any tips?
Thanks!
TLDR: Need help with interesting Scrapyâ€™s Crawlspider with Playwright.",3,2023-11-05 04:39:26
How to bypass shape? Target?,"â€‹
https://preview.redd.it/2k22ycn80dyb1.jpg?width=1280&format=pjpg&auto=webp&s=8c470f7e03d73b858321f85b63ad145c4ecb2a79",1,2023-11-04 16:40:12
How to scrape a GraphQL endpoint that uses Apollo,"â€‹
I was trying to get some data from this website, the Apollo dev tools extension works in that it returns some results, but I couldn't figure out a way to do it programmatically. When I use the Apollo dev tools, what is the url that the queries are going to? Is it just the page that your current tab is on? And how does the extension send the requests?
This is the extension page, and whenever I run the query, I can see a graphql.json being POSTed with a unique cipher and token field. The server also returned with a cipher and token in its response. 
Does anyone know how the payload is hashed? Is it possible to send this request some other way using a request API of python or something?
https://preview.redd.it/585wx3h4h2yb1.png?width=934&format=png&auto=webp&s=78d8620dfd67ce57c8054ff44548645dbcc28cd6
https://preview.redd.it/zdyq58h4h2yb1.png?width=969&format=png&auto=webp&s=68174888383ebbe06c5d8c2b89652ffe0255b903
â€‹
â€‹",1,2023-11-03 05:17:44
How to scrape JSON url that requires credentials?,"How can I read a JSON url that requires logging in? When I open the URL on chrome, I can see the data, but when I run my script below in Python, I get the following output: {""error"":""Unauthorized""}
I'm assuming that python can't read the JSON data because of the credentials. If it helps, I use gmail to login to the site. Is there a way to add credentials to the script below? Any help is greatly appreciated!
import requests
import json

response =requests.get('https://www.otmnft.com/api/reignmakerspro/football/liveListings?')
json_data = json.loads(response.text)
print(json_data)

â€‹",3,2023-11-02 17:37:04
Yet another starter questions thread,"Ok not so starter but I finally hit a wall in my web scraping journey and learned the basics of requests, BS4, Selenium (Just a quick and dirty Project), and Playwright.
1 - Requests:  I've found the requests that the website uses to show the data but sometimes can't manipulate it using another request to get the data back, in this case, how can I improve my requesting skills?
2 - Performance/Speed:  I can use a Headless browser but the scraping speed is slower, for doing commercial work is this acceptable? (Taking into account that the client will run the scripts)
3 - Running on other machines: Which is easier to package and make run on another machine with PyInstaller  (Are the alternative methods ?)
Thank you very much WS community, the threads are being very helpfu l :D",1,2023-11-02 20:44:16
Adding Proxy to existing Scraper,"Hello everyone,
because I'm not a developer, I took this project https://github.com/sandra-liedtke/ti_scraper to help me.
With a little adaptation, I now have a scraper running on my Raspberry that collects articles from selected news sites every hour and sends a summary to me by email.
Now I would like to query a news site more frequently, e.g. every minute. But then I get blocked. I would like to use proxies to get around this (be it free or commercial service providers). 
The only question is: How do I implement the use of a proxy service provider, which has to provide itself with a new IP after every call or every minute?
Or is there another tool with which I can check a specific news page for content at freely selectable intervals?
Thanks",4,2023-11-02 13:06:15
Code/Project Review,"I know self-promotion is not accepted and I don't want to come off in a wrong way... How can I share a link to my docker project for review without offending the community? I'm not looking for a code review but rather a feature or functionality review and suggestions.
Link to the project: Tor Rotator
It's very simple at this point, suggestions are welcome.",3,2023-11-02 08:36:35
Help indie SaaS developer market/sell,"Iâ€™ve seen many talented developers build amazing products/SaaS over here but, like me, struggles tremendously with marketing/sales. Unknown or expensive path to expose my SaaS to a wider audience
I think that would be an incredible problem to solve
What if we connect the indie SaaS developer with the promoter/influencers? (Not the hipped one, but the long tail niched affordable by the indie developer)
Thereâ€™s probably something similar to this out there. Thats fine! Would love to know. Iâ€™m just looking for feedback/initial thoughts on the problem statement and if anyone â€œupvotesâ€ my solution or have a better one (I honestly would love to hear)
Peace to you all",0,2023-11-02 02:54:11
What would be the best way to webscrape multiple websites for recipes?,"So I'm making a ""Recipe Finder"" website (mainly for a project to put on a resume) and since I'm interested in cooking, I thought a good project would be to make a website where you can select different ingredients that you have available (e.g. chicken breast + paprika + flour) and it would list all of the recipes from numerous websites that contain those ingredients.
I've never really done webscraping before, so I'm unsure how to go about it. Just thinking about it, individually having to webscrape 10s of different websites to gather all their recipes doesn't seem efficient, but I'm unsure how else to do it. 
So yeah, anyone have any advice on how to go about doing it? I'll probably be using AngleSharp for C# (I know, it'd be way better if I used Python). 
Thanks.",3,2023-11-01 18:21:42
"Instruments, Packages, etc. for parsing 10-X filings for Schedule(s) of Investments?","I have a select group of SEC 10-X (10-Q and 10-K primarily) filings I want to look at but I've been running into issues for the past several months with trying to parse the Schedule of Investments from these filings. My approach has been very... raw in that It's mostly just taking the .txt files and parsing them with BS4 and using a lot of shoddy logic to try and isolate the right tables before extracting and cleaning everything.   
Issue is, especially pre-2009, formatting is hella wonky. It's so inconsistent that I can't hardly come up with a good way to semi-consistently scrape the info I need. I need to do this for the statement of operations as well as the statement of assets and liabilities as well, but the schedule of investments is honestly more of a priority and has given me a lot more grief.   
Are there any packages I should be making use of that would make this more feasible, or better yet is this something that's already been done that I can look at for reference?? (I'll need to customize the code a bit for my own sake, but it would still save a lot of hours lol).   
Also, before anyone says anything, yeah I've considered going to vendors but the issue is that a good handful of the filings I'm using actually pre-date what pretty much any vendors I've looked at actually have, so it wouldn't be all that helpful in the grand scheme of things. Plus I want to be able to see this through and have something reproducible at the end instead of just ""go buy this"".   
I've tried looking around to see if anyone has already managed to craft code for this purpose, but everything either seems to make use of a format other than the .txt files I've downloaded (the HTML), or they are extracting something more akin to header information, etc. which is not what I'm after.",1,2023-11-01 22:10:54
Python Web scraping Project,"Hey guys!
I am a college student and I am working on a project right now. I need to scrape a website for a LOT of data. In total I will have to make around 20,000,000 requests. I know this will take a very long time. I am using Python and Beautiful soul to parse this data. The problem is that over time I will start receiving 403 responses. I have already created random headers to try to counter this. I have looked into rotating proxies but am struggling a little bit to start. I know I can buy tools for this but I want that to be a last ditch effort. I am trying to build an algorithm to get working proxies from free-proxy-list.net right now but Iâ€™m not sure if that will work since my list of working proxies keep coming back empty. I am new to this so any advice will help.
Thank you so much!",1,2023-11-01 21:53:05
Where to sell data?,"I'm looking for a marketplace to list data for selling. I've scrapped a few book summary websites (blinkist, shortform, getabstract, instaread), converted text to epub/pdf + audio in mp3. I'm planning to add more sites to this data. 
Where should I list this data for sell & what should be the asking price for it?",4,2023-11-01 12:15:48
Monthly Self-Promotion Thread - November 2023,"Hello and howdy, digital miners of /r/webscraping!
The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!

Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?
Maybe you've got a ground-breaking product in need of some intrepid testers?
Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?
Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?

Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!
Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",5,2023-11-01 11:02:04
Looking for tips on cheerio.js (css) selectors for prices on webshop detail pages,"I'm working on a scraping tool that scrapes multiple web shops. I'm scraping all sorts of data but for this question it's about returning a `currentPrice` and a optional `originalPrice` (if there is a sale). Off course all these websites have different structures so i pass in my selectors depending on the web shop I'm scraping. For a lot of websites this works fine, but some make my life a little bit harder. Some scenario's i come across:
- Different structure when product has a single price vs a product that is on sale- All kinds of hidden elements with prices in them- Sometimes even different structures for product detail pages within 1 web shop.
Maybe I'm just shit at making (CSS) selectors, or maybe i should be changing other things then my selectors, but i wanted to see if maybe someone here has tips for me on how to do it.
Here are some examples.
Product detail page with single price (https://www.bescards.nl/pokemon-battle-deck-tinkaton-ex/):
<div class=""wd-single-price wd-wpb text-left"">
    <p class=""price"">
    <span class=""woocommerce-Price-amount amount"">
        <bdi>
        <span class=""woocommerce-Price-currencySymbol"">â‚¬</span>18,99
        </bdi>
    </span>
    <small class=""woocommerce-price-suffix""> incl. btw</small>
    </p>
</div>

Product detail page with sale price (https://www.bescards.nl/pokemon-charizard-ex-premium-collection-box/):
<div class=""wd-single-price wd-wpb text-left"">
<p class=""price"">
    <del aria-hidden=""true"">
        <span class=""woocommerce-Price-amount amount"">
            <bdi>
                <span class=""woocommerce-Price-currencySymbol"">â‚¬</span>50,00
            </bdi>
        </span>
    </del>
    <ins>
        <span class=""woocommerce-Price-amount amount"">
            <bdi>
                <span class=""woocommerce-Price-currencySymbol"">â‚¬</span>39,99
            </bdi>
        </span>
    </ins>
    <small class=""woocommerce-price-suffix""> incl. btw</small>
 </p>
</div>    

Or this is another shop that does stuff weird with hidden divs:https://www.debroergrot.nl/products/pokemon-tcg-paldea-evolved-pawmot-premium-1-pack-blister (single price)https://www.debroergrot.nl/products/pokemon-tcg-scarlet-violet-paldea-evolved-booster-pack (sale)
I even got to the point where i was concidering other things, but since i don't see anyone do this i'm not sure these are the way:

Use openai API to ask it giving it a bigger piece of DOM (chatGPT seems to get it right every time)
Passing in a bigger portion of the DOM to a custom script that searches/filters for prices (hard with hidden stuff)
Creating some kind of machine learning thing myself
Taking a screenshot and getting the prices from there.

How do you guys do this? Am i missing something?",1,2023-11-01 17:11:28
Question about automatic scraping,"Hello,
I have made a script in Python that scrapes data and export it into an Excel file.
This data is being updated once a week so every week I need to run it manually.
How can I make it to run once a week automatically?",3,2023-11-01 08:22:53
"Webscraping state registry, possible?","Hey so I need to scrape the state registry for all businesses that contain ""XX"" keyword. Thing is the state (FL - sunbiz) doesn't have a 'contains' search filter like some of the other states, what are my options?
â€‹",0,2023-11-01 14:12:20
Scraping question,"How can I scrape an API for information about each username on a large list? On average, every 17th username has an account on the website, and I want the data of at least 10 million users. What's the fastest way to achieve this? I make 1-2 million requests daily using Python (asyncio, aiohttp). Also, I have a big list of usernames, so that's not a problem.",1,2023-10-30 23:16:02
all project scraping in Upwork,"I'm attempting to scrape project descriptions from different projects, but I'm facing challenges. Scrapy isn't giving me the access I require, and I need help finding the correct selector with Playwright. Could anyone please give me some guidance? 
Link: https://www.upwork.com/services/logo-design?all=true",2,2023-10-30 18:29:57
Timeouts when Running Puppeteer in GCP CloudRun,"I'm building a node/express app that uses puppeteer to scrape a few sites.  It runs fine locally, but when i deploy to CGP I always get a timeout error.
""TimeoutError: Timed out after 60000 ms while waiting for the WS endpoint URL to appear in stdout!     at ChromeLauncher.launch (/app/node_modules/puppeteer-core/lib/cjs/puppeteer/node/ProductLauncher.js:149:23)     at async /app/utils/getBrowser.js:20:17""
I know chromium can be slow on the cloud, so it setup my cloud run container with with 8gigs of ram and 8 cpus and that doesn't make any difference.
I am using headless chrome and this is how i instantiate a browser: 
browser = await puppeteer.launch({
timeout: 60000,
headless: ""new"",
executablePath: ""/usr/bin/chromium"",
args: [""--no-sandbox""],
});
Finally, I'm using docker, and I'm admittedly very new to docker.  I realize using the full Node isnt a  great practice, but I had no success getting alpine to work with chromium.  here is my dockerfile if anyone thinks it may be the issue: 
FROM node:lts
# Set working directory
WORKDIR /app
# Update and install dependencies using Debian's package manager
RUN apt-get update && apt-get install -y \
nmap \
chromium
# Clean up APT when done
RUN apt-get clean && rm -rf /var/lib/apt/lists/*
# Copy the package.json and package-lock.json files to the container first
COPY package*.json ./
# Install the node modules
RUN npm install
# Copy the rest of your app's source files to the container
COPY . .
CMD [ ""npm"", ""start""]
Any thoughts at all are welcome.",2,2023-10-30 15:46:55
No quick scraping option for this task?,"Hey, I'm newer to the world of programming and computational work (I'm in the social sciences), and I'm currently tasked with overseeing a project where we need gather all of the youtube links that congress members share on their websites to help with a political science research project we're doing. Many of these sites have a news/press release section where there will be a page that will display the top 5 or 10 most recent updates, then you can click next to the next page of 5-10 more  posts and so on. Some of these sites are pretty quick and can have one person click through all of the press releases pretty quickly to snag any of the youtube urls, but then there are others that literally have over 6,000 press releases to click through which takes a massive amount of manhours. The problem is that we need to do this for each congressional website which are all different, so we can't really build a one-size fits all webscraper for the task, so the thought right now is to just apply for a grant to get a bunch of undergrads to hammer away at the mindless tasks of going through all of the pages manually. This is also because out team does not have anyone particularly experienced with webscraping, though a few are quite experienced in other computational processes.   
However, I just wanted to check and see if we might be missing a more efficient way of doing this. I just spent an hour or two trying to see if the Scraper and WebPilot plugins for GPT-4 might be able to handle the task of iteratively gathering youtube links from those pages like we need, but it was way too buggy to actually work. Is there some other expedited or releatively efficient way (i.e., some tool or tutorial you'd recommend) for someone like me with only minimul python experience to be able to craft a scraper for each site within a couple of hours, or would it likely take 5+ hours for someone of my skill level to get to the end result of the links I want from each site? Thanks!",1,2023-10-30 15:50:08
Why can't I use find_all() on a BeautifulSoup-object in a for loop?,"I ran into this problem while working on this code right here:
python
issues = soup.find_all(""div"", {""class"":""issue no-underline""})
for issue in issues:
    content = issue.find_all(""div"")
    print(content)

The method issue.find_all() is unknown.
However this is working:
python
issues = soup.find_all(""div"", {""class"":""issue no-underline""})
for issue in issues:
    content = issue.div
    print(issue)

And so is this:
python
issue = soup.find(""div"", {""class"":""issue no-underline""})
content = issue.find_all(""div"")
print(content)

WHY? Could someone enlighten me, please? I need to filter further but don't know how.
EDIT:
It just occurred to me that I can run it like this:
python
issue = soup.find(""div"", {""class"":""issue no-underline""})
content = issue.parent.find_all(""div"", {#filter})
print(content)

But there must be another, more elegant way, right?",1,2023-10-30 12:41:48
Element scroll down problem,"Hello, I am new with the chrome extension called Web Scraper and I want to scrap a list of link from a website then scroll down and do this until the end but when I scrape, it scrolls down for 2 times, then goes all the way up, then comes back all the way down and tells me that the scrap is done even if I clearly see there is still tons of link so I dont know how to solve that. Here's my sitemap :
{""_id"":""Unleash"",""startUrl"":[""https://unleashevents.app.swapcard.com/event/unleash-world-1/people/RXZlbnRWaWV3XzQ4MTkyOQ==""],""selectors"":[{""delay"":2000,""elementLimit"":10000,""id"":""scroll"",""multiple"":true,""parentSelectors"":[""_root""],""selector"":""div.wrapper__Wrapper-ui__sc-1an8oac-0"",""type"":""SelectorElementScroll""},{""id"":""url"",""linkType"":""linkFromHref"",""multiple"":true,""parentSelectors"":[""scroll""],""selector"":"".container__List-cmp__sc-1smp56b-0 > a"",""type"":""SelectorLink""}]}",1,2023-10-30 10:24:31
Tips about Web Scraping Project,"Hello everyone! I would like some tips on which direction I can take in my Web Scraping project. The project involves logging into a website, accessing 7 different pages, clicking a button to display the data, and exporting it to a CSV to later import it into a Power BI dashboard. I am using Python and the Selenium library for this.  
I want to run this project in the cloud, but my current situation is that I only have a corporate computer, so downloading programs is quite limited, such as Docker, for instance.   
Do you have any suggestions on which directions I can explore to execute this project in the cloud?",2,2023-10-29 17:35:20
"Is it legal to scrap price of items in regular intervals from website's like amazon, meesho, flipkart etc..","Just a week ago my friend came to me and started telling about these ecommerce website increase and decrease prices of some items and he was waiting for the prices to go down and buy that.
That's how i got an idea to make a tool to track these changes and build a dedicated website.
But is it legal to scrap data from these sites ? Program will scrap data in a fixed interval multiple times, So I'm not sure will it be legal to scrap data from these sites. For now i just need to scrap prices which is generally public we can see it without logging in if I'm not wrong.",6,2023-10-29 05:07:01
"Are '//a[@href *= 'thing']' and '//a[ contains( @href, 'thing' ) ]' the same?","when doing select elements with chrome devtools xpath, i've notised that contains() is working fine, but *= doesn't.  so, what's the difference? thanks",1,2023-10-29 11:44:13
Exif data,"Hi, does anyone know a good exif metadata online viewer? I came over this one: Jeffrey's Exif Viewer which should be very good, but which is down now. Thanks",1,2023-10-29 09:38:33
SeleniumBase executing script on open browser,"I am used to doing this with Selenium but I cannot get it to work with SeleniumBase.
from seleniumbase import Driver
from seleniumbase.undetected import Chrome
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_experimental_option(""debuggerAddress"" , ""localhost:0249"")

web: Chrome = Driver(browser='chrome', uc=True,chromium_arg=chrome_options,remote_debug=True)

body = web.find_element(""/html/body"")
print(body.text)

This is what I have. I have tried starting my script in cmd with --remote-debug as well as putting remote_debug=True and port=""0249"" in Driver with no luck. Has anyone used SeleniumBase in this way? I want to be able to have a website open and try running various tests on the same webpage without it closing.
â€‹
Im trying to do something like this: https://youtu.be/Zrx8FSEo9lk?feature=shared&t=433",1,2023-10-28 19:40:49
bit of a dum question tbh,"How would i go about creating thousands of outlook accounts within minutes, i tried using python with selenium but as you would probably guess its to slow (with proxies as well), i want to generate accounts to use a method i found to get unlimted free trials on stuff, i would assume people use basic scripts like mine but maybe ones which are more built towards automation? im not to sure it may be a bit of a dum question tbh.",1,2023-10-28 18:42:35
Webscraping Article Titles,"[python beautiful soup4]I am attempting to webscrape a college's student news paper for all of the headlines that have been created over the years. The problem is that the headlines do not have their own class so I am stuck with the text that precedes the string. My code can grab this chunk but I do not know how to filter out the link that comes before the text.
```
<h2 class=""entry-title default-max-width""><a href=""https://www.newschoolfreepress.com/2023/10/25/lamp-clubs-shoes-optional-day-how-this-wacky-student-organization-is-fighting-social-alienation-and-overhead-lighting/"">Lamp Clubâ€™s â€œshoes optionalâ€ day: How this wacky student organization is fighting social alienation and overhead lighting</a></h2>
```
All I want to grab is the plain text that occurs after the link but I can't figure out how to filter out everything that comes before it.
Using a for loop and just adding .text to the current value seems to work but I don't know if that is a good way to do it. I also don't know how to navigate through multiple pages to scrape every article title. Will I need Selenium for this or does Bs4 have this ability? 
New to this so any help is appreciated.",1,2023-10-28 18:00:41
Facebook Group Scrape that I â€œownâ€,"Hi! I have read through all of the past posts on scraping a Facebook group, but I didnâ€™t find what Iâ€™m looking for. 
I admin and â€œownâ€ a private Facebook group Iâ€™d like to scrape. I know itâ€™s possible for admins to install programs that help with this- Iâ€™ve done it for content marketing companies Iâ€™ve worked with. 
Does anyone have any information on something I can install as an admin of a private Facebook group to scrape data? 
Any help, links, or suggestions of companies that can help with this, Iâ€™d love to hear!",0,2023-10-28 16:05:31
A website changes Cookies each refresh and request,"Hey, a newbie to web scraping here. I need to grab some html from a website, I managed to do it fine with selenium and clicking buttons, but I really want to do it using requests.
The issue is that each time I open the web page, the cookies change, and each ajax request within the page has different cookies. Some cookies do not change, but 2 or 3 of them do change.
Is there any common technique to do this, and how to handle it?",1,2023-10-28 15:49:29
New to web scraping,"I have recently started learning about web scraping & it's interesting how easy it is to scrape websites using BeautifulSoup 
Now i need to practice it, Can you recommend me some resources / Projects to practice it",1,2023-10-28 07:41:17
Need help with scraping Reddit (BeautifulSoup and requests),"I'm trying to get the time of when each post was created (15 hours ago, 40 minutes ago, 2 days ago, etc) on the hot page. When using urlopen I'm successful, but only the first 3 posts come up. 
I've seen multiple tutorials suggesting the following, but it comes back blank every time:
>>> def getdata(url):
...     r = requests.get(url, headers = HEADERS)
...     return r.text
...
>>> url = 'https://www.reddit.com/r/Python/'
>>> htmldata = getdata(url)
>>> soup = BeautifulSoup(htmldata, 'html.parser')
>>> data_str = """"
>>> for item in soup.find_all('span', class_='_2VF2J19pUIMSLJFky-7PEI'):
...     data_str = data_str + item.get_text()
...
>>> print(data_str)
â€‹
>>>
Any help or suggestions would be super appreciated. I'm a novice to programming and only knowledge I have is from this webscraping book I picked up (literally just to get this specific data)
â€‹",1,2023-10-28 03:45:07
Can I Scrape Stocktwits for data on the FTSE 100?,"Hello
I am doing a project on prediction of the FTSE 100 daily closing price and I plan to use microblogging services (Stocktwits) to discern market sentiment and use that as an influencer for my model. How would I go about scraping StockTwits for historical data and what is currently being mentioned?
(I am asking as I spent a good few hours reading research papers on twitter and how it affects market sentiment without  realising the Twitter API was paid and I can't ""legally"" scrape it. So I'm trying to not waste my time with this one.)
Thanks for any advice :)",2,2023-10-27 17:30:57
How Do You Charge For Web Scraping Services?,"Hey everyone, Iâ€™ve created a web scraper for a friend that pulls from advert sites and sends alerts on Telegram when thereâ€™s a new post. Currently, Iâ€™m awaiting API keys from the advert site to enrich the Telegram alerts with the advert ownerâ€™s phone number. Itâ€™s hosted on EC2, and heâ€™ll cover the hosting costs. He also wants ongoing maintenance like adding new advert links and compatibility with new advert sites.
Iâ€™m unsure how to charge him for the initial setup and for future improvements since this is new territory for me. Can you share your pricing strategies or any advice on this? Thanks!",6,2023-10-27 10:27:56
Selenium Python Help,"Hi, I'm new to coding and I am learning python. As my first real project I am creating a room reservation bot that can reserve practice/ study rooms at my university using python and Selenium. So far the code can log me and navigate to the time selection page but I have encountered an issue here.
There are two drop down menus for the reservation start time and reservation end time with unique names, but the individual time slots are shown like this.  
<select id=""EndPeriod"" name=""endPeriod"" class=""form-control  input-sm inline-block timeinput"" title=""End time"">
<option value=""00:30:00"">12:30 AM</option>
<option value=""01:00:00"">1:00 AM</option>
<option value=""01:30:00"">1:30 AM</option>
ect... for all 24 hours in 30 min blocks. 
â€‹
How can I search for viable text within a drop down menu, since the values for each list are identical and I can click on the drop down to expose the text,  or search for the Value = in Selenium? I've tried a lot of solutions online but none have been helpful. 
â€‹
Thanks",1,2023-10-27 18:21:12
"Need help accessing ""div""-class with requests or HTMLSession","Hi everyone
I'm just starting out with webscraping and python as a fun side project and need help. What I'm trying to do here is to get all the new threads from the community hub for Star Citizen announcements. 
This is how far I've come so far:
Code
``` python
import requests
from bs4 import BeautifulSoup
page = requests.get(""https://robertsspaceindustries.com/spectrum/community/SC/forum/1?page=1&sort=hot"")
soup = BeautifulSoup(page.text, ""html.parser"")
print(soup.body)
```
Output
html
<body>
<div id=""react""></div>
<script src=""/rsi/static/tavern/bundle.js?v=6.15.0""></script>
<script data-cookieconsent=""statistics"" type=""text/plain"">
          (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});
          var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;
          j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})
          (window,document,'script','dataLayer','GTM-KHNFZPT');
        </script>
</body>

Problem
The issue now is that the content I am looking for is under div id=""react"". When I inspect the element in my browser I can navigate through it. I've also been trying this with requests_html aswell, since I came about a forum post which explained that parts of webpages can be loaded by js after loading. But no success here aswell.
Can anyone point me in a general direction?",1,2023-10-27 18:16:25
how to wait for multiple lazy loaded css selectors?,"INFO: This is 1to1 copy of post written on r/Playwright. hope that by posting here too I can get more ppl to help. 
I spent so much time on this I just cant do it myself. Basically my problem is as follows: 
1. data is lazy loaded
2. I want to await full load of 18 divs with class .g1qv1ctd.c1v0rf5q.dir.dir-ltr
How to await 18 elements of this selector? 
Detailed:
I want to scrape following airbnb url: link I want the data from following selector: .gsgwcjk.g8ge8f1.g14v8520.dir.dir-ltr which has 18 elements that I wanna scrape: .g1qv1ctd.c1v0rf5q.dir.dir-ltr. everything is lazy loaded. I use scrapy + playwright and my code is as one below:
```
import scrapy
from scrapy_playwright.page import PageMethod
def interceptrequest(request):
    # Block requests to Google by checking if ""google"" is in the URL
    if 'google' in request.url:
        request.abort()
    else:
        request.continue()
def handleroute_abort(route):
    if route.request.resource_type in (""image"", ""webp""):
        route.abort()
    else:
        route.continue()
class RentSpider(scrapy.Spider):
    name = ""rent""
    start_url = ""https://www.airbnb.com/s/Manhattan--New-York--United-States/homes?tab_id=home_tab&checkin=2023-11-20&checkout=2023-11-24&adults=1&min_beds=1&min_bathrooms=1&room_types[]=Private%20room&min_bedrooms=1&currency=usd""
def start_requests(self):
    yield scrapy.Request(self.start_url, meta=dict(
        playwright = True,
        playwright_include_page = True,
        playwright_page_methods = [
            # PageMethod('wait_for_load_state', 'networkidle'),
            PageMethod(""wait_for_selector"", "".gsgwcjk.g8ge8f1.g14v8520.dir.dir-ltr""),
        ],
    ))

async def parse(self, response):
    elems = response.css("".g1qv1ctd.c1v0rf5q.dir.dir-ltr"")
    for elem in elems:
        yield {
                ""description"": elem.css("".t1jojoys::text"").get(),
                ""info"": elem.css("".fb4nyux ::text"").get(),
                ""price"": elem.css(""._tt122m ::text"").get()
        }

``
And then run it withscrapy crawl rent -o response.json`. I tried waiting for networkidle but 50% of the time it timeout after 30sec. With my current code, not every element is fully loaded. This results in incomplete parse (null data in output json)
Please help I dont know what to do with it :/",1,2023-10-27 15:29:00
Any sources to scrape Math formulas from,"I need to create a database of math formulas, the format preferably should be latex , mathml or any standard.  
Can anyone direct me to any source?",2,2023-10-27 05:12:29
TransferMarket Scraping,"Hey there. Im an economics student developing an econometrics model to estimate points obtained by futbol (soccer) clubs. My only ""coding"" experience is some work in stata and r.studio (close to nothing). Im trying to scrape from transfer market some data sets for clubs and their value for aug 1st 2022. If anyone has any clue it would be immensely appreciated. So far I've only done the following: 
https://preview.redd.it/mr34ispiulwb1.png?width=2258&format=png&auto=webp&s=d26bf9b797fe0b99f16b576a4154cff2bf735028",1,2023-10-26 20:16:48
Web Scraping or Web Crawling,"Hi, I'm fairly new to web scraping and want to understand the basics. Can someone explain the difference between web scraping and web crawling, and when  should I use each of these?",1,2023-10-26 17:17:25
Where do you host your scraper bots?,Running bots locally on your own computer is often not a good enough option. Where do you host and what does your deployment workflow look like?,4,2023-10-26 09:17:35
Cigna Contracted Providers,"I have a program that gets a list of unique zip codes based on a group's health insurance census. I work with small employers trying to give their employees access to a good health insurance network and am trying to help present them with Cignaâ€™s Network penetration based on their group's geography. 
I am trying to create a script that searches Cigna's provider directory for a list of specific zip codes and returns all of the provider information on contracted Primary Care Providers (PCPs). Into a Google sheet so all provider info is neatly parsed into a spreadsheet. 
Iâ€™m generally new to Python and have messed around with Scrapy. However, I canâ€™t seem to figure it out.
If you hate how difficult Health insurance providers make everything so convoluted and complicated when using your health insurance. Now is your time.",2,2023-10-26 12:30:24
Scraping Address information based on URL list,"Hi Everyone,
First time posting here in the community.
I am hoping that someone is able to direct me in the right direction regarding an application and or plugin that would allow me to take my list of company urls and have them scrapped for their address and contact information.
As I have tried several applications and plugins and seen no success I hope to hear from someone with a bit more knowledge.
â€‹
Looking forward to seeing the suggestions!",2,2023-10-26 11:07:39
Does styleseat.com and booksy.com have a public API? Potentially wanting to scrape these sites for my site.,"Does styleseat.com and booksy.com have a public API?
Iâ€™m not a developer but would like to know before hiring one to understand the complexity of what I would like done. 
Does styleseat.com and booksy.com have a public API that is useable for someone to scrape data and upload the data to their own website?",0,2023-10-26 11:23:08
How frequent can I send http request to website without it being considered as DoS attack?,"I am scrapping data from couple of sites which are fairly popular locally (scrapping is legal on each of those). I need my app to work as fast as it can but can't send too many requests to a site because of potential denial of service/server overloading.
I am thinking of putting explicit delay (maybe 2-5s) between each of my requests. How do I know if this is too much or too low. Is there rule of thumb?
Any help would be very appreciated!",7,2023-10-25 12:41:59
Scraping store details from a store locator,"Hi all! I want to extract a list of all stores from a store locator like this one 
https://dita.com/a/store-locator/list
Does anyone know how to? I donâ€™t know python! Any tools that could help? Really appreciate the help",2,2023-10-25 17:31:34
N\t\t\t\t....what is this when running response.css??,"Am a newbie when if comes to scrapping using scrap...i am able to scrap but with this code its not returning the text...instead its just tttt...i guess its in table format? How can i scrap this as a text or as a readable formatt? 
This is my code in the scrapy console..
In [53]: response.css('div.description::text').get()
Out[53]: '\n\t\t\t\t\t\t\t\t\t\t\t\t\t'",1,2023-10-25 20:35:43
Is it legal to scrape job boards,I am planning to scrape job boards like indeed for personal use. I only want the public data and not using a login for the websites. I see that most websites say it is illegal to do so but I am not planning on selling it or make profit from it. I might open source the code though. Will it be risky for me to do so legally speaking?,5,2023-10-25 12:35:45
Web scraping r/wallstreetbets for stock trend and sentiment analysis,"Hello,  web scraping experts!
I am working on a stock trend and sentiment analysis project using data from r/wallstreetbets. However, I am having a tough time figuring out how to scrape the data efficiently.
I have some experience with web scraping, but I am new to scraping Reddit data. I am particularly interested in learning how to:

Extract the stock tickers mentioned in each post
Calculate the sentiment of each post (positive, negative, or neutral)
Gather data for a large number of posts efficiently

I am using Python for my project. Any tips or advice would be greatly appreciated.Thank you in advance!",5,2023-10-25 05:08:28
How to find list of all the companies in Germany which used Magento to build their website.,Which tool would be helpful?,1,2023-10-25 10:04:17
Instagram/tiktok comment scraping legal?,"Iâ€™m working on a project for my uni , before getting into
It , I would like to know if itâ€™s legal to scrap data on Instagram pages/profiles , comments etc using a scraping bot ..then using a chatbot to comment/ reply to posts etc",1,2023-10-25 07:48:42
Is VPN good for scraping on a small scale for general protection?,"VPN services are unsuitable for large-scale scraping since being blocked is easy. But I'm scraping government sites that are publicly available at a small scale, I'm not making 100K requests or anything like that. I would like to use a VPN just for peace of mind. My IP and requests have not been blocked before but I'd still like to use a VPN for peace of mind.
Would this be a suitable situation to use a VPN service? Would the VPN service block me?",6,2023-10-24 21:57:58
scraping tiktok,"i'm building an ios app that needs to get the transcript and description of a tikotk video, given a link to the video. what should my approach be? is it possible to use a selenium script and not face any limit issues? 
thanks",3,2023-10-24 20:26:56
How to get a list of all public videos recently uploaded on YouTube?,"We can search for videos on YouTube by using search engines (not just Google), meaning that they must have a way to get a list of videos on YouTube. How do they do that? I looked at https://youtube.com/robots.txt, and there are some sitemaps listed. However, I do not see any sitemap that lists all videos on YouTube.
I want to do this because I think the recommender system of YouTube sucks and I want to write my own.",2,2023-10-24 16:52:47
Webscraping website with embedded mapbox,"Hi
I am looking to get the lat long of the on-net building in the embedded map in this website:
https://www.lumen.com/en-us/resources/network-maps.html
Is this doable?
Thank you!!",1,2023-10-24 20:29:52
Best way to export Instagram followers?,"I'm not sure this is the best place to go for this info. I have a paid service for exporting followers. All I need is an active account and it works fine. But my biggest issue is I have an IP ban on instagram and cannot create new accounts. I also use a paid proxy service but I still keep getting the IP ban even when using a fake IP address. 
Does anyone know a better way to export followers without getting banned? If not, does anyone know a way to create a large amount of instagram accounts without getting an IP ban message?",2,2023-10-24 04:44:01
save 3d models,"Hi, so for a project I'd need to download some of the buildings shown on the website here. Is there any way to download separate buildings? thx
https://www.wien.gv.at/stadtplan3d/#/",1,2023-10-24 07:01:31
Need help with my code,"Hello,
I just got a course from a lead generator and they were teaching about scraping through linkedin there.
It had 3 ways and none of them are working for me.
One worked on the first day but its showing an error now.  

I tried to scrap linkedin group members from the code but it always gives an error. I tried it on different accounts, browsers and computers but its still not working.
Please help me with it.
I have attached the error screenshotÂ below.

â€‹
https://preview.redd.it/gcgl89th43wb1.png?width=522&format=png&auto=webp&s=1725e71d817bc74ec2030c9dd87a8ff6b84a8048
here is the full code -
// Change this
const MAX_USERS = 100;
const loadMore = () =>{
const loadNodeBtn =
document.querySelector("".scaffold-finite-scroll"").querySelector(""
.scaffold-finite-scroll__load-button"");
if(loadNodeBtn ){
loadNodeBtn.click();
}
}
const scrollToBottom = () => {
window.scrollTo({
top: document.body.scrollHeight,
behavior: ""smooth"",
});
}
const jsonToCsv = (jsonData) => {
const keys = Object.keys(jsonData[0]);
const csvRows = [];
csvRows.push(keys.join("",""));
for (const row of jsonData) {
const values = keys.map((k) => row[k]);
csvRows.push(values.join("",""));
}
return csvRows.join(""\n"");
};
const downloadCsv = (csv) => {
const blob = new Blob([csv], { type: ""text/csv"" });
const url = window.URL.createObjectURL(blob);
const a = document.createElement(""a"");
a.style.display = ""none"";
a.href = url;
a.download = ""data.csv"";
document.body.appendChild(a);
a.click();
window.URL.revokeObjectURL(url);
};
const delay = (ms) => new Promise((resolve) =>
setTimeout(resolve, ms));
const scrapperData = async () =>{
let data = [];
let nodes = document.querySelectorAll("".artdeco-list__item"");
let totalGroupMember =
document.querySelector("".groups-members-list"")?.querySelector("".p
t1"")?.innerText?.split("" "")[0]?.replaceAll("","" , """");
while(true){
if(nodes.length < MAX_USERS && MAX_USERS <=
totalGroupMember){
let prevNodes = nodes;
scrollToBottom();
nodes =
document.querySelectorAll("".artdeco-list__item"");
if(prevNodes.length === nodes.length){
loadMore();
await delay(3000);
}
}
else{
break;
}
}
nodes = Array.from(nodes);
nodes?.slice(0,MAX_USERS)?.forEach((node) => {
let userName =
node.querySelector("".artdeco-entity-lockup__title"")?.innerText;
let connectionLevel =
node.querySelector("".artdeco-entity-lockup__badge"")?.querySelecto
r("".artdeco-entity-lockup__degree"")?.innerText?.slice(2);
let profileUrl =
node?.querySelector("".ember-view"")?.href;
let headline =
node?.querySelector("".artdeco-entity-lockup__subtitle"")?.innerTex
t;
data.push({
name : JSON.stringify(userName),
connectionLevel : JSON.stringify(connectionLevel),
profileUrl : JSON.stringify(profileUrl),
headline : JSON.stringify(headline),
});
})
return data;
}
scrapperData().then((results) => {
console.log(
`Scraping Completed! Downloading CSV...`
);
const csv = jsonToCsv(results);
downloadCsv(csv);
});
â€‹

I tried to scrap data from search results but its not working either.
It goes on, scrapes everything, but fails to download.

â€‹
https://preview.redd.it/1tcwgf4253wb1.png?width=535&format=png&auto=webp&s=3293be898cf2ef28c6f2a2fb61e12ded99531db6
â€‹
here's the code -
let allProfiles = [];
let page = 1;
window.scrollTo(0, document.body.scrollHeight);
await new Promise((resolve) => setTimeout(resolve, 500));
// it will click on the next button
let nextButton =
document.querySelector('button[aria-label=""Next""]');
const peeps = getProfiles();
allProfiles.push(...peeps);
// is the nextButton disabled?
let isNextButtonDisabled = nextButton?.disabled;
while (!isNextButtonDisabled) {
console.log(""page"", page);
let randomDelay = Math.floor(Math.random() * 3000) + 1000;
console.log(`Waiting ${randomDelay}ms so you don't get flagged Ã°
Â¤Âª`);
await new Promise((resolve) => setTimeout(resolve, randomDelay));
window.scrollTo(0, document.body.scrollHeight);
await new Promise((resolve) => setTimeout(resolve, 900));
const people = getProfiles();
allProfiles.push(...people);
nextButton = document.querySelector('button[aria-label=""Next""]');
isNextButtonDisabled = nextButton?.disabled;
nextButton?.click();
page++;
}
// dedupe the profiles
allProfiles = allProfiles.filter(
(v, i, a) => a.findIndex((t) => t.linkedinId === v.linkedinId)
=== i
);
// it will show the number of profiles scraped
console.log(`Congrats! Ã° You just scraped ${allProfiles.length}
profiles!`);
const ts = new Date().toISOString();
const fileName = ""linkedin-profiles-"" + ts + "".csv"";
convertJsonToCsvAndDownload(allProfiles, fileName);
}
function convertJsonToCsvAndDownload(jsonData, fileName) {
// Convert JSON to CSV
const csvData = [];
// Extract the headers
const headers = Object.keys(jsonData[0]);
csvData.push(headers.join("",""));
jsonData.forEach((item) => {
const row = [];
for (const key in item) {
if (item.hasOwnProperty(key)) {
const value = item[key].includes("","") ? `""${item[key]}""` :
item[key];
row.push(value);
}
}
csvData.push(row.join("",""));
});
// Create a Blob containing the CSV data
const csvBlob = new Blob([csvData.join(""\n"")], {
type: ""text/csv;charset=utf-8"",
});
// Create a URL for the Blob
const csvUrl = URL.createObjectURL(csvBlob);
// Create a link element
const link = document.createElement(""a"");
link.href = csvUrl;
link.target = ""_blank"";
link.download = fileName;
// Append the link to the body
document.body.appendChild(link);
// Trigger a click event on the link
link?.click();
// Remove the link and revoke the Blob URL
document.body.removeChild(link);
URL.revokeObjectURL(csvUrl);
}
function getProfiles() {
const allPeeps = [];
const listOfProfiles =
document.querySelectorAll("".entity-result"");
for (let i = 0; i < listOfProfiles.length; i++) {
const el = listOfProfiles[i];
const spanElement =
el.querySelector("".entity-result__title-text"");
const linkedinId = el
.getAttribute(""data-chameleon-result-urn"")
?.split("":"")?.[3];
// Extract the person's name
let name = spanElement
.querySelector('span[aria-hidden=""true""]')
?.textContent?.trim();
const textRegex = /[A-Za-z0-9\s]+/g;
const textMatches = name?.match(textRegex);
if (textMatches) {
// Join the matches to get the extracted text
const extractedText = textMatches.join("""");
name = extractedText.trim();
}
const title = el
.querySelector("".entity-result__primary-subtitle"")
?.textContent?.trim();
const location = el
.querySelector("".entity-result__secondary-subtitle"")
?.textContent?.trim();
// Extract the LinkedIn profile URL
const linkedinProfileUrl = spanElement
.querySelector(""a.app-aware-link"")
.getAttribute(""href"");
allPeeps.push({
linkedinId,
name,
title,
location,
url: linkedinProfileUrl?.split(""?"")?.[0],
});
}
console.log(`Found ${allPeeps.length} profiles!`);
return allPeeps;
}
await scrapeLinkedinSearch();
â€‹

I tried a method by making an app script in google sheets which extracts data from linkedin using designation and company name. but its giving an error when i save the app script.

â€‹
â€‹
https://preview.redd.it/zio69ixb53wb1.png?width=897&format=png&auto=webp&s=86fffd457a77477c2393d30d7db77f5f273fb991
â€‹
/**
* Find a Linkedin profile from company name and job title
*
* u/param {string} companyName company where your prospect is
working
* u/param {string} jobTitle job you are targeting
* u/return if found the linkedinURL + Name of the prospect
* u/customfunction
*/
function getPerson(companyName,jobTitle) {
// Get a Custom Search API Key
// follow this link to get it
https://developers.google.com/custom-search/v1/overview
var key=""YOUR CUSTOM SEARCH API KEY""
// Create a Programmable Search Engine
//follow this link to create it ->
https://programmablesearchengine.google.com/
let searchEngineId = ""YOUR SEARCH ENGINE ID""
// Save your project and it's ready to use
let search = ""site:linkedin.com/in intitle:""+jobTitle+""
""+companyName
// Call Google Custom Search API
var options = {
'method' : 'get',
'contentType': 'application/json',
};
response =
UrlFetchApp.fetch(""https://www.googleapis.com/customsearch/v1?key
=""+key+""&q=""+search+""&cx=""+searchEngineId, options);
// Parse linkedin URL and Name
let url = JSON.parse(response).items[0].formattedUrl
let title = JSON.parse(response).items[0].title.split(""-"")[0]
// display the results in 2 columns
var results = new Array(1);
let info = new Array(2);
info[0]=url
info[1]=title
results[0]=info
return results
}
â€‹
â€‹
Thank you in advanced. Please help me if possible. Its quite urgent.",1,2023-10-24 05:23:14
Sort By New Items?,"Hello there, I'v found a website that get items randomly- Currently it is in BETA and the only 2 ways to sort is by price high to low or low to high. I'm trying to see if there a way to make a script to sort by when the item was listed. Like sort by Newly listed.",0,2023-10-23 22:52:28
Scrap Entire r/reddit?,"Is there a way for me to scrap the entire a specific reddit community's threads and posts into .txt or .csv painlessly?
Reason: I prefer to look for keywords / posts to navigate data using my own software.
P.S: I have no programming knowledge.
Thanks for the help!
https://preview.redd.it/bxu6wi1a9wvb1.png?width=1005&format=png&auto=webp&s=8113f41245ac662a20ec46613ab556a8bc890842",9,2023-10-23 06:12:32
Legal risk from scraping?,"I just learned about robot.txts and that it may be against website policy to scrape it, if we do it anyway can we get punished legally?
I have been scraping the same page at weeks interval for a year and they just reached out to me with threats",3,2023-10-23 08:44:05
Python web scraping video?!,Am looking for a youtube video and i understand there are loads but thats the problem. I want to scrap a website which has products title and link. You click on the link and it takes you to the products main page where i want to scrap product info and description and part number...so really need a video explaining how to build a scraper that will go to each links and scrap the necessary information..is there a specific youtube video which explains this?!,2,2023-10-22 20:15:57
Pytrends 429 Error max limit,"Hi, Google trends scrapping is not working using pytrends not sure what was the reason, can someone suggest alternative way to do that. https://github.com/GeneralMills/pytrends/issues/602",1,2023-10-22 14:18:15
Botasaurus feedback?,"Hello there! I'm the creator of a Botasaurus, a web scraping framework. I've put a lot of effort into it, and I'd greatly value your insights and feedback. If you're interested in discussing it further, please book a meeting with me at https://www.omkar.cloud/l/meet-chetan/. I look forward to connecting with you!",2,2023-10-22 09:19:14
Help with scraping,"Hi
I recently have started learning javascript, and decided to contribute to a repo (i learn by just picking start doing)
Project has a Extension ans stuff which uses js to scrape and stuff and then show it 
I need help on this part https://github.com/shashankx86/repo_miru/blob/main/repo/mangakatana.com.js#L115
what is does is scrapes img to show
you can take this page as reference https://mangakatana.com/manga/the-hundred-ghost-stories-that-led-to-my-death.26032/c59
and can also take reference to similar Extension
https://github.com/shashankx86/repo_miru/blob/main/repo/asuratoon.com.js
https://github.com/shashankx86/repo_miru/blob/main/repo/isekaiscan.to.js
any help would be grate 
if you can provide code it would be more grate
Thank you adv",0,2023-10-22 07:27:01
Morning webscraping,"Just woke up and thought Iâ€™d continue working on my web scraping project on this beautiful Saturday morning. Iâ€™m using Python and Selenium to get listings for a used iPad. If youâ€™re working on your own web scraping projects, Iâ€™d love to hear about them. What challenges have you encountered? Any tips to share? 
Also, if youâ€™re interested in joining a Discord call to brainstorm, troubleshoot code, or just chat about programming, feel free to reach out. Itâ€™d be fun to chat with fellow webscrapers",5,2023-10-21 16:33:31
Is there a way to copy and paste all discord DMs with a user into a server?,Trying to collect the messages of a friend of mine who has many alt accounts into one place. How could I go about this?,1,2023-10-21 14:11:46
Puppeteer Extra Amazon Captcha (PoC),A proof of concept that uses Tessaract.js to solve Amazon text-based captchas.,3,2023-10-21 06:45:08
"Can't scrape e-commerce site with filter (price, latest, top sales, etc) but can without them","I can scrape this link : https://shopee.ph/Mobiles-Gadgets-cat.11021712 
but not this:  https://shopee.ph/Mobiles-Gadgets-cat.11021712?locations=Overseas&minPrice=100&page=0&sortBy=sales as it always results in an empty string
and the only difference is that I put search filters in the second one.   
the code I'm working with is:
try:
            for url in url_list:
                # Open the URL in the browser
                self.browser.get(url)
                time.sleep(10)

                self.i = 0
                self.scroll_page()

                # Create a BeautifulSoup object to parse the page source
                soup = BeautifulSoup(self.browser.page_source, 'lxml')
                print(soup)

                # Find all product items on the page

                product_items = soup.find_all('div', {'class': 'col-xs-2-4 shopee-search-item-result__item'})

                for item in product_items:
                    # Extract product data using the defined method
                    product = self.extract_product_data(item)
                    product_data_list.append(product)

                # Sleep for 60 seconds before processing the next URL
                #time.sleep(60)

        except Exception as e:
            print(f""Error while scraping {url}: {str(e)}"")

Any ideas?",3,2023-10-21 04:56:47
Freelance roles,"Hi, are there any other platforms other than Upwork to find webscraping gigs? I've applied for several roles and still haven't received a response.",8,2023-10-20 10:40:29
Scraping https://www.msn.com/en-us/feed,"When I scrape  https://www.msn.com/en-us/feed I get html that includes the following:  Your current User-Agent string appears to be from an automated process, if this is incorrect, please click this link:<a href=""http://www.microsoft.com/en/us/default.aspx?redir=true"". How do I get past this? Should I try to make the automated process click the link or would that not work? FYI I'm just a humanities undergrad trying to do a little project so it wouldn't be overloading Microsoft's servers or anything.",5,2023-10-20 08:29:50
Dynamic webscraping,"Greetings y'all, I need some help, I'm dynamically scraping a website you see, not the entire site though, just a <ul> my code is set to check the ul every minute and save the length of the ul, if the length changes it takes the latest li that is added, all this is just done in pure python, the scraper is obviously selenium based, my question is is the a way to do the whole process with Selenium, the way I'm currently doing it does always work",0,2023-10-20 10:04:00
Simple methods to scrape product images from search results/links on ordinary e-commerce sites?,"Hi, I was wondering if there are simple or tried-and-tested methods to scrape product images from ordinary (not mainstream) e-commerce sites?
For background, I'm looking into e-com sites where:

You can search for products according to categories/tags which leads to a generic search results page.
Search results page url contains the category tag/id and page number.
Clicking on X product will bring you X product's main page where there are product photos of each angle, where you can get its direct link if you open it in a new tab. These are the photos I want to scrape.

I'm more interested in the general process or concept of the methods.. I'm guessing since all you need are the LINKS, hopefully someone could help point out the common direction to gather these links for image scraping.
I'd like to know if can be done with little to no code or at least the confirm the fact that built in tools already exist so I don't have to reinvent the wheel.
TIA!
â€‹",1,2023-10-19 21:18:32
Shopee scraper using Python not working in other countries,"Using a working python code that scrapes a shopee webpage and it works if I use the shopee page that is the same as my country. However, I can't seem to scrape those in other countries like SG or MY. Any way to use proxies to circumvent this? Using selenium+soup+undetected_chromedriver",1,2023-10-19 21:00:17
Need help with SeleniumBase and Cloudflare verification.,"Hey guys, I got a problem that I find myself unable to solve. Right now, I'm interested in scraping three websites : sushiscan.fr, sushiscan.net and anime-sama.me .
I'm creating a script to download mangas scans and store them locally, so I can actually read them without all the annoying ads, and more specifically merge everything chapters of a manga into a single pdf file. The merging into pdf part doesn't cause any issue, and I can, thanks to SeleniumBase and cloudscraper librairies, get my stuff to work with sushiscan.fr (not protected by cloudflare), anime-sama.me (protected by cloudflare), but no way to make it work with sushiscan.net : it gets stuck on the cloudflare verification challenge. I tried following the tutorial at https://github.com/seleniumbase/SeleniumBase/tree/master/examples, and still no way to make it work. One interesting thing though : the script does work when connected to my local university wifi network, but no way to make it work on my own personal wifi. I talked about it with an the IT lead from it, and he told me there was no special configuration for the network that would explain how it gets through cloudflare's challenge. Would any of you have an idea to help me ? I don't have a VPN either, which could be a track to follow if we could get it work. 
Thanks y'all !  
Edit : after testing with ProtonVPN, it doesnt work either.",3,2023-10-19 09:19:07
Best way to web scrape Amazon reviews and ratings (stars)? Issues with multiple pages,Iâ€™m using the scrapy package in python to mine amazons reviews but Iâ€™m having an issue where it will not scrape multiple pages even if my urls are correct. Has anyone done this before who could help me decided what package to use and any examples please? Iâ€™m so lost thank you!,1,2023-10-19 15:15:42
Are there any websites which are unpossible to scrape?,Are there any websites which are unpossible to scrape?,3,2023-10-19 07:27:51
Need Help Improving Web Scraping Speed & Process,"Objective:
Scrape a specific product's webpage frequently (1-2 times per second) to detect new adverts. Once detected, notify my client via Telegram.
Current Setup: 
- Using Axios + Cheerio for scraping.
- Employing a residential proxy.
Issue: 
When my client gets the Telegram notification and opens the advert link, the phone number is hidden behind a button which, when clicked, executes JS code to reveal it. A competitor often calls the seller before we can. 
Concern: 
I'm aware that using Puppeteer or Playwright might slow down the process.
Possible Solution: 
Considering building a mobile app that allows the client to call immediately upon detecting a new advert.
Query: 
1. How can I optimize the advert detection process?
2. What's the best way to quickly access the phone number on a newly detected page, considering the JS button hurdle?
Your insights would be invaluable!",1,2023-10-19 11:30:08
Number of restaurants in europe.,Hello. I have a question. Is it possible to find number of restaurants in europe registered in google places api?,1,2023-10-19 10:37:28
Can't figure out how to make this work...,"So, been trying to make a super simple web scraping program, just to collect a few blog post texts (it's for a data science project, I'm a student).
I have the directory, but on my main py file, looks like this:  
import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_blog_for_keywords(url, keywords):
    try:
        response = requests.get(url, verify=False)
        response.raise_for_status()  # Check if we got a successful response
    except requests.RequestException as e:
        print(f""Error fetching {url}: {e}"")
        return []  # Return an empty list on error

    soup = BeautifulSoup(response.content, 'html.parser')
    post_elements = soup.find_all('a', class_='post-title')

    matching_posts = []
    for post in post_elements:
        title = post.text.strip()
        link = post['href']
        if any(keyword.lower() in title.lower() for keyword in keywords):
            matching_posts.append({'title': title, 'link': link, 'source': url})
    return matching_posts

if __name__ == '__main__':
    urls = [
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
        'obviouslyanurlishere.h',
    ]

    keywords = ['example', 'example', 'example', 'example example example', 'example example', 'example', 'example', 'example', 'example']

    all_results = []

    for url in urls:
        all_results.extend(scrape_blog_for_keywords(url, keywords))

    df = pd.DataFrame(all_results)
    df.to_excel('keyword_matching_blog_posts.xlsx', index=False)

    print(""Scraping completed and saved to 'keyword_matching_blog_posts.xlsx'"")

Going crazy here, because the scraping program runs, but the excel file always comes out empty... Anyone?",1,2023-10-19 09:28:59
Playwright cloud,"Hello. Any suggestions where to run cheap, 2 playwrights .js files, every 15sec? GitHub Actions bit pricey with so many intervals.",1,2023-10-19 09:19:49
Discord Scraper,Is there a Discord scraper that will allow us to scrape public messages by users including threads?,0,2023-10-19 07:01:19
"I don't understand why I can't scrape this site, or honestly what tf is going on with this site anymore. Someone PLEASE help.","Hey, I build simple web scrapers that scrape websites and integrate to other data sources so people have all their data in one place. This site is either blocking scraping or I'm missing something else but I can't find any answers.  
The Robots.txt page returns a 404.
The site code doesn't look like any other site I've ever inspected.
Thats all I have for you.
â€‹
https://9038876.onlineleasing.realpage.com/",1,2023-10-19 00:15:41
How to get API for getting sms?,How to get api for getting sms messages on this site https://www.phoneblur.com,1,2023-10-18 18:49:27
Web-Scraping a subreddit up to the 1000 limit? After PRAW changes.,"Hi! I'd like to scrape a subreddit. Due to recent API changes, I'm not sure how it's done now. I'm okay with getting only the latest 1000 posts as the limit is.
Â 
What's the current method? Any code on github or something similar? Thanks!",3,2023-10-18 11:57:29
Unable to scrape target with BS4 and Selenium. Is it me or a curse?,"I scrape a site and now I am doomed. (Hopefully, because I'm a newbie). I want to scrape the data-score of the span in the highlighted div. (In this case ""5"")
â€‹
How to target the data-score? (Which is 5)
I did the following in BeautifulSoup: (rating_parents is a list with all elements whit: div class = index_factor_Mo6xW
for rating in rating_parents:
            label = rating.find('h4', class_='index__title__Rq0Po')
            test = rating.find_next('div', class_='index__block__7hodp')
            print(label.text)
            if test is not None:
                #data_score = test.get('data-score')
                data_score = test.find('span', class_='index__stars__nfK6S')
                print(data_score.get('data-score'))
            else:
                print('none')

This produces this result:
Es geht immer besser aber auch deutlich schlechter #print(rating_title)
Work-Life-Balance # print(label.text)
4 #data-score print(data_score.get('data-score')
Karriere/Weiterbildung
4 #print(data_score.get('data-score'))
Why is it 4 and not 5 as in the DOM (image above) ?
A friend of mine said: Yeah, its dynamically loaded with JS. 
Both of us started trying to get the data-scores with selenium (which we are even newer to than to bs4) and used the xPath instead of HTML. When targeting the element (the span) directly with the xPath the object is not found. (Unable to locate element) 
This is the xPath:
/html/body/div[3]/div/div[1]/main/div/div[4]/div/article[2]/div/div/div[4]/div/span
I did:
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

options = webdriver.FirefoxOptions()
options.add_argument(""--headless"")

options.page_load_strategy = 'none'

s = Service('./geckodriver')
driver = webdriver.Firefox(service=s, options = options)

driver.implicitly_wait(5)

url = 'https://[THE WEBSITE]'
driver.get(url)
time.sleep(20)

Because I get the unable to locate message, I wanted to check until which point the xPath works. Until this (bold):
/html/body/div[3]/div/div[1]/main/div/div[4]/div/article[2]/div/div/div[4]/div/span
The /div/span is not detected. 
The node of the bolded part in above xPath looks like 
<h4 class=""index\_\_title\_\_Rq0Po"">Work-Life-Balance</h4><p class=""index\_\_plainText\_\_JgbHE"">Some string. Some string</p> 
When doing:
html = article.get_attribute('innerHTML')
as you see, it get's pretty much all of the div â€“Â except the div inside it and so also not the span inside it. 
This part:
Why do I don't get that?
So why can't I target the data-score with Selenium? And why do I get results in BS4 which differs from what is actually going on in the DOM?
Sorry for this verbose question. I hope everything is clear and I don't miss important points. 
Thank you for your time, and me (and now my friend too) lit a candle for you, when you can enlighten us. 
Cheers, 
â€‹
â€‹
â€‹",2,2023-10-18 12:56:36
Building a PC for scraping. Which part of hardware is important? RAM? CPU? any other parts?,"Gonna build a scraping desktop but I don't want to spend extra money on the unnecessary parts.
I hope the desktop computer can run around 200 scrapers(Python Playwright) simultaneously 24/7. I may run each scrapers in separate Docker containers.

Which part of hardware is important?  I guess RAM? 
If RAM is important, I will buy 64gb RAM. Is 64gb enough for my use case?
If CPU is also important, what kind/level of CPU should I consider for my use case?
I run the scrapers in separate containers because I guess it is more stable to run them in isolated environments. I tried to run around 10 scrapers (Python Selenium) in my laptop without Docker, and the laptop crashed and restarted after a while. Will running scrapers in separate containers make the system more stable? or should I run the scrapers without Docker to save some computational resources?",1,2023-10-18 12:14:24
Extracting Simple Twitter (X) Data From a User,"I am doing an analysis project of an individual's Twitter (X) posts ranging all the way back to 2010. All I want to do initially is get a data sheet containing the date, content, and URL for each one of their tweets. Is there any way to automate this process? I tried using Twint and a few other scraper programs, but my understanding is that X has disabled their ability to work. Any ideas on how I can make this process a little easier?",2,2023-10-18 02:40:49
TikTok Data Scraping,"Hi! I want to build a bot which can send me a notification whenever someone posts a new video on TikTok. My best guess would be to scrape the data from the web URL, but how do I specify to start an action whenever a new video is appearing.
Thanks in advance",1,2023-10-16 13:03:34
Idea for Reddit/Twitter Scraper,"I'm working on a web scraper tool focused on social media - Reddit, Twitter, etc. It would analyze mentions of keywords to provide market insights like rising trends/interests and lead generation by finding prospective customers talking about the problem your business solves and automatically notify and/or respond to them. Would brands/developers be interested in this data for market research and analysis? What platforms and use cases could benefit most from targeted social listening?",2,2023-10-15 11:17:39
Playwright/github help,Hello. Is there anyone who can help me to setup the playwright core to GitHub actions? Thanks in advance!,1,2023-10-15 17:09:16
Selenium - Iterate with click and back trough elements with links?,"Hello - i have the following problem using selenium -
I have a page with several links to click to get to some detail-informations (with the same url)
I get all the elements with this statement:
elems = driver.find_elements(By.XPATH,'//a[contains(@href, ""auswaehlen"")]')

Now i want to iterate trough this elements, click on each element, parse and work with all data using beautiful soap bs4
but evertime when i do a
driver.back()

to get back to the initial overview of all links and then click the next element i get an error-message / the program crashes with the information that it canÂ´t find the element
Is there any way to iterate trough this elements, element by element using click and the back-funcionality as seen above?",1,2023-10-15 08:24:51
Web Crawling at Scale: Navigating Billions of URLs with Efficiency,"Hey r/webscraping! I just finished a coding project where I built a distributed web crawler inspired by Google's architecture. I wrote a detailed series tutorial explaining the design decisions and implementation:
Part 1 - Architecture overview: https://medium.com/p/7f4281f9f539
Part 2 - Code walkthrough: https://medium.com/p/7a9b9a1e3829
The full source code is available on GitHub: https://github.com/tonywangcn/distributed-web-crawler
This was a super fun project where I learned a ton about building large-scale distributed systems. Let me know if you have any feedback on the code or tutorial! I'm hoping this can be a useful resource for anyone wanting to learn more about distributed web scraping.",39,2023-10-14 12:27:41
Need help to determine how to host scrapers online.,"Hi All,
I am a coding/dev noob and have 3 .exe web scrapers that take about 5 hours to run and a GitHub .py script that I run on a Heroku bot. I'm looking to see if there is a budget-friendly option to host all 4 to run daily on the cloud. Currently running the .exe scrapers on my local machine. I was suggested to run a Windows VPS.
Requirements: 
-Need to run the 3 .exe daily at a set time.-
-Py script needs to run during specific hours during the day. About 8 hours a day.
â€‹
Can anyone recommend the best solution to host all of these? This is just a side hobby of mine, and I don't have a huge budget, but I will spend some.
â€‹
Thank you!",3,2023-10-14 17:10:30
Playwright cloud,What is the cheapest method to run 2 playwright ts files in the cloud? Any suggestions?,1,2023-10-14 09:57:27
You guys have jobs?,"hey is there anyone because they were really great at scraping hired by tech companies?
â€‹
I dont see a lot of linkedin job posts in this space was wondering why it could be?
â€‹
EDIT:
As far as I understood companies prefer to buy data and not hire for web scrapers type of jobs.
But then how do you know if theres a type of data companies are interested as an outsider so you can yield that and sell to them?
Feel like there's a gap especially if you dont have contacts in industry.
Sometimes freelancer works i guess here and there..
But I feel like there should be a website a small business come search for data they are interested and see if theres someone yielded that data and make an offer..
(apify exists but it lets you use some scraper which still requires somewhat a tehcnical knowledge normal non-tech people are directly interested in data in format they want)",9,2023-10-13 14:50:19
The Architecture of a Web Crawler: Building a Google-Inspired Distributed Web Crawler. Part 1,"Learn to build a scalable, Google-inspired distributed web crawler from scratch. This in-depth tutorial covers designing a resilient crawler architecture, leveraging technologies like Kubernetes, Golang, NodeJS, Redis, MongoDB, Prometheus, and more. Gain hands-on experience constructing an intricate system capable of gathering web data at scale. This series walks through setting up a robust dev environment, establishing a modular project structure, and explaining the rationale behind each component. Upcoming posts will dive into the technical implementation of core elements like worker nodes, message queues, data storage, logging, and monitoring. Whether you're a developer looking to expand your skills or a business seeking a competitive edge through web data, don't miss out on this valuable resource. Follow along with the series on GitHub (https://github.com/tonywangcn/distributed-web-crawler).
Read the first part here: https://medium.com/@tonywangcn/the-architecture-of-a-web-crawler-building-a-google-inspired-distributed-web-crawler-part-1-7f4281f9f539
Welcome to check out other writings about building crawlers:

How to efficiently scrape millions of Google Businesses on a large scale using a distributed crawler https://medium.com/@tonywangcn/how-to-efficiently-scrape-millions-of-google-businesses-on-a-large-scale-using-a-distributed-35b9140030eb 
How to efficiently scrape millions of Google Businesses on a large scale using a distributed crawler https://medium.com/@tonywangcn/a-step-by-step-guide-to-building-a-scalable-distributed-crawler-for-scraping-millions-of-top-tiktok-802a7d754e7e",11,2023-10-13 09:51:11
App for one stop shop,"Hello I have currently been looking into web scraping for various reasons. I am having a hard time finding a service that does well rounded tools.
Very simplified idea.
A for browser, phone android or Apple or application for windows and Mac.
That could plug and play.
Put In site then gives you a list of pages from the sitesmap.xml. 
With elements to select to scrape. Then goes through and makes a copy of text, video pictures etc. 
For local download with automated or manual file conversion of video, pics or text into various formats.
Also be able to copy the entire layout of the webpage to then view on the app offline.
Virtually no coding just plug and play and select what you want. I think everybody gets the idea of a one stop shop tool without any coding. 
Also will be able to monitor social media accounts to scrape things as they come. Also gather data from discord, telegram etc. 
Does something like that exist?
If it doesnâ€™t I am considering building it myself. Current software engineer (mostly backend) I have worked on some big applications with fang so I have the experience in the entire tech stack. 
Was considering using flutter to make it easy for an app. Then use Java for backend (most of my experience). Also I say app so the hardware is your local with minimum backend work to just feed new IPâ€™s. Will help keep cost low. Also features that come to mind having it local would give the user more freedom.
Donâ€™t want to waste my time making it if there is a tool that does pretty much that. Iâ€™ve searched just canâ€™t find anything that makes it simplified with no code. If anything copy paste.
Iâ€™m new in the sense of researching web scraping to do something with it. So please excuse my ignorance if I left out any info that would be a hold up etc.
If I made this do you think enough people would want the same thing? Any problems people wish were solved with tools? Could give me an idea of problems I may run into to avoid. Considering making it open source.",1,2023-10-13 19:16:07
Help please.,"Hello guys, I need help with web scraping from Twitter and Facebook for research purposes knowing that I am not a python developer, iam front end developer, What to do to extract data from those sites, or If you can send me extracted data maybe ..",1,2023-10-13 12:54:59
Raven: Proxy Lister and Tester,"So i was working on this personal project, and encountred this problem; I needed a massive list of proxies for scrapping, obviously public so i will manage the rotations, and found a couple of good lists on github, but the process got tedious and tiring especially when i had to test the proxies against the custom target websites i was working on so i created this tool: Raven.
Raven is a proxy lister and tester that could get all the proxies you need for you(obviously public ones),
for now it gets only HTTP proxies, and it could test the proxies for you on custom targets of your choice. It is multithreaded so it could get through 6K proxies in around 1 to 2 mins.
It is open source and i will keep working on it, since i am still working on the main project.
You can both use it as a library in your code, or as a docker image as a fast tool with various flags.
I tried to document the need to know parts, and will probably improve it in my free time.
Open to feedback and PRs.
Github Repo: https://github.com/HatemTemimi/Raven",3,2023-10-12 20:39:16
"is it okay to put ""The name of this Reddit page"" on your resume to describe that you did this at your job?","is it okay to put ""The name of this thread"" on your resume? (BTW I have to say ""The name of this thread"" or it is taken down)
If not, why?",1,2023-10-12 21:00:36
US-based SERP APIs with DPA?,"Hello,
Does anyone know of a US-based SERP API that has a Data Protection Agreement out-of-the-box? By US-based, I mean all infrastructure is hosted in the US and data cannot be transferred/accessed outside the US.
I've seen some companies that only offer a DPA for enterprise plans and others that have a standard DPA but aren't US-based.
We're just starting out, and though we're growing quickly, we really only need 50k queries/month. We don't have enough volume to justify paying for an enterprise plan.
Any pointers would be appreciated!",1,2023-10-12 19:20:40
Newby wants to scrap Berlin for newly openend restaurants,"Hey guys, sorry to dig out this old thread. 
Maybe you guys can help me. I want to scrape google for all newly openend restaurants in Berlin every 2 weeks. It's about 30 new openenings. All I need is actually the name of the restaurant and the adress. 
Which tool would you recommend? 
We could scrap on google maps or google business page I assume.
I do not need a CSV or anything fancy to evaluate the results. 
Really appreciate your help. If anyone is based in Berlin I would buy the first round of beers ;)",0,2023-10-12 11:27:21
"Remaining undetected, no proxies.","I scrape and automate on a site that requires a user's SSN to create an account, and to log in to use, so  I assume that using proxies or changing the header won't help me (if I'm wrong about this, please let me know).  This is a significant source of income for me, and if I get banned, it's over.  I've already received a warning for breaking the site rules in a different way (unrelated to botting).  I've used the community discord bot to check and they pretty much have no anti bot technology implemented.  The site has hundreds of thousands of users, but they don't seem to try to prevent it from crashing or malfunctioning during peak usage days.   How long should I wait before refreshing for real time updates?  I currently have my bot wait anywhere from 5 seconds to 1 minute, depending on if it's currently a high or low traffic period of the day.  I've been botting on the site for about a month so far, and haven't been caught, but I'm still nervous, especially since I want to reach out to other users of the site to sell my bot.  Are there any dead giveaways that I should avoid doing? Like if I access a dropdown before the entire page loads?  I can code bots just fine, but I'm still new to any theory related to this.  I want to know if there's a general rule of thumb or a more specific way of remaining undetected.  I'm using Python Selenium Chromedriver, if that helps.",2,2023-10-12 06:40:54
(With headers) Can't GET SEC endpoint,"Hello, i have been using the SEC API for a couple weeks now, and recently i got an error saying that i was identified as an automated tool and that i needed to use the headers as suggested in the docs. So, i went to the documentation, set the headers 'User-Agent, Accept-Encoding, and Host' for my axios requests but they produced a 404 error, even though the url works in my browser. I then tried to debug and go to postman with the same headers, yet it still gives me the 404 error. Any help is greatly appreciated.",2,2023-10-11 20:35:28
QUESTION: Non technical doofus seeks help scraping,"Hello! 
New to web scraping so have zero idea what I'm doing. 
Wondering if there is a tool that can help me with what I want to accomplish. 
I need to scalably search a bunch (100s) of online resumes searching for two specific keyphrases. All resumes that have one or both of the keyphrases need to be saved (by clicking 'save to favorites')...you can only navigate these resumes 1x1. So in order to progress from one resume to the next you have to manually click 'next'. 
So:
Scan resume for keywords.
Moving on to the next resume (single click) if no keywords are present. 
Saving the resume to my account (single click) if the keywords ARE present. 
Are there any ai/robot tools that can help with this?
THANKS!
â€‹",1,2023-10-11 23:03:29
Web Scraping into CSV file,"I am currently trying to write a web scraping code using beautiful soup that gathers stats from rotowire.com for nhl players. This includes Goals, assists, points, +/-, PIM, and SOG. Also for goalies, GAA, SV%, GP, and wins etc. ( There is a full chart on their website already that I basically want all of). I canâ€™t seem to figure it out, can someone give me a hand? I want to store all of the data in a chart as a csv file so that I can put it into a sql database and be able to navigate through the data myself with specific queries. This is just a small starter project that will help me with fantasy hockey:)",1,2023-10-11 20:55:05
Target website banned my entire pool of IPs?,"Hi,
I was having a lot of succeess against this taget website. I believe my IP rotated about 300 times. So I used roughly 300 different IP addresses. After a few hours however, it seems the entire pool of IP addresses has been banned. I ran some tests to check it's the IP pool and not user agent or something like that and it's definitely the pool.
My question is, how can they ban my entire pool of IPs when I haven't used many? The IP pool has 50k IPs (datacenter) I only used 300. This target website is very lowkey, not targeted by webscrapers or whatever. So it's unlikely these IPs were banned prior to me using them. They did something yesterday and banned my entire pool",6,2023-10-11 09:32:45
Easiest Method to Filter Specific Issues in Google Reviews for Localised Small Businesses,"Hello everyone,
I am embarking on a project that involves identifying certain specific issues customers might be facing on the websites of small businesses (e.g. customer service is bad), in a specific country, through their Google reviews. I am in search of an effective way to sift through the reviews to pinpoint these problems.
Is there any tool or method known to you that could help filter or search through Google reviews to uncover specific complaints or issues regarding a website, particularly focusing on small businesses in a specific country? A solution capable of handling a substantial amount of reviews with some level of automation would be ideal.
Your insights or recommendations would be greatly valued. Thank you in advance!",1,2023-10-11 17:41:44
How you guys find which website should be scrape,I want to know how you find or decide which website to scrape to get a particular data .,0,2023-10-11 15:00:09
Scraping Google Reviews,"Hi guys, I' trying to scrape google reviews for specific hotels but I'm runnining into an issue. Only the section with rating data is loaded when I try to scrape the site and the reviews are not (they load dynamically while scrolling on the site). I'm using requests_html and BS4 (selenium would solve my problem but it would take ages since I need to scrape all of the reviews for over 100 hotels). Do you have any advice?",1,2023-10-11 12:04:31
Scrape and download multiple PDF files from ASPX page,"Hey. New to scraping and using browser web tools, sorry for any wrong assumptions.  
Context: every year the agency who handles our apartment building gives us access to the original receipts for maintenance, cleaning, utilities costs etc. in PDF form through their website. The files can be accessed from their main website with a set of credentials, this leads to an ASPX page where we can manually download the PDFs.
Problem is, there are more than 200+ PDFs and manually clicking on each of them is a tedious task. I like to think this is evil design on their part to make tenant's lives (even more) miserable, but I digress.
I've inspected the page with a browser dev tools and managed to find the javascript calls for all the PDFs. They look like:
javascript:__doPostBack('xxx','xxx.pdf');
javascript:__doPostBack('xxx','yyy.pdf');
javascript:__doPostBack('xxx','zzz.pdf');
If I run these in console one by one, I do get the PDF file.
How can I download them all in one go? Running the whole batch in one go only gets me the PDF from the last line. Definitely missing something, but what is it?  
Thanks a bunch.",1,2023-10-11 11:42:52
I have a site where I'd like to reference events in my region but,"I have a site where I'd like to reference events in my region but I don't know how to go about scrapping these events, do you have any ideas? I'm in France",1,2023-10-10 20:53:53
Puppeteer cookie expiration problem,"I'm using Puppeteer to extract data from a website that requires login, and the scraping process is generally successful. However, there's a complication involving a cookie that expires every 30 minutes. This expiration time gets refreshed whenever the page undergoes a change. The issue I'm encountering is that in a regular web browser, this updated expiration time functions as expected. However, in Puppeteer, it doesn't seem to work as intended. After 30 minutes, the website logs me out and displays a ""session expired"" message.
I've attempted various solutions to address this problem. I've tried using incognito mode, engaging in different activities on the website, saving the cookie every 10 minutes and reapplying it, and even closing and reopening the page with the saved cookie. Unfortunately, none of these approaches have proven effective.
Do you have any suggestions or ideas for resolving this issue?",3,2023-10-10 10:31:45
web crawling & scraping to find businesses,"Hello,
I am trying to do some research by finding all businesses that I can within certain demographics. Without going into too much detail, I know that there are around 50k of these businesses in the USA that are registered (census data shows this) , and probably thousands more that are small businesses on Instagram and etsy, but wouldnt be registered businesses.  I am looking to find as many of these businesses as possible through different search terms.
I am new to this, and am not exactly sure how and if this will be possible, but I have worked with some developers in the past, and it seems like it is something that could be done.  In essence, I want to be able to crawl/scrape certain websites to find these businesses and as much pertinent information about them. (name, location, website, email, phone, etc.)  I believe the sites I would need data from instagram, etsy, linkedin, google, and maybe Facebook. From some of my research, I know that these sites can be hard to crawl.
I am really just looking for a place to start researching and was hoping to get some advice on where to start. I dont have much coding experience, so I would be looking to outsource a bunch of this, unless there was any no code options that are out there.
Any tips are appreciated. Thanks",1,2023-10-10 06:05:28
Trying to scrap a link with !X in url,"Hey,
I am trying to scrape https://www.britishhorseracing.com/
This site uses java - if I use Splash via http://localhost:8050/ I can render the site normally and get the data I want.
If I run the spider in cmd, I get
2023-10-09 21:50:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET [https://www.britishhorseracing.com/racing/results/fixture-results/?\_escaped\_fragment\_=%2F2023%2F965](https://www.britishhorseracing.com/racing/results/fixture-results/?_escaped_fragment_=%2F2023%2F965) via http://localhost:8050/execute> (referer: None)
but no data - the  def parse_fixture(self, response) function is not called and executed
I have this code: https://pastebin.com/fF8z5Sbw
I assume the !X in the link is causing problems, any ideas how to circumvent the ""#!"" ? It seems scrapy just runs into an error page because it ""escapes"" the part of the url
â€‹
â€‹",1,2023-10-09 20:54:37
Can I scrape this entire website?,"The owner of this website died in August , and I don't know what will happen to it, I really want to use it further since there is so much information, formulas and online calculators on it. Is it possible to archive it?",1,2023-10-09 19:40:30
AI Lead Finder?,"Just curious if there's any tool that can say for instance follow a prompt like: ""Provide me emails of Architects"" Trying to see if there's been any automation in this space with AI being able to source leads for business development outreach.",0,2023-10-09 17:33:11
Cheapest per request proxy provider?,"I am trying to find the cheapest proxy service that sells ""per request""  packages, like several well-known providers which I won't name here since it's better to avoid naming particular companies according to the rules here.
The cheapest I could find is ~3,5M requests for 249 USD, are there any significantly cheaper options available on the market? I mean, like order of magnitude cheaper. For example 3,5M requests for 50 USD.
Or, in fact, maybe it's better to create my own infrastructure to reach these pricing levels. What are your suggestions? How to approach building such infrastructure if I were to go in that direction?",3,2023-10-09 10:25:18
How to scrape tweets with certain keywords in specific periods !,"I am wonder what is the best way with code or with no-code to scrape tweets for certain words in certain periods ...etc
any working recommendations there is a lot of tweeter GitHub project but they look like not working in the last month for some issues
Ø´Ø±ÙƒØ© ØªÙ…Ø¯ÙŠØ¯ ØºØ§Ø² Ø¨Ù…ÙƒØ© 
Ø´Ø±ÙƒØ© Ø¹Ø²Ù„ Ø§Ø³Ø·Ø­ Ø¨Ù…ÙƒØ© 
Ø´Ø±ÙƒØ© ØªÙ…Ø¯ÙŠØ¯ Ø¹Ø²Ù„ Ø§Ø³Ø·Ø­ Ø¨Ù…Ø³Ù‚Ø·
Ø´Ø±ÙƒØ© Ø¹Ø²Ù„ Ø§Ø³Ø·Ø­ Ø¨Ø§Ù„ÙƒÙˆÙŠØª 
Ø´Ø±ÙƒØ© ÙƒØ´Ù ØªØ³Ø±Ø¨Ø§Øª Ø§Ù„Ù…ÙŠØ§Ù‡ Ø¨Ù…ÙƒØ©",0,2023-10-09 13:29:45
Anyone knows how to deal with Walmart bot detection?,"It's so annoying, somehow it knows and blocks me whenever I try to use request or selenium to access the Clearance page. I can access using normal web browser fine, but undetected selenium and request with normal header just can't seem to get through. Its backend API expired after about 10 minutes so I can't use that either. Anyone has a suggestion?",3,2023-10-08 19:38:41
DB to save scraped content,I am trying to build a price comparison website and want to store the scraped data in a database. Should I choose a SQL database or a NoSQL database?,6,2023-10-08 10:44:29
Read data from ebook link?,"Hello - i have this following ebook from a website:
http://ebook.yellow.co.nz/books/jjjx/#p=1
The initial site before is:
https://yellow.co.nz/print-publications/yellow-ebook/ 
Is it somehhow possible to read this ebook and provide the data in  HTML/TEXT/EXCEL?  
â€‹",2,2023-10-08 07:50:48
Automating Data Download from Website,"I have a ""dataset.xlsx"" file containing information on various S&P GSCI commodities. I downloaded this data manually from the S&P GSCI webpage, and now I want to automate the process to keep my dataset up to date.
For example, consider this S&P GSCI Precious Metals page: https://www.spglobal.com/spdji/en/indices/commodities/sp-gsci-precious-metals/#overview.
Here are the steps I'd like to automate:
Click on the ""Export"" link to download the data.
Repeat the process for all the S&P indices in my dataset (e.g., Energy, Metals, Industrial Metals, etc.).
I've noticed that the ""Export"" link seems to be connected to the plot on the page, and selecting different time frames on the plot changes the data available for export.
I'm currently using Python, and I believe web scraping could be a solution, but I'm not sure how to proceed, especially when dealing with dynamic content. Can anyone provide guidance or code examples on how to achieve this?
Thank you!",1,2023-10-08 10:55:49
I want the phone number to become visible?,"Ok, so this website only reveals the phone numbers for customers who want tutors, if you pay a certain fee. Can anyone tell me how to web scrape the phone number without paying any money? I dont know anything about web scraping or coding... I want the phone number revealed. Please help me.
https://www.teacheron.com/teacher-job/4qCk",0,2023-10-08 13:29:17
Scraping Askgamblers,"I'm trying to scrape a list of casinos from askgamblers. com/online-casinos/countries/cy/
While looking through the Network tab (Fetch/XHR), I can't locate any way they are actually pulling/populating the data...can anyone on here take a quick look and confirm that I'm not an idiot?  
â€‹",1,2023-10-07 18:51:47
"Viability of a master thesis using Twitter,Facebook/Instagram data","I'm thinking of doing a master thesis based on a sentiment analisys of social media posts about urban mobility.
So from my research this past week, I discovered that Twitter API and Facebook/Instagram, are basically unattainable for me, so I would need to resort to webscraping. I found some tools you guys linked in this subreddit, but before I would look to far into those as to not waste my time, I would like your opinion on the viability of getting the data I need. I would need per example to search for specific hashtags in Twitter. For Facebook/Instagram I would need to get the comments of public pages and if possible search for hashtags too. 
I should have made this post sooner, since I only have a week to deliver my idea for the thesis, I would apreciate your help very much",2,2023-10-06 17:15:59
Does anyone know where I can find any pre-scraped large-scale Google Trends datasets for free?,"I've been searching for a while now and I cannot seem to find any pre-built datasets out there that are free.  I would like to avoid using the API and doing it myself. I'll take any large trends data set you got,  provided that it is not industry/category specific and more general in nature! 
Thank you in advanced!",1,2023-10-06 17:31:42
How do I scrape json api that the server is sending the website?,"For example, if you look at this page: https://www.nordstromrack.com/clearance?origin=topnav&offset=2&page=2, you will see that the server is sending the website a json with all the data through this request: https://www.nordstromrack.com/api/browse/query/clearance?top=72&origin=topnav&offset=2&page=2
I just can't figure out how to request this api directly or intercept it somehow. Anyone has an idea?",2,2023-10-06 12:54:50
How I develop a Google Maps Scraping Bot? Please READ full article if you are interested.,"Creating a Google Maps scraping software isn't a simple task. Big websites like Facebook, Instagram, Google Maps, and TikTok invest heavily in security to protect their data. They use advanced techniques to hide their source code from users who try to access a website's HTML. Understandably, they don't want their data to be easily accessible to the public.
So, how did I develop a method to scrape data from Google Maps? It took me many months to figure out an approach. I decided to use a screenshot-based scraping technique. My main goal was to scrape Google Maps, so here's what I did: I downloaded the source code of a Google Maps webpage and extracted the data in HTML format using the BeautifulSoup HTML parser. In simple terms, I automated the process of copying and pasting code from a webpage using the inspect element tool to extract the relevant information.
This approach turned out to be fantastic and opened up opportunities to build web scrapers on a large scale. Now, using this technique, I can extract information from almost any website on the Internet because web servers can't hide their source code when you use the inspect element tool. I've had a good laugh at many developers who struggle to extract information from websites by other means and waste time and money on expensive APIs that often yield unreliable results.
However, there's one major drawback to this approach. If Google were to disable the ability to inspect HTML code on Google Maps, my application would stop working in an instant. I hope they never do this, but if they do, I'll have to find another way to scrape data.",0,2023-10-06 11:06:56
Is it possible to scrape the entire profile of a user after they have voluntarily agreed to 'sign in using LinkedIn' and share that data on my app,"Basically, the user would like to be able to move their skills as well as past work experience of their own profile on to my app. Noticed that most third-party scrapping services expect a 'session cookie' to perform the scrapping on behalf of the user the cookie belongs to.
In this case I would like to scrape only one profile, the one which belongs to the user signing in. I understand r_fullprofile was deprecated long ago, how can I go about doing this. Thanks",1,2023-10-06 09:49:30
Any Advice on scraping eBay/Amazon/google for prices,"I want to make a scraper that can evaluate products based on what they sell for on e-commerce sites.
I know there are APIs that do this, but they are extremely expensive and limiting.
I know Iâ€™ll probably need proxies and some sort of captcha Bypass.
Are these sites difficult to scrape?
Thanks in advance for the responses.",4,2023-10-05 22:24:54
Realtime scrape or store and display?,"Hey have you ever let a button click and realtime scrape to display results for an user on your website?
Or you store these scraped data organize and then display to users on web.
I am working on a project to host on pythnonwyhere and using a 3rd party scraper service if it takes more than 1 minute i cant display results getting empy server error
â€‹
Btw:If someone can save me from this hell of 3rd party scraping and partner up with me would be great i already have users for this shitty version with my marketing but i cant make it work :(.Somone good with node.js and grabbing endpoints etc...",1,2023-10-05 22:44:43
Scraping GraphQL,"I'm looking for help on scraping a website that surfaces its information with GraphQL. I've done a bit of scraping in the past, used Selenium, BeautifulSoup, and just hit public API's, but haven't really run into any GraphQL.
I first tried running postman interceptor, but i'm getting a 404 error with the populated call, but I'm not sure how interceptor works with GraphQL sources.
Here's an example url: https://www.playhq.com/cricket-australia/org/nsw-premier-cricket/kingsgrove-sports-t20-and-poidevin-gray-shield-summer-202324/poidevin-gray-shield/game-centre/3b0a93de
The API I'm trying to hit is: https://spectator.playhq.com/graphql
I've looked through search results and I'm wondering if it's an authentication thing?
Any help would be great.",3,2023-10-04 20:10:43
Sraping tiktok videos,"So I want to create an application that scrapes the transcript of tiktok videos and uses it. It looks like the official API doesn't provide transcript info. If I do find a solution to scrape the transcript, would it be legal to create such an application? i'm not sure if tiktok videos are copyrighted to the creator and this would be another issue (my application would be crediting the creator though). also im curious if the same applies to instagram reels!",3,2023-10-04 19:21:20
[selenium] website is suddenly not loading part of the page?,"I had a script I ran daily for months that is now broken. I noticed the page load has slowed to a crawl, and now when I click a drop-down to get the thing i want to scrape I get ""this item can not be loaded"". It works fine with a normal web browser. Any way to work around this? Including alternate technologies.",1,2023-10-04 15:51:50
Pay per GB proxys,"Hello,
Iâ€™m new to data scraping, but Iâ€™m looking to get more serious about it. I have therefore looked at different residential proxy ip services, and all of their plans are â€œ$/GBâ€. 
My question is: what do those GB signify? One service wants $10 per GB, and Iâ€™m scared of racking up a huge bill by visiting a website many times. Does it mean the data required to load a website? If that is the case, how could it ever be profitable to use such a service? 
Iâ€™m thankful for any clarification you can bring me!",9,2023-10-03 21:27:54
Failed to download a file with headless Selenium,"Dear webscraping gurus,
I'm currently trying to download a file from a website as part of my data pipeline. Apparently, the server I'm using couldn't operate while the code trying to open windows everytime it being run, ""why?"" you may ask, not a clue. On the flip side, if I went headless, the data won't download anything. You'd ask ""Why not using requests?"", I tried, but the process requires me  to click several buttons, input login information, and closing popup boxes. Does anyone know how to handle this kind of problem?",3,2023-10-03 17:19:45
Help a noob out ðŸ¥¹,"So I have to scrape a lot of data for work. 
Basically I'm supposed to see how much of a particular product is sold on this website: https://www.daraz.com.bd/hair-oils/?pos=1&acm=201711220.1003.1.2873589&scm=1003.1.201711220.OTHER_1611_2873589&from=lp_category&searchFlag=1&spm=a2a0e.category.3.3_1
Now the catch is that this information is available only on the mobile site and not on the desktop site. 
I have already tried the scrapewizard tool which is in the first top post here but unfortunately it only gave me no. of reviews and not quantity sold. 
I have no background in tech. I am in the sales team just trying to decide how much to order of a particular product. I have to do this for multiple categories and it is not humanly possible for me to do that. 
Please can the lovely people here either -
Let me know of a way I can do this in the fastest and free-est format possible
Or if I should be using any other ""keyword"" while trying to scrape the data using the scrapewizard tool. Here is what I had input : Brand name, Product Name, number sold",3,2023-10-02 15:46:17
Basic web scraping?,What's the best software or whatever to have automate searches and a web scraper collect names and email addresses of wedding agencies or venues? I just need them saved in two columns of an excel.,0,2023-10-02 15:13:26
Web scraping ethics,"Hello everyone, I found out about web scraping because I needed to efficiently parse info from Wikipedia and some public databases to create a tool, but it seems like a lot of web scraping related resources mention the ethics of it. Is it wrong to scrape info from the open web? Is webscraping used for nefarious purposes? I donâ€™t know enough to see how",4,2023-10-02 03:21:50
How to see how website converts/parses graphql json response to html,"I have a website that uses graphql endpoint and it returns a json response. However the values for the keys in the json response do not contain the html content.  Idk how the website uses the json response to convert it to the correct html. I tried looking at the JavaScript files but doesnâ€™t help.  
For example, the json response would contain coordinates and bunch of vectors for creation of an svg image and keywords such as â€œboldâ€ or â€œunderlineâ€ but not the actual html content in any of the keys.",2,2023-10-02 01:03:18
Monthly Self-Promotion Thread - October 2023,"Hello and howdy, digital miners of /r/webscraping!
The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!

Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?
Maybe you've got a ground-breaking product in need of some intrepid testers?
Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?
Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?

Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!
Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.",4,2023-10-01 11:04:00
Suggest Me any Course(Udemy) or any Tutorial (YouTube) to learn Web scraping using Python,"I am good at Python
No experience in Html, css, js
I want to learn Web Scraping at a professional level
Suggest me any course / Roadmap or any advice !!!",2,2023-10-01 06:54:58
Validate what a business does,"Hi there
I have 1200 companies I suspect are interesting to my sales team.
I want to use sales navigator to find contacts at these companies 
But I need to validate they offer the service I am looking for before I do that. 
I wrote a python script to check the website had a certain menu item but itâ€™s not being very successful and tagging companies as false when they are true.
Is there a way to visit a website and click through several pages to find key words or is there a way to find SEO terms theyâ€™ve used to rank better in search? 
Thanks",1,2023-10-01 10:24:28
Webpage monitoring help needed,"I have currently a VPS service. 
-I need to monitor specific content selector of some specific pages (<10).
- I need to be logged in to my account when doing the monitoring.
- The scan interval should be 10 seconds.
- If there is a change then I need to receive an alert though Telegram or Discord.
- Must be done in a way that my ip-address will not get banned.
Tell me a way please to do this, so the usage of  RAM will not get overloaded 24/7.",1,2023-10-01 08:37:14
Security through inefficiency [Rant/Funny Story],"So I've spent about a week trying to figure out how to scrape this website.  At first the issue was getting around cloudflare bot detection, eventually found an article that mentioned bypassing it completely, so now I'm sending my requests directly to the webserver and that fixed that.
I have about 500 pages I'm trying to scrape, each response takes about 3 seconds. At first I was sending a request, waiting for a response, then sending the next.  This would have taken about 25 minutes, although after 100 requests I started getting ""Too many request errors"" so it would probably take longer.
Plus, 25 minutes is already more than I'd like.  So I figured, send all the requests at once. (Using proxies to avoid ""too many requests"")  This is a pretty popular app, I figure they can handle 500 requests.
Welp, turns out their app is popular but their website must not be because it completely DDoS'd the website.  Not great.
So I figured, send requests in batches, with slight delays between each request, and a larger delay between batches.
This is when I realized what's was happening with this website.  If I sent a batch of 10 requests (which took 25 seconds for all responses to come through), and then went to their website and tried to load the page manually, the page would take 25 seconds to load.
This website is handling each request sequentially, and each request takes, at minimum, 1.5 seconds.  On average it's closer to 2.5.
Their backend must also be awful because 1 request takes 3 seconds, 5 requests takes 6 seconds, but 10 requests takes 25 seconds.
I feel like I'm going crazy.  This performance is absolutely horrible right?",5,2023-09-30 13:08:05
Ideas on Gathering Team URLs from Unique Webpages,"Hey everyone,
So I currently have a project where I gathered a list of 40k unique homepage URLs for an industry (doctor offices). I have been trying to work on getting the team pages of these businesses (I have already scrapped the homepage) using python. 
The issue is that I cannot get a consistent use case for extracting the team pages URLs, since the urls extension can look anything from /team to /about/team, and even /meet-our-staff etc added onto the end of the base homepage url .
Iâ€™m trying to find a way to get at least 70% of the team URLs pages.
Iâ€™ve used BeautifulSoup to:

Trying different variations of the homepages + different  forward slash extension strings of what a team url could have. And when I get a 200 code to then append that url to a new team_lst (this has the issue of appending any page even if it does not have the team info)
Grab all the href out of the webpage, this has worked the best, but some href values only have the /team or /team/about. Not the entire url of the team page.

Iâ€™ve also tried selenium and see if I can use a bot to try a list of words the team/staff button could have. But with them being nested sometimes and all unique pages, it is proving difficult.
I think the biggest challenge of course is the uniqueness of small business html tree structures.
I have some code I plan to use to scrap all text on the page into a single long string and use Regex to parse out what I need. The challenge is getting the actual team url. 
Any ideas would be super helpful, Im wondering if there are different approaches I could take or if someone has had the same issue.",1,2023-09-30 18:34:54
"Is it possible to make a web scraper that collects 1,000 Instagram accounts a day with a certain amount of followers and a certain gender?","I got a boring job requiring me to find 1000 Instagram accounts with a certain number of followers and a certain gender then copy and paste the URL to a Google spreadsheet. the job takes at least 9 hours a day. I know it can be automated I just don't know how to automate it.
here's the pseudocode version of what I need to do:
go to Instagram
look at profile
if profile follow count ==  or less than ""x""  and profile gender == ""x"" then copy profile URL and paste in google spreadsheet
if profile has ""xxxx"" written in bio and if profile follower count greater then ""x"" do not copy
do not copy if profile has no posts, highlights or stories
â€‹
â€‹",5,2023-09-30 07:48:59
"Scrape Facebook ""pages"" by state","I'm attempting to scrape Facebook pages by profession/service like ""power washers"" or ""concrete repair"". It works but I'd like to be able to do this by state. I can set locations to city but that is too granular with not many results. Any ideas?",1,2023-09-30 14:59:06
Pulling comments from Facebook post,"I am trying to pull from a facebook group a specific post and from that specific post the comments.
I went to facebook API and used the graph API but I don't know how to set up authorization to actually do this. I looked on youtube and google and all I can manage to do is pull the facebook groups name and ID but anything else I cant.
Anyone know?",1,2023-09-30 10:58:44
scraping changing data,how would one go about scraping data that changes regularly such as the score of a football game or  sport of any manner,1,2023-09-29 21:58:54
Getting into scraping,"I'm completely new to this but after researching a little earlier, I'm looking to use a scraping tool alongside openAI to fetch keywords/phrases from a website that would save me a huge amount of time at work.
Would anyone be interested in having a chat with me about this to see if it's feasible?",3,2023-09-29 15:48:13
How are you checking for page changes without scraping the full page contents?,"I'm using Scrapy / Scrapy cloud and Zyte API.  
We will have (when complete) about 300 spiders running often.....how often depends on how often the content on our 10 target sites changes.  
How would you guys solve this. I'm happy to run the spiders every X minutes and change the data in the DB but am wondering if there is a more efficient and elegant way to check for site changes.",2,2023-09-29 17:39:34
Scraping linkedin,"Hello, I am a master's student planning to do some scraping of linkedin for a research project with a professor. We have done a bit of work together previously scraping Craigslist which was a breeze. We are aiming to pull profiles from people in data science and data analyst roles and do some analysis on career paths, skills listed, language in describing past jobs etc.
I put together a selenium script that can do what I need once logged in, but I am concerned that managing logins, dealing with captcha, email verification, account and IP bans will be a pretty serious task if I want to scrape at a larger scale. I'd be hoping to scrape ~10k profiles if I can.
If anyone has experience scraping LinkedIn recently, I'd love to chat with them. Let me describe my current plan that I am hoping to refine and improve alot before I try to execute.

Manual account creation using residential IPs, I might create ~20-50 accounts manually. I don't know if it is worth trying to automate this step given the captchas and verification steps.
Login via proxy servers, using selenium, along with grabbing email verification code in an automated way. Probably retrying repeatedly if I get asked to solve captcha.
Scrape ~50-100 profiles, using a search, and using selenium to simply click the download profile pdf for later analysis.
Rotate to a different proxy+account and repeat.

So far while testing and developing I manually created a dummy account, scraped a couple profiles via selenium, and got it banned perhaps due to suspicious behavior with login attempts.
Any feedback and recommendations are welcome. I noticed a rule said avoid references to paid services and tooling. If this is kosher I'd personally be open to discussion of such tools although I'd prefer a free from scratch solution.",4,2023-09-28 06:17:27
strange results from scraping fandom wiki,"i am trying to implement a scraper in python via requests and bs4 that can handle making local copies of fandom wikis among other wiki type sites for offline use and my own fun. but i noticed in my testing and experimentation that even browsers will not save a fandom wiki page correctly. there is something broken with the css or other and i am uncertain about the cause. even when all the css and scripts are apparently present, the page still displays incorrectly. everything is the wrong size and incorrect location despite the order being correct.
â€‹
does anybody have insight or suggestions?
â€‹",1,2023-09-28 01:27:07
Implementing a Price Comparison Website.,"Hey everyone,
I'm thinking of developing a Price Comparison Website, and I'm curious about the technical aspects behind these platforms. Do price comparison websites typically gather data by scraping eCommerce stores, or do they rely on APIs?",3,2023-09-27 14:48:31
Kudos team,"Just popped in to say  https://webscraping.fyi it's awesome, thanks for putting all these resources together
Diving full force into  https://ayakashi-io.github.io/  just what I was looking for!",6,2023-09-27 10:32:53
"(Python, requests) How to Account for Fast/Slow Responses","I am currently working on a scraping project and I've noticed a trend in response time depending on the number of pages listed for a search query; response time increases with the number of pages available for a given query. It is safe to assume that this is due to computational cost server-side and I can't do much to speed things up. However, I still need a way to validate whether or not the latency is due to something on my end or theirs. Do you have any recommendations for tools/methods for diagnosing slow requests?
Also I apologize if this is a shitty question; 99% of my work is with scientific computing and I am new to scraping.",3,2023-09-27 14:14:14
Mod reminder: read the sub rules,"Strictly no self-promotion outside of the monthly thread (pinned)
Avoid references to paid services and tooling
""DM me"" = one week ban

We have noticed an increase in self-promotion masquerading as quasi-helpful contributions. To ensure the quality of our sub and minimize blatant and subtle spam, we've established these guidelines. 
If you encounter any content that violates these rules, kindly use the Report button. Let's keep our discussions centered on addressing the technical aspects of web scraping. This is not a platform for selling services or datasets.
â€‹",5,2023-09-27 02:16:53
Webscraping Discord,For quite a while I've been trying to make a script in python that monitors the discord website to keep track of a server that I use for promo codes but they expire quickly and I cant sit at my desk waiting hours for it to randomly pop up. I try to use BeautifulSoup but all the tutorials I follow end not working. Anyone got any useful tutorials on how to webscrape a specific discord server or know how to make a code that works for discord.,2,2023-09-26 22:25:20
Anyone also having issue with Cloudflare despite using Bright Data Unlocker?,"I started using the brightdata web unlocker endpoint to have it bypass my turnstile block but neither endpoint (datacenter, unlocker) make it pass the block page. I am getting a 200 response to cloudflare however therefore brightdata seeing it as a 'successful' request.
I do have contacted brightdata support and apparently they are working on it. I do have found an alternative in scrapeops' proxy aggregator which successfully bypassed the cloudflate turnstile.
Anyone had issue with the brightdata Data Unlocker endpoint and Cloudflare before?",1,2023-09-26 19:48:19
"[Help] I am not able to access ""www.realestate.com.au"" using requests & Selenium","I am trying to access this website:  ""www.realestate.com.au"" 
I can access it normally using my normal browser but I am not able to access it using:

Requests   (I accessed it in the start but now it is causing issues Error Code: 429)
Selenium  (It does not show anything. White blank page)
Undetected Chrome Driver (It does not show anything. White blank page)

Can anybody check this website and tell me how can I access it and get some data from it?",1,2023-09-26 15:53:17
Any tips on scraping Seeking Alpha using R?,"I'm trying to extract earnings calls transcripts with a simple rvest script. The first time, it scraped less than half the urls I gave it, and now it's returning only blanks. I'm trying again with randomised pauses but if that doesn't work, has anyone managed this successfully?
scraper <- function(x){
page <- read_html(x)
call_text <- page %>% 
html_nodes("".jL_jg"") %>%
html_text() 
return(call_text)
Sys.sleep(runif(1, min=60, max=480))
}
â€‹
â€‹",0,2023-09-26 12:49:17
How hard would it be to scrape zip codes for a particular list of names if it's in a defined area?,I'm trying to clean up a database and I need the zip codes for a list of customers. They are all in the same US county area.,1,2023-09-25 19:45:16
Web-scraping League Table data,"Hi everyone,
I'm trying to pull data from the Times UK University Rankings (https://www.thetimes.co.uk/uk-university-rankings) into R (rvest/httr) but struggling to find the right structures. Ideally I'd want to do it for all subjects and for all years. Is it possible with this data?
Cheers!",2,2023-09-25 13:47:53
Beginner Webscraping for Website,"As the title says Iâ€™m new to webscraping and coding but want to put some basic data in a table and put that on my website.
Though I have some basic understanding of coding, it seems like there are some potential pitfalls and it might be easier to get Brightdata or Scraperbee to maintain the Webscraper for me. I can invest some time into learning to code a scraper but the thought of continually maintaining the functionality is a bit scary to me. 
Any advice for a newbie starting out? Should I build this myself or hire it out?",1,2023-09-25 06:46:04
What libraries are good for websraping in python that can help to get around blocked sites?,"There are a few libraries such as BS, Scrapy, Selenium, etc. in python that allow for web-scraping. But which one works well to work around websites in which there's a wall and doesn't allow for web-scraping.
In the past I've experimented with Chromium in Python to get around the wall, basically it opens up a chrome browser and literally directs it to that page and then pulls the HTML lol... does't seem like the best option, wondering if there's another alternative?",2,2023-09-24 22:42:56
What are the ways to bypass Cloudfare using python?,"Can anybody tell me what are the ways to bypass Cloudfare using python?
It should not be a paid service though.",2,2023-09-23 21:30:19
How to scrape more than one APIs when you do not know their URL ???,"""https://www.cyberscope.io/audits""
I am scraping this website. I just checked it's network tab and there are some APIs where I can get data from. 
I want to scrape audit reports But the problem here is that all these audit reports have their own API. 
Can you help me to scrape it more efficiently?",2,2023-09-23 21:20:13
Questions about the legality of using a SPA's public API,"Hi guys,
I'm a web scraping newb. I'm not sure if this is precisely web scraping related, but I figured you guys would know more about this since web scraping encounters this sort of thing.
A website is an SPA so I could use a headless browser, but I found in the network tab an API that's used to fetch the data. There is an API key for the API and if I remove the API key from the request I get denied access, so the API is NOT for everyone's use, but the website is.
Would it be against the rules to use the API with their API key? They let people use their site which uses the API anyway. If I use the API directly I wouldn't have to waste more of their resources by loading all the other things like the front-end.
Let's say I have a list of 3 Million strings and I want to use the API for all 3 million strings would that be a bad idea?
It seems like it's a gov't site with public data. Would that make a difference?
Thank you!
TLDR: So, in a nutshell, can I use a publicly displayed API & API-key? If it's a gov't site would it make a difference?
â€‹",2,2023-09-23 15:28:48
Would an easier way to scrape 100s of websites be useful to you?,"In the process of building AI agents, we've found that what we built could eventually be good at dynamically scraping data across a variety of websites (10s to 100s of different sites at a time)
Our understanding is that existing web scraping tools are bad at this because you need to write custom scraping configurations per site. Not only that, but when a site changes styling, it might completely break your automation. With agents however, you can provide a high level natural language overview of the data you'd like from a website or class of websites, and the agent system will deal with the details of traversing a page and fetching data automatically.
Weâ€™re curious how useful this might be for people. If youâ€™ve experienced issues that this might solve or have already explored the space, I'd love to hear from you!",8,2023-09-22 20:33:06
Is not possible anymore to scrap social media ?,"Hi there.
A few years ago I used a tool to scrap emails from facebook groups and pages and it worked really well. 
Now I logged back in the account and all the services have been discontinued. 
Reading around it looks like is now impossible to scrap data from social media. 
I would need to scrap emails from
Facebook Groups, Facebook Pages, Reddit Groups, Instagram tags, Tiktok tags. 
For what I've gathered, this isn't possible anymore right ?
Thanks",4,2023-09-22 18:06:56
Class project,"Hi guys. I am taking a class for university, the topic is (something in the field of movies, music, video games) and I am planning on making a site that is a wiki of all of these media forms that have corgis in them. Thinking any appearance of them. A corgi shows up in a single episode I would like my site to show that. I only know Javascript and c++. I assume webscrapping is the technique I need and Puppeteer is the tool I've come across that uses JS (node) am I on the right path or will learning puppeteer not help me accomplish my goals?",2,2023-09-22 19:40:19
Webscraper.io - Each selector populates its own row,"This is driving me nuts, and I must be doing something wrong. I can't seem to set this up so I can get my data to populate properly to get each row to match. Each of my text selectors pulls the right info of Parent Name, Parent Email, Parent2 Name, Parent2 Email and Student. It puts the info in the correct column of the spreadsheet when pulled but it pulls all Parent Name, then goes to all Parent Email, etc.. 
Any ideas what I'm doing wrong?
Here's screenshots of what I'm doing: 
â€‹
My Website and the Selectors being used
My Selector Graph
â€‹
The Output where isn't filling across and just down but under the right column",1,2023-09-22 17:53:36
How to sort Instagram accounts in a google sheet?,"Hello,
I scraped a list of followers from an Instagram account that I saved in a google sheet representing in one column the nickname of the scraped accounts and in the other column the link to the account.
I'd now like to sort the contents of this list to keep only the public accounts to which I can send a DM and I'm looking for a way and tools to automate this, to do it as quickly as possible rather than manually.
Does anyone have an idea of a tool or service that could check every instagram account on my list to keep only those that are public and that I can contact?
Thanks a lot!",1,2023-09-22 15:12:27
What is the famous XPath playground on web to learn XPath?,"I try to search this website for people recommandation, but I can't find it again.
It's an interactive XPath tutorial.
The web site ask you to solve XPath tasks to move forward, kind of tech game
Any idea ?",2,2023-09-22 00:18:26
Setting SeleniumBase Driver options,"I've been playing cat-and-mouse with a web site, gradually stretching the limits of my scraping knowledge.  I started with BeautifulSoup then needed to convert the whole thing to Selenium.  Now, they have added a CloudFlare ""Very you are human"" checkbox.  It seems like the recommended away around this is SeleniumBase and the undetected-chromebrowser driver.
In a brief test, I can get past the human test (at least for now until they make more frequent or complicated) using;
from seleniumbase import Driver
and changing;
driver = webdriver.Chrome(options)
to;
driver = Driver(uc=True)
but I lose all the options I set up for Selenium to crawl and download from the site properly;
options = Options()
options.add_experimental_option(""detach"", True)
options.add_experimental_option(""prefs"", {
""download.default_directory"": outdir,
""download.prompt_for_download"": False,
""plugins.always_open_pdf_externally"": True
})
options.add_argument('ignore-certificate-errors')
options.add_argument('log-level=3')
options.add_argument('no-proxy-server')
options.accept_insecure_certs = True
How can I get these options into the call to the SeleniumBase driver or some other undetected browser?
The only page I could dig up with related info was;
https://github.com/seleniumbase/SeleniumBase/discussions/1221
but I don't fully understand the example.  My python code doesn't currently use classes.
Any help appreciated.  Thanks.
â€‹
â€‹
â€‹
â€‹",3,2023-09-21 13:05:20
Am learning python so i can start webscraping...,"Problem is the book i am reading i understand covers the basic but its not for webscraping. Also on youtube couldnt find material where i can learn python for scraping...
Are there any recommended resources where i can learn python mainly for webscraping?",3,2023-09-21 00:48:08
"How can I crawl a website, including its PDFs, and search for text?","I've been searching extensively and haven't found a solid answer to this question yet. 
I have a website that I want to crawl that has a number of PDF links. I want to also crawl those PDFs and search for specific key words in them. 
Are there any third party services or APIs that can not only crawl website data, but crawl PDFs and allow searching?",2,2023-09-20 17:16:53
Advice on scraping websites for businesses emails ?,"I have a long list of websites i need to scrape for emailing. However i've found myself running into a couple of problems when attempting to do so. Apologies for any misplaced vernacular -  i have no idea what i'm talking about.  
I have tried using scrapebox, which works pretty well if the email is available in the page source and not hidden behind JS. Obviously however it misses a large chunk of emails because of the websites where the email is embedded.  
I have also tried using a couple of scrapers on Apify. The problem here though is similar, whilst its able to get emails that are embedded etc.. It often misses a lot of the emails that Scrapebox was able to get ?? Doesn't make any sense to me.  
Combined i'm then left with separate data sheets that are a nightmare to integrate.  
I have also found upon manual inspection that some websites they are both missing the emails and returning nothing.  
Any advice for someone who isn't code savvy would be greatly appreciated!  
â€‹",1,2023-09-20 13:29:39
Is there a web scraper for Facebook pages?,"I have been doing some research and wanted to see if any of you had any luck in finding a web scraper for Facebook pages? 
What I am trying to do is create a search on Facebook under pages, to bring up all of the pages in a specific city that match my search term. Then from there I would like to be able to scrape the data from the Facebook page's about section (Name, URL, phone number, email etc.).
Any recommendations or advise here?",5,2023-09-20 03:17:10
"HomeHarvest: Real estate scraping library for Zillow, Realtor.com & Redfin","â€‹
Demo of HomeHarvest
https://github.com/ZacharyHampton/HomeHarvest
â€‹
Hey,
I saw there was a lack of real estate site scrapers freely available online & there was also a large want for them, so a friend and I decided to create this project.
â€‹
We currently support Zillow, Realtor.com & Redfin. As always, this is requests-based, and quite fast (relative to the search size).
You can aggregate the data from all the sites, some sites, or individual sites, you choose.
â€‹
Not technical? Use this at https://tryhomeharvest.com/.
Feel free to give feedback in the comments, we would love to hear your suggestions.",5,2023-09-19 17:56:19
Advice on Automating My eBay Business Using ActivePieces and Web scraping.,"Hello fellow Redditors,
I'm reaching out for guidance on how to harness the power of automation tools like ActivePieces (similar to Zapier) and web bots for scraping to optimize my eBay business. I'm brand new to computer automation and eager to explore its potential.
About My Business:
 I run an eBay business where I sell branded items and flip collectible video games and trading cards. 
I have a vision to scale up my business in the realm of collectibles with the assistance of automation.
My Automation Ideas:
Automation 1: I'm thinking of using a web scraper to search eBay and other platforms for undervalued items. The scraped data would be sent to a spreadsheet, where I've set up formulas to determine whether I should make a purchase. For instance, if an item typically sells for $100 and I need to buy it at $70 to make a profit, I want to scrape and highlight listings that fit this criteria for further review.
Automation 2: Another idea is to scrape sales data for specific items, including their selling price and frequency of sales. This data can help me decide which items to target in Automation 1. Imagine having a list of 100 items, and the automation tool goes to eBay, fetches the sales data for each item over the last 90 days, and organizes it in a new spreadsheet.
Automation 3: Lastly, I'm considering a more creative use of automation, such as taking an item's name or SKU, gathering product images, and sending them to Canva to generate updated images using prebuilt templates. This would be a time-saver, but I'm still working out the details.
I would greatly appreciate any advice, insights, automations you think would help or even potential tutorials if someone is willing to guide me through the basics of using these automation tools. Additionally, I'm more than willing to share my knowledge and experiences with others in my eBay business. Thank you in advance for your help and support.",3,2023-09-19 18:42:06
What library is best for parallel/asynchronous programming for Web Scraping?,"I am doing scraping for last 2 years now but sometimes scraping a specific website takes too much time. To make this process quick I want to learn parallel programming.
I tried asyncio but I could not understand it. If you have a good source then please share it with me.
If there is some other library much better and beginner friendly then do tell me.",0,2023-09-19 19:28:30
"How to scrape website which loads data after ""browsing"" website?","I want to have an offline copy of the Danish Dictionary and the government has one available online. 
The problem is that HTTrack and Webcopy don't ""browse"" the website, thus they never fetch the actual dictionary which is downloaded after loading the site. 
After loading the website, it fetches .bin files which contain the actual dictionary. 
The site is https://roplus.dk/#ordbog/ . Does anyone know a way to get a complete offline copy of this site?",2,2023-09-19 15:17:11
Scraping from HTML,"Up to now, I've been able to scrape anything I've wanted with Power Query. But now I want to scrape data from a web site (sailboatdata.com) that has each ""table"" encoded as a separate HTML page. Power Query can read the HTML and convert each page into tables but all I want to do at the moment is pull one or two individual table entries from many HTML pages, and Power Query seems to be a clumsy way to do that. What would be a better method?
Some background:

The web site does use a consistent scheme for paths and filenames, so I can easily generate a list of the URLs I want to search. (Already done.)
I don't know Python but I'm willing to learn it if it's the best path to a solution for this problem.
I'm using Windows 10 but I also have Debian Linux.",2,2023-09-19 15:08:51
A guy built an Amazon scraper for me. Is it legal?,"Hi, I am running an Amazon FBA business. A guy built a bot which scrapes data which will give me valuable information. The bot just scrape data which is publicly available on the Amazon website. 
We tried to use the Product Advertising API in first place but it seems like there are some requirements to use it which I do not have (I am working on getting them). 
So in the meantime he built the bot wich scrape the Amazon product pages. I did not run the bot yet because I am afraid Amazon will ban my seller account.   
So my question: Is it okay to scrape Amazon or will I get in trouble? Or should I maybe wait to get access to the API and then use the bot with the API, this should be allowed or am I wrong?",8,2023-09-19 06:45:35
[PROBLEM] - Can't get any tag inside 'a' tag while page source shows the link in span tag,"I am trying to scrape houzz.co.uk but I am getting a problem.  
Link: ""https://www.houzz.co.uk/professionals/architects-and-architectural-designers/trevor-brown-architects-pfvwgb-pf~1385257482""  
I want this URL from the page:
https://preview.redd.it/97goqxhx98pb1.png?width=708&format=png&auto=webp&s=12edfa850a1db59bbfff55a89acb5a8392cc3f7c
This is the HTML for this URL:
https://preview.redd.it/ueq2bwku78pb1.jpg?width=518&format=pjpg&auto=webp&s=d78135e2f5964cb652f99309bb6bcff206375459
I have confirmed that this page is not JavaScript rendered but when I find this ""span"" tag, BeautifulSoup is not getting it. I also used ""requests-html"" and rendered the page but still it is not finding this ""span"" tag.
I am able to get ""a"" tag but anything inside can not be found.   
Can anybody help me with this?",0,2023-09-19 15:21:04
XHR requests returning image/gif,"Hello, 
I am trying to web scrape lego.com and need to make endpoint requests because of the load more form of pagination at the bottom. 
I am using the developer tools in chrome to view the XHR requests to the endpoint in the network section. 
When I click the ""Load More"" button at the bottom of the page, I see XHR request that is made to get more items but the ""content type"" that is being returned is ""image/gif"". 
Can anyone help me see the JSON response for this webpage? 
The lego page I am trying to scrape",1,2023-09-19 08:21:45
A reliable linkedin email extrator tool/extension?,"Tried Kendo free (35% success rate) and SignalHire paid (50% success rate). Is there a reliable tool that isn't expensive? (40$ to 70$). Most emails I was getting from kendo/signalhire would either bounce back or even look fake (different name in the email or just a guessed email ex adam phillips [adam.phillips@gmail.com](mailto:adam.phillips@gmail.com))
Thanks",2,2023-09-18 16:49:29
How would I go about scraping this website?,The main details needed are only visible when clicked,3,2023-09-18 07:04:52
Watching variables in an online service's javascript file,"Apologies if this is technically not considered web scraping.
I'm wondering if it'd be possible to interact with the Javascript files of an online staking service like Stake.com to extract the roulette's winning numbers each round.
I don't want to know how to do it specifically, but actually if it'd be possible at all since I don't quite understand how it works. When there's a new winning number, how is that streamed to the browser client? For example, would it be possible to watch variable values if I found which ones to track in the obfuscated javascript code?",0,2023-09-17 17:34:49
Issues With Smartproxy and PHP Curl Script,"I was trying to use smart proxy to enhance my php curl webscraping script, but for some reason itâ€™s not working properly.
The script works fine when I try to run it from my local computer, but I want to be able to run it from my websites hosted server on A2Hosting.com
I spoke to customer support with Smart Proxy and they said itâ€™s not an issue on their end, and itâ€™s not an issue with my code. So it looks like itâ€™s some kind of issue with my web hosting provider. I submitted a ticket with my webhosting provider, but am a bit anxious to find a solution here.
I was using php to crawl sites because I wanted to have it scheduled to auto run as a crown job, so I didnâ€™t want to do a Python script or anything because I canâ€™t do Python from jobs with my web hosting provider.
Anyways, I know this is vague, but if youâ€™ve ever encountered any similar kind of issue Iâ€™d appreciate your feedback!",1,2023-09-17 02:14:45
Searching for quantity of stock still available when it is not written in the html,"As from title, I am searching for the quantity still in stock for products in a site.
Unfortunately, I've searched for it and it is not written anywhere in the html.
Since I'm a beginner I was wondering if there are other ways to search for it.
Thank you! :)
â€‹",1,2023-09-16 17:31:20
Scraping data from McDonald's,"I'd like to compose a list of all McDonald's restaurants in certain European countries.
Many countries (eg. the UK, Netherlands, Switzerland) use a common API which delivers the results as a JSON array:
https://www.mcdonalds.com/googleappsv2/geolocation?latitude=51.54&longitude=-0.15&radius=200&maxResults=150&country=gb&language=en-gb
https://www.mcdonalds.com/googleappsv2/geolocation?latitude=52.55&longitude=5.60&radius=200&maxResults=150&country=nl&language=en-nl
https://www.mcdonalds.com/googleappsv2/geolocation?latitude=46.20&longitude=6.14&radius=200&maxResults=150&country=ch&language=en-ch
The parameters should be quite self-explanatory.
But the API seems to have a upper limit regarding the ""maxResults"" parameters depending on the ""country"" parameter. While the GB call returns 150 results, the NL call only returns a subset of 38.  
It seems to be a Google API. IS there any known way to bypass this behaviour?
At the moment I bypass this limitation by setting the maxresults to a low number, changing the latitude and longitude parameters by a slight margin and calling the API again and again eliminating double results afterwards. But I'm worried that I might miss results in dense areas like London or Amsterdam.  
Any ideas or suggestions?",1,2023-09-16 15:35:05
Need help with scraping Blinkit,I need to scrape Blinkit grocery data from https://www.blinkit.com. The catch is that I need the same grocery data from different area pincodes. I successfully scraped the data of a particular grocery from one pincode but I can't seem to understand how to change the area pincode or integrate the area pincode in the url of grocery that is on blinkit. I am using selenium and BeautifulSoup. Please help.,1,2023-09-15 21:50:29
Not able to scrape details from PDP.,"As the title suggest I am able to scrape details from PLP but not from PDP. I am new to web scraping. Please help me out. Till â€œdiscountâ€ I am able to scrape ( itâ€™s in PLP) . From â€œsizeâ€ I am not able to do it( itâ€™s in PDP).  
Code -> 
Import necessary libraries
import nest_asyncio
nest_asyncio.apply()
import pandas as pd
import asyncio
from playwright.async_api import async_playwright
Define an asynchronous function to scrape a website for T-shirt data
async def scrape_website(url):
    # Set up Playwright and launch a Chromium browser
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(url)
    # Scroll down to load all T-shirts (you may need to adjust the range)
    for _ in range(10):  # Adjust the number of scrolls as needed
        await page.keyboard.press('PageDown')
        await page.wait_for_timeout(100)  # Add a time delay to allow content to load (you may need to adjust this)

    # Extract T-shirt details
    tshirt_data = []

    # Locate the elements that may contain T-shirt information (adjust this based on website structure)
    tshirt_locator = page.locator('.nw-productview.nwc-anchortag')
    tshirt_handles = await tshirt_locator.element_handles()


    for handle in tshirt_handles:

       # Extract the link from the href attribute of the anchor tag
        tshirt_link = await handle.evaluate('(element) => element.getAttribute(""href"")', handle)
        tshirt_link = f'https://tommyhilfiger.nnnow.com{tshirt_link}' if tshirt_link else 'N/A'

        # Extract T-shirt brand name
        tshirt_brand_name = await handle.evaluate('(element) => element.querySelector("".nw-productview-brandtxt"")?.innerText', handle)
        tshirt_brand = tshirt_brand_name.strip() if tshirt_brand_name else 'N/A'

        # Extract T-shirt name
        tshirt_name_element = await handle.evaluate('(element) => element.querySelector("".nw-productview-producttitle"")?.innerText', handle)
        tshirt_name = tshirt_name_element.strip() if tshirt_name_element else 'N/A'

        # Determine which price element exists for this T-shirt
        tshirt_price_element1 = await handle.evaluate('(element) => element.querySelector("".nw-priceblock-amt.nw-priceblock-sellingprice"")?.innerText', handle)
        tshirt_max_price = tshirt_price_element1.strip() if tshirt_price_element1 else 'N/A'

        tshirt_price_element2 = await handle.evaluate('(element) => element.querySelector("".nw-priceblock-amt.nw-priceblock-sellingprice.is-having-discount"")?.innerText', handle)
        tshirt_selling_price = tshirt_price_element2.strip() if tshirt_price_element2 else 'N/A'

        # Extract Discount
        tshirt_discount_element = await handle.evaluate('(element) => element.querySelector("".nw-priceblock-discount.is-having-discount"")?.innerText', handle)
        tshirt_discount = tshirt_discount_element.strip() if tshirt_discount_element else 'N/A'

        # Remove brackets from the discount
        tshirt_discount = tshirt_discount.replace('(', '').replace(')', '')

        # Size chart
        for sizes in tshirt_link:
          tshirt_size_chart = await handle.evaluate('(element) => element.querySelector("".nwc-btn nw-size-chip"")?.textContent', handle)
          tshirt_size = tshirt_size_chart.strip() if tshirt_size_chart else 'N/A'

        #Colors available
        for colors in tshirt_link:
          tshirt_color_available = await handle.evaluate('(element) => element.querySelector("".nw-color-name is-active"")?.innerText', handle)
          tshirt_colors = tshirt_color_available.strip() if tshirt_color_available else 'N/A'

        #Specs
        for specs in tshirt_link:
          tshirt_specs_chart = await handle.evaluate('(element) => element.querySelector("".nw-pdp-desktopaccordiondetailsul"")?.textContent', handle)
          tshirt_specs = tshirt_specs_chart.strip() if tshirt_specs_chart else 'N/A'

        #Finer Details
        for finer_details in tshirt_link:
          tshirt_finer_details = await handle.evaluate('(element) => element.querySelector("".nw-pdp-desktopaccordiondetailssection"")?.textContent', handle)
          tshirt_finer = tshirt_specs_chart.strip() if tshirt_finer_details else 'N/A'

        # Append the data to the list
        tshirt_data.append([tshirt_brand, tshirt_name, tshirt_max_price, tshirt_selling_price, tshirt_discount, tshirt_size, tshirt_colors,tshirt_specs,tshirt_finer,tshirt_link])

    # Close the browser
    await browser.close()

    # Return the scraped T-shirt data
    return tshirt_data

Define the URL of the T-shirts page
url = 'https://tommyhilfiger.nnnow.com/tommy-hilfiger-men-tshirts'  # Replace with the actual URL of the T-shirts page
Run the asynchronous function in an event loop
loop = asyncio.get_event_loop()
tshirt_data = loop.run_until_complete(scrape_website(url))
Check if data was successfully scraped and saved
if tshirt_data:
    # Create a DataFrame from the scraped data and save it to a CSV file
    df = pd.DataFrame(tshirt_data, columns=['T-shirt Brand', 'T-shirt Name', 'Maximum Retail Price', 'Selling Price', 'T-shirt Discount', 'T-shirt Size chart','T-shirt colors','Specs','Finer Details','Link'])
    df.to_csv('tommyshirt_data.csv', index=False)
    print(""Data scraped and saved successfully."")",1,2023-09-15 20:39:48
Recommendations on webscrapers to use,"My current job requires me to find companies that are exhibiting at expos and get their contact details. 
Basically i have to use a list of expos to find participating companies then go to the company website and find their contact info . i have to do this for every expo and every company in those expos. Was wondering if there is a way to automate this process with webscrapers? any help would be appreciated.Â thank you!",2,2023-09-15 04:13:05
I am trying trying to get ANY datasets created by you guys!,"I've just launched a website repository where people can share and access datasets, with the goal of making datasets more accessible. I'm also planning to integrate a donation feature to encourage people to support contributors if they wish. If you have a dataset you'd like to share, please don't hesitate to reach outâ€”I'm interested, it's super easy to post/list!
Edit: itâ€™s called sellagen.com btw!",2,2023-09-14 18:36:06
What IDE you guys have been using to create scrappers,"I m using pycharm , am I need to shift to vs code",1,2023-09-14 20:25:01
Scraping twitch,Hi guys I'm doing a project that requires me to scrape a list of twitch users that have over 100k followers/subs how would I go about doing that and is 200 enough or should I add more,2,2023-09-14 04:58:53
Looking to partner on a project,"Hey there, I'm searching for a partner experienced in scraping data from Reddit and Twitter. I have a simple data business that I think could be incredibly profitable. 
I'm looking for someone who has is experienced or has the ability to scrape social platforms such as reddit and twitter. You would need to understanding APIs, rate limits, and best practices. Additionally, you would need to be able to be proficient at organizing such data for simple usability.
If this is you, please message me and we can talk details. Apologies for the anonymity and lack of information, I don't want to just throw out the idea for anyone to run with.",2,2023-09-13 22:15:47
Best Scrapy Projects To Learn From,"Hi there,
I am trying to learn some more about scrapy and I am interested in reviewing some open source projects that integrate scrapy well.
Does anyone have either their own or some example repos that they think utilize scrapy really well?
Thank you!",2,2023-09-13 18:59:00
Scrapping from magzter,"Magzter has so many magazines available, however I am yet find a way to download. Is there any way anyone been able to download them now?",2,2023-09-13 18:35:18
Finding clients for a webscraping as a service business,"I recently found a company online that offers web scraping as a service and I thought this was cool.
So I was curious about how to find clients for a web scraping business if I was going to start one. Is there a forum, job board that I can get them from etc.",3,2023-09-13 14:02:28
Is anyone here ever tried to scrape the upwork,Just wanna know their experience,1,2023-09-13 19:16:18
Best Node Combination for Scraping and Inserting Data into Excel,"Hi everyone!
I am scraping eBay for compatibility data and I want to insert every vehicle data into an excel file. Can you recommend the combination of packages to do this? My current setup is puppeteer, puppeteer cluster and xlsx. What I specifically want to do is insert the data right below the cell where the product number is located. So, I am looking at a script that searches for the product number inside the excel file, once it matches, writes the scraped data to the cells below it.",1,2023-09-13 15:04:42
Scraping an airtable data,"Hi All!
I was wondering if anyone had ever tried to scrape an airtable page. I want to scrap from here.
Is such a page possible to scrape?
Thanks a lot :)",1,2023-09-13 11:29:04
How to deal with CloudFlare human verification when using Selenium?,"Hello, I am very new to web automation, and as a fun project decided I would automate the process that I upload some stuff to redBubble. I am still learning how to deal with captchas, and right now I just give myself time to manually intervene and solve the puzzles as they pop up, however on this site, particularly when I go to upload a new work, it just gives me the checkbox saying 'Checking if the site connection is secure' and when I click it, it just has a small loading animation then pops up again.
â€‹
looks like this
I have tried a lot of research on google and reddit for solutions, I have tried using undetected chrome, I have tried copying some selenium driver options that were recommended (not that I understand them), I even take time in my script to go to https://cloudflare.manfredi.io/en/tools/connection to check if my connection looks like a bots and it doesn't there, furthermore I have many randomized time sleeps and move the mouse myself while it runs, so idk why it thinks I am a bot. I assume it is something innate about the browser Selenium opens, as when I do this manually on chrome (what I normally use to surf internet) I get no such check.
Any advice would be greatly appreciated, I've been stuck for ages on this.",3,2023-09-13 03:40:30
Trying to scrape this site but it seems to not find anything,"Im trying to scrape the titles from this site(with the whole filters added) i need to do it for a couple more stuff to search, but i cant just do it, i've tried everything.
I've tried with beautifulsoup and playwright but can't get any single tag, when trying with cheerio and axios on javascript i get a 403, am i missing something ? Can someone help me with this, please?",2,2023-09-12 19:26:44
Challenge Pages on Indeed,"Hello,
I am practicing scraping without a headless browser controller. I am trying to use Axios to grab the DOM from a request URL. The problem is, I am returned a Cloudflare challenge page that contains nothing but a bunch of obfuscated Javascript. I don't get this page when visiting the page as a human. I am only making one request and I got it on my first ever request so I am not being limited or anything. Aside from setting my request headers to match my browser, I honestly don't know what else to try. Has anyone dealt with anything similar?
EDIT: Modifying my TLS fingerprint by rearranging the ciphers my client sends doesn't seem to work
Example link: https://in.indeed.com/jobs?q=Software&l=India&start=0
Returned page:
<html lang=""en-US"">
  <head>
    <title>Just a moment...</title>
    <meta content=""text/html; charset=utf-8"" http-equiv=""Content-Type"" />
    <meta content=""IE=Edge"" http-equiv=""X-UA-Compatible"" />
    <meta content=""noindex,nofollow"" name=""robots"" />
    <meta content=""width=device-width,initial-scale=1"" name=""viewport"" />
    <link href=""/cdn-cgi/styles/challenges.css"" rel=""stylesheet"" />
  </head>
  <body class=""no-js"">
    <div class=""main-wrapper"" role=""main"">
      <div class=""main-content"">
        <noscript
          ><div id=""challenge-error-title"">
            <div class=""h2"">
              <span class=""icon-wrapper""
                ><div class=""heading-icon warning-icon""></div></span
              ><span id=""challenge-error-text""
                >Enable JavaScript and cookies to continue</span
              >
            </div>
          </div></noscript
        >
      </div>
    </div>
    <script>
      (function () {
        window._cf_chl_opt = {
          cvId: ""2"",
          cZone: ""in.indeed.com"",
          cType: ""managed"",
          cNounce: ""3141"",
          cRay: ""80590e2d2fe21f48"",
          cHash: ""147fb695286b3bf"",
          cUPMDTk:
            ""\/jobs?q=Software&l=India&start=0&__cf_chl_tk=n.btGmEn7Tl1Dg7_ltP8mRKhIburEcL5nHF9v6J9eAo-1694531197-0-gaNycGzNC2U"",
          cFPWv: ""b"",
          cTTimeMs: ""1000"",
          cMTimeMs: ""0"",
          cTplV: 5,
          cTplB: ""cf"",
          cK: ""visitor-time"",
          fa: ""/jobs?q=Software&amp;l=India&amp;start=0&amp;__cf_chl_f_tk=n.btGmEn7Tl1Dg7_ltP8mRKhIburEcL5nHF9v6J9eAo-1694531197-0-gaNycGzNC2U"",
          md: ""HlI0j5tM.h1drfRaOWsRniTffM5GmLSjkXEnBGbRcUw-1694531197-0-AUV1WSZFWuOr0GB5s5EWt8N3rQuuPTz0XKQS2JU0Fr6-9qnMVpPRZJIm65Qgyd80KYpGZJ89vzNxZ9-M6x5uLHch1LXuKQbQ6BwRcHL-ShCQbkTxIWCNEIcE4Cg57-P1MvSMLZWL7UZ_nbQfc49ZCvY-3HETpejY-75uH_YvrGdvkg-z5poCm-eTZmFGSopc8a87XV-G5iDw17h5nZfgW5lTod9skfzOFWv_iARJ88kuqd5PlyYizsWQi4_Bp664CFx8UUfbj6faG8U8eITzOqNu2icn11imchELG74pPvVyPOwi1hg93SR-QJ1HqWaF2OQENMxaSQ2xk5i6a7eyd3ik8xxQLnOsWBiLhihS1KmlrOCBHZJnnEjqqoqkOwEN-XFbV9atOu3vKaljV7NjzSRplu-gldQFZwOfzyhzshEgxWShdhWjLQwCsySOv4KCkIOWIz48ISPNJ3eXkGkEaL1xEZ3fbzPvAhel0pfDHuEJJuylMdvMe83SQpWE7cvXUPoT5E-0cjxDRG6QI2oFCelcnslvzuh57QL-6BPBx_XOAUbNO3Aw7xbVfWs8s92HvE5mYNbRHrfK0uSauTtx2ip3N33LjjZlz9VP0wzO7DWqv043BAnsevL5GXLV9a-P2AooeOxYuGC3tR4Pd3N22hRXMMMvqiJzAO1GeVGxJCvHPRFWXicsoBaAG-TrdpCVG6yFB_wW1TxvKrgs_MxVThHlP1HIEqXQ1hjMjo0L-XEZ7SyP1GmOK7I_mnlGQqqAbqIvufeZh8ZF5By2a27pZwl-k4NNxCKZ6CmU6GAEubQzaYwjDMqr_LbaO6Shgb_pSEtJkQhNQeIRdVFlKbsuaIgxiWiqf98Rlkmkj_r1iNvvE116FqblLFm0ph-22E4k2cq0UWizmoAiiZMpa1BI-VEu7-avhQziLgShU8v08MgKjPzxYAK3-hRt2QcE1hcKk7-UMcUqH8CvFY494eXiCWDyiEs6o4zd2Wq9niDCrComv0m5w8bkos00s8llVNYVEPl_wwEzrwqyBPJLyPy7_y4XXalcyP37yH7M0ueVzKgDvrfaN_T7Kzv-KnxWOLJrnkKvASRrBPIPqV6w1w19thqDyjhfLM7rftlTVgU6-qskjyfbHnAuPZF65gj5FrShHVz7Roh-A9-utMMD68pMipRiNXw0WuUiwUISSbfjoKDonXaMgfxlOaqlxIy4ZOMbnIA29hKkk1ayoiJKfh_tSmtq7ulpkEA5oQ0liPpPz5oduRCR8zdJbsuIutZPzd6cyaUsVrqY8cV80IrdDRpBFUda-0r75rfEive0HdWV0c3IK6KXFL4U_bCAMnxIuGzDXXIdabjDqh-VmiHPtCNF3hn0KU4papnFE-K5G9clpEaHM0mWkEcskWOrziiE5cu_5XJ4H_bd2ad53sZrhiT6j7nSFhK2fbneEyGN0wyjZ3LNFBupYSiM5A8fERRQZv0WQhcTGxyVSD_mtUxzDJrT8tqli0wrTocmkep0Zk8k86f-K2MmRqaLLiNDFgocCADGR3NSkmHZPPGs3AQY-s3AEVCHEVx2NY50gdfGTULDcAYLC9nACfXN7hIUFsxoPn_m8kbjpXwo1KqlWck83s6i6u8n4CGQrho1r6qDDJniHCORWrd5E0RupKCwBvqyHV3ucQWebF_QpaiXkBybSGB5EdAcbHEq6FrHoTFvc3yvz4rPUueAzZIBblEzjE1NumPlmWmicoE2_MABx2dWmdYMoye0NpBsnu38VEmYL0_V8DYBe5blNKmfAyqho7xUZB7B0cqF1qWfkNfH-GjoRB1XPx9nFjnD3Okw_DS1zcwiLqD2C7Agpn98SLP4jSHABbzCmw23ps5L6WOHUwfus0Om8O9CM5ZzGkGJY-jJSrC3S0ga53tZqpu2K3ClEKrw1b1AlsMoTEMdDrsudB6W1A6ZqdtAis2B1jWx9nhReX57me9LdIEVPlQrnRMdk4EidCa0vMqKyeMzxqMouHNR6TQxH6GCD-WKAmEKnLhw7hU_Pqa8bV2vUW0NB0TjvChoZ_knSGHTawi82fdnf9C7M9Zw5QoFLSZg6XLO1Eh-ymOCUh8TJihSxUWb8g_FxFKpQwtZE-VNpPqSWbz-3zEEDFR4-CPDlPS3XnZtO_LsBnzpnvKNTvs28-UlDc5fdvTtdu51Hu8-SJt9ILEh-D76a1d9BVAIjskRKqiLKHZrlau1qDMMnxGytQkg6DSO8-npbhTMIEftMJSFZgdqKKCnpaDFdvRUGWc4Zz2uGiS8lwpnZnbMEkevx5j41EuBakYEgQ3iZnfjs0xCwNBY5orZear6txPtV-lg-8XEYgll-3AtGg7DsLXc7NNMlwxnWpyaozBP0nDZCA5xbd_ReuZcOCUTR65S1iyo_1vvoX1zJjFW_z-TKB6MQTp1NzGHveT99KPYQ0gR8wUzabsOP7Q3RtbDmq6m9AollhYHnaqLtwo82lzlFgdGW8Jg4CN5zeQ5yMCQ3lcIpKlDWGAkEv1P5BYmoJgzO4ikcdRyz781wG7Si-EzPfKiXAu-lRkc3dKF0-KIemI0VGhTVG0E9xxmoCGOgsxJgBTwHPF8vjZlMIPiB5iroGY4O2kKEScpwVLpPNlHZ7AfS0VEy2-aEOdHd79a2ykCiT_ePsmmQgK7g6vSiriJNQx2zZVDz-rMwofdwfXgtn4q553M1EseoGKeY7690UizUP9Q3ysvUeMVybVEVw5Qlq0W_th6qWORUyg9cYbPw2xIkYW0jqUw5L1Wtaz6FrKCf9S1FY1hl_-iiSNEomndnFJfxSOOapBoJEiAk3-EnfMGVn05G03SKGG4C1a6HMDG52JHOfml1Cuw-25H32sS"",
          cRq: {
            ru: ""aHR0cHM6Ly9pbi5pbmRlZWQuY29tL2pvYnM/cT1Tb2Z0d2FyZSZsPUluZGlhJnN0YXJ0PTA="",
            ra: ""TW96aWxsYS81LjAgKFgxMTsgTGludXggeDg2XzY0KSBBcHBsZVdlYktpdC81MzcuMzYgKEtIVE1MLCBsaWtlIEdlY2tvKSBDaHJvbWUvNzcuMC4zODY1LjEyMCBTYWZhcmkvNTM3LjM2"",
            rm: ""R0VU"",
            d: ""JuTUcpmejimvleIi/FUSNi9zcphOh/tzgvIS7PplApoXAWdansPP5eqaiQDjT9LnfalxzPKmME8Tk0urwrGlvPmblWj1FBwc71PwUVRqYjDwvWF16JZ/KRyrx9J//5Q6i9nCu7ZTHs71xxW4hK2t72ifzOsG3n6Ob74AIctMIzzvbqOs+JiswOV45ncpDh2fxD2fAnrp5JSEv/cwd6r1z6IX7rkIFtYJCSqsqWMUyxpwA4nBB1eO45R7Zysk4xQJaEetF14TSezlFlJ1brgdmp+M9G5d8ha9gIR0ekfLyAn+no0aIZAp/LIjjnv+wJdXDYE1ZjF5Jm25DQOA5biq/0d0B1YKOeqoiaQ9G/KXHQbXUJ/wwz5PmHLinzoadNMzQOGRAlBR/ewBBdJfWwCtvn7PB40oGH6peD1vfphLgtEEWjbA8+BsqwGTgEA86F/XjS736T3299XZWqS2bADRmFCZCitYqkI7JbvQystLZkYEEZispCvzv1R2WIBhgNNxHT7jkU372P3lUohyOvy9K5iC/acJUR9KkT4lQsZQbRGuuXQCYrxK4W4ULbJrPQXz"",
            t: ""MTY5NDUzMTE5Ny4wMDkwMDA="",
            cT: Math.floor(Date.now() / 1000),
            m: ""KtGmqAoUgv3RXaF15rVQOJ8pBtS6AGNCgQaVAP1Yqiw="",
            i1: ""ac9g2yWa9ACtXsFYLgecAg=="",
            i2: ""mq4Cy45AXphFIwvXUZkwRg=="",
            zh: ""2kfjrLYfAVNem19Lin3oP+VKo7ladb55se1eVtiLLnQ="",
            uh: ""Y8XdOAU1zN5Mj90pcgcwuvH3oHiZYN8bTyjSxzTIfSU="",
            hh: ""kUYHEl0zk3i+74OOEWwmnbBoUjF7XhVtDyrwHPwh8do="",
          },
        };
        var cpo = document.createElement(""script"");
        cpo.src =
          ""/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=80590e2d2fe21f48"";
        window._cf_chl_opt.cOgUHash =
          location.hash === """" && location.href.indexOf(""#"") !== -1
            ? ""#""
            : location.hash;
        window._cf_chl_opt.cOgUQuery =
          location.search === """" &&
          location.href
            .slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length)
            .indexOf(""?"") !== -1
            ? ""?""
            : location.search;
        if (window.history && window.history.replaceState) {
          var ogU =
            location.pathname +
            window._cf_chl_opt.cOgUQuery +
            window._cf_chl_opt.cOgUHash;
          history.replaceState(
            null,
            null,
            ""\/jobs?q=Software&l=India&start=0&__cf_chl_rt_tk=n.btGmEn7Tl1Dg7_ltP8mRKhIburEcL5nHF9v6J9eAo-1694531197-0-gaNycGzNC2U"" +
              window._cf_chl_opt.cOgUHash
          );
          cpo.onload = function () {
            history.replaceState(null, null, ogU);
          };
        }
        document.getElementsByTagName(""head"")[0].appendChild(cpo);
      })();
    </script>
  </body>
</html>

â€‹",3,2023-09-12 15:16:30
"What do I put at the entry for ""cookies""? Octoparse-tool","Im totally new to webscraping and I'm trying to do my first steps with an tool from Octoparse. But I do not know what I should put for ""cookies"" and I'm truly lost. I can't find anything on the Internet either. 
Could someone please help me?",1,2023-09-12 17:11:55
Real Estate Data Scraping,"I currently spend $70 to access information about homes listed as either ""For Sale By Owner"" or those with recently expired listings (indicating the end of their Realtor agreements).
I am seeking a real-time data acquisition solution.  Although I am a subscriber to the Multiple Listing Service (MLS), where Realtors input this exact data, I am skeptical that they permit direct data scraping from their website. My current third-party data provider likely obtains this data through an API or by paying a licensing fee.  Can anyone assist in finding a solution for this?",1,2023-09-12 10:16:30
Legality of monetizing previously scraped data,"I found the data i need on the internet, someone had already scraped it for research purposes and published it online. How legal is it to use a transformed version of that data for my paid app? or to grow my free app?
also. i see a lot of services that sell scraped data, how come they how no legal trouble? i thought scraping isn't allowed if it hurts the scraped business.",2,2023-09-12 03:37:38
Perimeter x,"Did anyone manage to bypass perimeter x? Iâ€™ve been trying for a while but found no solution.
Any help would be highly appreciated, Iâ€™d pay for kind gesture.",2,2023-09-12 01:08:08
Beginner data scraper having trouble retrieving data,"So I am trying to use a Goodreads scraper for book reviews that I found on Github but it is a year outdated and not working. Would anyone happen to understand this error so I know how I can fix it?
Link to scraper- https://github.com/maria-antoniak/goodreads-scraper/tree/master 
Thanks
2023-09-11 14:54:06.218897 get_books.py: Scraping 374233.If_on_a_Winter_s_Night_a_Traveler...
2023-09-11 14:54:06.218949 get_books.py: #1 out of 3 books
Traceback (most recent call last):
File ""/home/user/workspace/goodreads/goodreads-scraper/get_books.py"", line 247, in <module>
main()
File ""/home/user/workspace/goodreads/goodreads-scraper/get_books.py"", line 223, in main
book = scrape_book(book_id)
^^^^^^^^^^^^^^^^^^^^
File ""/home/user/workspace/goodreads/goodreads-scraper/get_books.py"", line 170, in scrape_book
'book_title':           ' '.join(soup.find('h1', {'id': 'bookTitle'}).text.split()),
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'text'
2023-09-11 14:54:14.568906 get_reviews.py: Scraping 374233.If_on_a_Winter_s_Night_a_Traveler...
2023-09-11 14:54:14.568963 get_reviews.py: #1 out of 3 books
Traceback (most recent call last):
File ""/home/user/workspace/goodreads/goodreads-scraper/get_reviews.py"", line 333, in <module>
main()
File ""/home/user/workspace/goodreads/goodreads-scraper/get_reviews.py"", line 299, in main
reviews = get_reviews_first_ten_pages(driver, book_id, args.sort_order, rating=args.rating_filter)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/user/workspace/goodreads/goodreads-scraper/get_reviews.py"", line 158, in get_reviews_first_ten_pages
switch_reviews_mode(driver, book_id, sort_order, rating=rating)
File ""/home/user/workspace/goodreads/goodreads-scraper/get_reviews.py"", line 38, in switch_reviews_mode
driver.execute_script(
File ""/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.4_1/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py"", line 404, in execute_script
return self.execute(command, {""script"": script, ""args"": converted_args})[""value""]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.4_1/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py"", line 344, in execute
self.error_handler.check_response(response)
File ""/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.4_1/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py"", line 229, in check_response
raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.JavascriptException: Message: TypeError: document.getElementById(...) is null
Stacktrace:
@https://www.goodreads.com/book/show/374233.If_on_a_Winter_s_Night_a_Traveler:2:16
@https://www.goodreads.com/book/show/374233.If_on_a_Winter_s_Night_a_Traveler:3:8
â€‹
â€‹
â€‹
â€‹",1,2023-09-11 22:02:14
Scraper Race! Puppeteer JS vs Playwright Python vs Selenium Python,"Hey Everyone, I ran a [rather silly] race between Puppeteer, Playwright and Selenium to see which one would be fastest on a simple scrape.
Far from a comprehensive benchmark, this race is 100% free from advanced configurations, multi-threading or anything complicated. It just opens Wallapop (a second hand marketplace in Spain) and times how long it takes to extract the first 2000 results of a search.
Another thing to note is that I ran this on Google Colab, that throttles resources unpredictably, so take this as it is, just a simple-fun race with lots of questionable decisions.
If you like this simple format, have any ideas on how to improve a race like this or have a strong urge to prove Ward Cunningham wright, let me know in the comments!
(Also, if you think your tool of choice isn't being represented fairly, feel free show how simple code improvements yield more speed with the same resources :)",7,2023-09-11 09:22:29
"Can't connect to this website, 403 enable JavaScript and cookies to continue","I have been playing around with the headers but nothing worked. Would appreciate any help
*this code is from postman for the full page
import requests

url = ""https://aeonretail.com/Form/Product/ProductList.aspx?gspscol=100190&psc=5&gspsp=3""

payload={}
headers = {
  'authority': 'aeonretail.com',
  'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
  'accept-language': 'en-US,en;q=0.9,ar;q=0.8,en-AU;q=0.7,en-GB;q=0.6,it;q=0.5',
  'cache-control': 'no-cache',
  'cookie': 'aigent_cookie=b4fdc152-1786-4a42-b6ce-722e0b8f67bb; w2cFront_UserId=20230613_f2ca16cc-9bb6-4097-9beb-0314748ee40e; __access_user_id=259909402.1686635371.85640228; ASP.NET_SessionId.Front=mnrwhht5o2l4n3mq0lividns; __session_id=259909402.1694397441.2003107691; authkey=e3b94165-ae32-4a2a-8164-831477110907; cf_clearance=brY5xoLFC_Iw0sAomCWq7KTY0CDOGtoAZTlcV3GE9Zs-1694419544-0-1-a83dc16a.b6c17a8c.d11afdc9-0.2.1694419544; __last_acs_date=259909402.1694419558269',
  'pragma': 'no-cache',
  'sec-ch-ua': '""Chromium"";v=""116"", ""Not)A;Brand"";v=""24"", ""Google Chrome"";v=""116""',
  'sec-ch-ua-mobile': '?0',
  'sec-ch-ua-platform': '""Windows""',
  'sec-fetch-dest': 'document',
  'sec-fetch-mode': 'navigate',
  'sec-fetch-site': 'same-origin',
  'sec-fetch-user': '?1',
  'upgrade-insecure-requests': '1',
  'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'
}

response = requests.request(""GET"", url, headers=headers, data=payload)

print(response.text)

It works fine from the browser and I don't get redirected to anything so I don't think it's a ban",2,2023-09-11 08:38:45
How to filter 5 star ratings and just find the average of 1 to 4 star ratings in Google maps to find the genuine rating of a business ?,"I am tired of fake ratings and fake reviews in Google maps and I have been cheated many times by fake 5 star reviews. I am just looking to find the genuine overall rating of a place by filtering out 5 star ratings because fake ratings are mostly 5 star and just finding the average of 1-4 ratings so that we can find the overall genuine rating of a Business.
Is there any way or any tool or any application for that ? Need suggestions on this.",0,2023-09-10 19:23:43
Running selenium-script in the cloud?,"Hello - i have a working selenium-scripts which scrapes data from a website and send an email-alert when something is found - works fine and is great.
Now i want to run the script all the time and donÂ´t want to run my computer all the time.
I would need an mostly easy soluton to run this script with  selenium dependency in a cloud.
Which solutions - the easier the better - are possible?
Any recommendations?",3,2023-09-09 20:58:56
"Blank Output File Error, I'm trying to scrape from this url ""https://watson.foundation/fellowships/tj/fellows"" but everytime I open the file, I get a blank csv and txt file output...","Install and load the rvest package
if (!requireNamespace(""rvest"", quietly = TRUE)) {
  install.packages(""rvest"")
}
library(rvest)
Define the URL
url <- ""https://watson.foundation/fellowships/tj/fellows""
Read the HTML content of the webpage
webpage <- read_html(url)
Extract information using CSS selectors
title <- webpage %>%
  html_nodes(""h4.fellow-name"") %>%
  html_text()
name <- webpage %>%
  html_nodes(""h3.fellow-school"") %>%
  html_text()
college <- webpage %>%
  html_nodes(""h2.fellow-project"") %>%
  html_text()
description <- webpage %>%
  html_nodes(""div.fellow-description"") %>%
  html_text()
Combine the extracted data into a data frame
fellows_df <- data.frame(
  Title = title,
  Name = name,
  College = college,
  Description = description
)
Print the first few rows of the data frame to verify
print(head(fellows_df))
Save the data to a text file
write.table(fellows_df, ""fellow_info.txt"", row.names = FALSE, quote = FALSE, sep = ""\t"")
cat(""Fellow information has been scraped and saved to 'fellow_info.txt'.\n"")",1,2023-09-10 01:06:34
Can't Download TikTok Series Videos,"TikTok has just launched TikTok Series.
I paid to be able to access the video series.
But it is impossible to save the videos. I tried to record my screen on my iPhone but in the end I only have a blackscreen video. TikTok prevents videos from being saved / recorded.
I tried on my PC but TikTok only offers the Series on the mobile application.
Is there a way to save the videos? Any Idea?",1,2023-09-09 16:37:23
ASPX Database,"Can anyone give me some pointers on how I could scrape this database into a spreadsheet? I've tried different Python tutorials and nothing's quite working for me.
https://www.bpl-orsnapshot.net/PublicInquiry_CJ/EmployeeSearch.aspx
Thank you.",0,2023-09-08 23:15:26
How do you compare apples to apples scrapping tools/packages?,"Hi all,
I work for a large financial data company.  Specifically for our product, we scrape newswires and general financial announcements from the web which we take in and enrich our data.  We come across multiple different tools that help with annotating & scrapping such sites.  Most if not all are semi blackbox where it's one model applying to any and all doc/url/sites we submit.  This requires tagging which leads to training which may and may not improve quality/accuracy.  But this is very manual and time consuming.  We get demos from internal Data/data science teams, external vendors etc who all try to pitch.  Basic questions to these operations always lead to tagging/training but typically of one model for all which we find usually wont work (there's no silver bullet that can work for all variations/examples etc).  
We built up an internal team that work primarily with Python and the differ packages to create our own models but break up the use cases (sources/urls/pdfs) to groups that are similar in nature and create a custom solution for each scenario to increase the accuracy and reduce the training.  This is especially true for structured data.
My question for the group.  I want to be able to assess and compare such tools as they come along in an apples to apples way.  What I was thinking was to create some test cases (excel file of URLs/PDFs etc with their expected output) for a set of structured data (maybe 1K sample) + semi structured + unstructured.  and run such datasets against the tools as a baseline comparison.  This can be different applications, or new tools like ChatGBT or  Bart etc.  While this will only give me Non-tagging/training results, it at least gives a base line.  Maybe I can add  say ""after 100 tags, here are the results"" and compare again.  What do you guys think would be a good approach to measure/compare apples to apples scraping tools/packages on structured/semi/unstructured data/docs?
â€‹",2,2023-09-08 18:35:16
How to automate one by one search in a website and keep the matching result.,"I have the following need:
-I log into a vendor website and there is a Searchbox where I need to put one by one some information (Text, Code, etc). Then, I will take me to a result page, if all informatin is correct, I need to extract that.
Because I do not have access to the whole database, I need to do one by one Search. How could I automate this manual search and keep the results in csv or Excel to save time?",1,2023-09-08 19:30:35
New To Web scraping,"Simple question. Why do I keep hearing about people using selenium on this sub but most of the YouTube videos are recommended playwrite and saying itâ€™s more mature. Like why are not you guys using or talking about it at least, is there something very special about selenium or bad about playwrite.
Just a bit about my background, I still have not yet started to learn web scraping in terms of the code itself but just made some research on the methodology of doing it.
Thank you all in advance!",1,2023-09-08 04:51:20
Why are some sites harder to scrape than others (in terms of bypassing bot detection)?,"I am quite new to web scraping. I've found some sites I have no issue being detected, while other sites I am detected as a bot upon my first html request. What accounts for the variability in bot detection per site? Does browser also have a role to play in this (e.g. when using Selenium's webdriver)?",1,2023-09-08 01:19:51
Looking to exchange my 1 billion Tiktok user profile data with the same amount of Instagram profile data.,"I scraped 1 billion Tiktok user profiles along with their following/follower count, bio, like count etc.
I'm looking for someone with 1 billion Instagram user profile data to exchange.",6,2023-07-24 22:22:45
Scraping LinkedIn for jobs,"Just finished this, maybe will be useful for some: https://github.com/cwwmbm/linkedinscraper.

What is it? Python script scraping LinkedIn for jobs and stores them into database. config.json is where all the setup and parameters are being set. There's also a web interface to launch on local machine so you can browse them. Screenshot: https://github.com/cwwmbm/linkedinscraper/blob/main/screenshot/screenshot.png
Why is it? Because LinkedIn is the worst website in the world and the less I go there the lower my blood pressure will be. Everything about it is annoying. Everything. Also, I needed a project.
Important notes:


It scrapes jobs synchronously, so it takes a bit of time. I couldn't make asynchronous sync work; tried several libraries and for all LinkedIn refused connection. First launch is the longest because the database is empty, so it'll scrape everything it finds (within the parameters you set). I did 10 queries, it took about 15 minutes. All subsequent launches will be much quicker because it won't scrape jobs that are already in the database.
It's in the readme but it bears repeating: if you are using this application, please be aware that LinkedIn does not allow scraping of its website. Use this application at your own risk. It's recommended to use proxy servers to avoid getting blocked by LinkedIn. If you do not use proxy it's extremely likely your IP will be blocked very quickly. I use paid residential proxies to do this. It costs less than a coffee for a lot of mileage.

What 
FAQ:

Why is web interface so basic? Your mom is basic. But also, I never ever programmed web interface in my life. This is the first time I created html file. If you think you can do better please be my guest.
I tried to run it and it shows an error, can you please help me? Use ChatGPT to help you debug. If I could make it write web interface for this you can make it debug it for you. I believe in you.
What problem this addresses, specifically?


LinkedIn can't just show recent jobs, they will be peppered with sponsored postings.
LinkedIn makes decisions what's relevant to you and what not based on internal algorithm written by someone who hates humanity
LinkedIn will show you same jobs that you saw for months without showing you anything new
LinkedIn will show you jobs completely irrelevant to you that only have in common one common word in the description. 
My current job recommendations on LinkedIn are: a 6 month old job, a construction job (I'm in software), a job in Phoenix, AZ (I'm in Canada), and two software jobs that have nothing to do with me at all.
Last week I was shown a job for the first time - it was two weeks old, it was very relevant to me, and it was 2 weeks too late to do anything about because they had over 200 applicants and stopped taking applications. I had similar experiences almost every month.
LinkedIn is a product that's written by people who don't use it. So it's understandable that it sucks donkey dicks, but it also got the monopoly on job postings where I am so I got no choice.
LinkedIn is a stupid name
It's also a stupid concept
I hate it
Fuck LinkedIn.",27,2023-06-08 05:01:42
